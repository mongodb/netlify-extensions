 {
    "url": "http://mongodb.com/docs/drivers/node/current",
    "includeInGlobalSearch": true,
    "documents": [
        {
            "slug": "aggregation-tutorials/filtered-subset",
            "title": "Filtered Subset",
            "headings": [
                "Introduction",
                "Aggregation Task Summary",
                "Before You Get Started",
                "Tutorial",
                "Add a match stage for people who are engineers",
                "Add a sort stage to sort from youngest to oldest",
                "Add a limit stage to see only three results",
                "Add an unset stage to remove unneeded fields",
                "Run the aggregation pipeline",
                "Interpret results"
            ],
            "paragraphs": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs the following operations: Matches a subset of documents by a field value Formats result documents You can also query for a subset of documents in a collection by using the\nQuery API. To learn how to specify a query, see the\n Read Operations guides . This tutorial demonstrates how to query a collection for a specific\nsubset of documents in a collection. The results contain\ndocuments that describe the three youngest people who are engineers. This example uses one collection,  persons , which contains\ndocuments describing people. Each document includes a person's name,\ndate of birth, vocation, and other details. Before you start this tutorial, complete the\n Aggregation Template App  instructions to set up a working\nNode.js application. After you set up the app, access the  persons  collection by adding the\nfollowing code to the application: Delete any existing data in the collections and insert sample data into\nthe  persons  collection as shown in the following code: To view the complete code for this tutorial, see the  Completed Filtered Subset App \non GitHub. First, add a  $match  stage that finds documents in which\nthe value of the  vocation  field is  \"ENGINEER\" : Next, add a  $sort  stage that sorts the\ndocuments in descending order by the  dateofbirth  field to\nlist the youngest people first: Next, add a  $limit \nstage to the pipeline to output only the first three documents in\nthe results. Finally, add an  $unset  stage. The\n $unset  stage removes unnecessary fields from the result documents: Use the  $unset  operator instead of  $project  to avoid\nmodifying the aggregation pipeline if documents with\ndifferent fields are added to the collection. Add the following code to the end of your application to perform\nthe aggregation on the  persons  collection: Finally, run the following command in your shell to start your\napplication: The aggregated result contains three documents. The documents\nrepresent the three youngest people with the vocation of  \"ENGINEER\" ,\nordered from youngest to oldest. The results omit the  _id  and  address \nfields.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const personColl = aggDB.collection(\"persons\");"
                },
                {
                    "lang": "javascript",
                    "value": "await personColl.deleteMany({});\n\nconst personData = [\n  {\n    person_id: \"6392529400\",\n    firstname: \"Elise\",\n    lastname: \"Smith\",\n    dateofbirth: new Date(\"1972-01-13T09:32:07Z\"),\n    vocation: \"ENGINEER\",\n    address: {\n      number: 5625,\n      street: \"Tipa Circle\",\n      city: \"Wojzinmoj\",\n    },\n  },\n  {\n    person_id: \"1723338115\",\n    firstname: \"Olive\",\n    lastname: \"Ranieri\",\n    dateofbirth: new Date(\"1985-05-12T23:14:30Z\"),\n    gender: \"FEMALE\",\n    vocation: \"ENGINEER\",\n    address: {\n      number: 9303,\n      street: \"Mele Circle\",\n      city: \"Tobihbo\",\n    },\n  },\n  {\n    person_id: \"8732762874\",\n    firstname: \"Toni\",\n    lastname: \"Jones\",\n    dateofbirth: new Date(\"1991-11-23T16:53:56Z\"),\n    vocation: \"POLITICIAN\",\n    address: {\n      number: 1,\n      street: \"High Street\",\n      city: \"Upper Abbeywoodington\",\n    },\n  },\n  {\n    person_id: \"7363629563\",\n    firstname: \"Bert\",\n    lastname: \"Gooding\",\n    dateofbirth: new Date(\"1941-04-07T22:11:52Z\"),\n    vocation: \"FLORIST\",\n    address: {\n      number: 13,\n      street: \"Upper Bold Road\",\n      city: \"Redringtonville\",\n    },\n  },\n  {\n    person_id: \"1029648329\",\n    firstname: \"Sophie\",\n    lastname: \"Celements\",\n    dateofbirth: new Date(\"1959-07-06T17:35:45Z\"),\n    vocation: \"ENGINEER\",\n    address: {\n      number: 5,\n      street: \"Innings Close\",\n      city: \"Basilbridge\",\n    },\n  },\n  {\n    person_id: \"7363626383\",\n    firstname: \"Carl\",\n    lastname: \"Simmons\",\n    dateofbirth: new Date(\"1998-12-26T13:13:55Z\"),\n    vocation: \"ENGINEER\",\n    address: {\n      number: 187,\n      street: \"Hillside Road\",\n      city: \"Kenningford\",\n    },\n  },\n];\n\nawait personColl.insertMany(personData);"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $match: {\n    \"vocation\": \"ENGINEER\"\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $sort: {\n    \"dateofbirth\": -1,\n  }\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $limit: 3\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $unset: [\n    \"_id\",\n    \"address\",\n  ]\n});"
                },
                {
                    "lang": "bash",
                    "value": "node agg_tutorial.js"
                },
                {
                    "lang": "javascript",
                    "value": "const aggregationResult = await personColl.aggregate(pipeline);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  person_id: '7363626383',\n  firstname: 'Carl',\n  lastname: 'Simmons',\n  dateofbirth: 1998-12-26T13:13:55.000Z,\n  vocation: 'ENGINEER'\n}\n{\n  person_id: '1723338115',\n  firstname: 'Olive',\n  lastname: 'Ranieri',\n  dateofbirth: 1985-05-12T23:14:30.000Z,\n  gender: 'FEMALE',\n  vocation: 'ENGINEER'\n}\n{\n  person_id: '6392529400',\n  firstname: 'Elise',\n  lastname: 'Smith',\n  dateofbirth: 1972-01-13T09:32:07.000Z,\n  vocation: 'ENGINEER'\n}"
                }
            ],
            "preview": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs the following operations:",
            "tags": "code example, node.js, sort, limit, aggregation",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "aggregation-tutorials/group-total",
            "title": "Group and Total",
            "headings": [
                "Introduction",
                "Aggregation Task Summary",
                "Before You Get Started",
                "Tutorial",
                "Add a match stage for orders in 2020",
                "Add a sort stage to sort by order date",
                "Add a group stage to group by email address",
                "Add a sort stage to sort by first order date",
                "Add a set stage to display the email address",
                "Add an unset stage to remove unneeded fields",
                "Run the aggregation pipeline",
                "Interpret results"
            ],
            "paragraphs": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs the following operations: Matches a subset of documents by a field value Groups documents by common field values Adds computed fields to each result document This tutorial demonstrates how to group and analyze customer order data. The\nresults show the list of customers who purchased items in 2020 and\nincludes each customer's order history for 2020. This example uses one collection,  orders , which contains documents\ndescribing individual product orders. Since each order can correspond to\nonly one customer, the order documents are grouped by the\n customer_id  field, which contains customer email addresses. Before you start this tutorial, complete the\n Aggregation Template App  instructions to set up a working\nNode.js application. After you set up the app, access the  orders  collection by adding the\nfollowing code to the application: Delete any existing data and insert sample data into\nthe  orders  collection as shown in the following code: To view the complete code for this tutorial, see the  Completed Group and Total App \non GitHub. First, add a  $match  stage that matches\norders placed in 2020: Next, add a  $sort  stage to set an\nascending sort on the  orderdate  field to surface the earliest\n2020 purchase for each customer in the next stage: Add a  $group  stage to group\norders by the value of the  customer_id  field. In this\nstage, add aggregation operations that create the\nfollowing fields in the result documents: first_purchase_date : the date of the customer's first purchase total_value : the total value of all the customer's purchases total_orders : the total number of the customer's purchases orders : the list of all the customer's purchases,\nincluding the date and value of each purchase Next, add another  $sort  stage to set an\nascending sort on the  first_purchase_date  field: Add a  $set  stage to recreate the\n customer_id  field from the values in the  _id  field\nthat were set during the  $group  stage: Finally, add an  $unset  stage. The\n $unset  stage removes the  _id  field from the result\ndocuments: Add the following code to the end of your application to perform\nthe aggregation on the  orders  collection: Finally, run the following command in your shell to start your\napplication: The aggregation returns the following summary of customers' orders\nfrom 2020: The result documents contain details from all the orders from\na given customer, grouped by the customer's email address.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const ordersColl = aggDB.collection(\"orders\");"
                },
                {
                    "lang": "javascript",
                    "value": "await ordersColl.deleteMany({});\n\nconst orderData = [\n  {\n    customer_id: \"elise_smith@myemail.com\",\n    orderdate: new Date(\"2020-05-30T08:35:52Z\"),\n    value: 231,\n  },\n  {\n    customer_id: \"elise_smith@myemail.com\",\n    orderdate: new Date(\"2020-01-13T09:32:07Z\"),\n    value: 99,\n  },\n  {\n    customer_id: \"oranieri@warmmail.com\",\n    orderdate: new Date(\"2020-01-01T08:25:37Z\"),\n    value: 63,\n  },\n  {\n    customer_id: \"tj@wheresmyemail.com\",\n    orderdate: new Date(\"2019-05-28T19:13:32Z\"),\n    value: 2,\n  },\n  {\n    customer_id: \"tj@wheresmyemail.com\",\n    orderdate: new Date(\"2020-11-23T22:56:53Z\"),\n    value: 187,\n  },\n  {\n    customer_id: \"tj@wheresmyemail.com\",\n    orderdate: new Date(\"2020-08-18T23:04:48Z\"),\n    value: 4,\n  },\n  {\n    customer_id: \"elise_smith@myemail.com\",\n    orderdate: new Date(\"2020-12-26T08:55:46Z\"),\n    value: 4,\n  },\n  {\n    customer_id: \"tj@wheresmyemail.com\",\n    orderdate: new Date(\"2021-02-29T07:49:32Z\"),\n    value: 1024,\n  },\n  {\n    customer_id: \"elise_smith@myemail.com\",\n    orderdate: new Date(\"2020-10-03T13:49:44Z\"),\n    value: 102,\n  },\n];\n\nawait ordersColl.insertMany(orderData);"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $match: {\n    orderdate: {\n      $gte: new Date(\"2020-01-01T00:00:00Z\"),\n      $lt: new Date(\"2021-01-01T00:00:00Z\"),\n    },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $sort: {\n    orderdate: 1,\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $group: {\n    _id: \"$customer_id\",\n    first_purchase_date: { $first: \"$orderdate\" },\n    total_value: { $sum: \"$value\" },\n    total_orders: { $sum: 1 },\n    orders: { $push: \n      { \n        orderdate: \"$orderdate\", \n        value: \"$value\" \n      }\n    },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $sort: {\n    first_purchase_date: 1,\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $set: {\n    customer_id: \"$_id\",\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({ $unset: [\"_id\"] });"
                },
                {
                    "lang": "bash",
                    "value": "node agg_tutorial.js"
                },
                {
                    "lang": "javascript",
                    "value": "const aggregationResult = await ordersColl.aggregate(pipeline);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  first_purchase_date: 2020-01-01T08:25:37.000Z,\n  total_value: 63,\n  total_orders: 1,\n  orders: [ { orderdate: 2020-01-01T08:25:37.000Z, value: 63 } ],\n  customer_id: 'oranieri@warmmail.com'\n}\n{\n  first_purchase_date: 2020-01-13T09:32:07.000Z,\n  total_value: 436,\n  total_orders: 4,\n  orders: [\n    { orderdate: 2020-01-13T09:32:07.000Z, value: 99 },\n    { orderdate: 2020-05-30T08:35:52.000Z, value: 231 },\n    { orderdate: 2020-10-03T13:49:44.000Z, value: 102 },\n    { orderdate: 2020-12-26T08:55:46.000Z, value: 4 }\n  ],\n  customer_id: 'elise_smith@myemail.com'\n}\n{\n  first_purchase_date: 2020-08-18T23:04:48.000Z,\n  total_value: 191,\n  total_orders: 2,\n  orders: [\n    { orderdate: 2020-08-18T23:04:48.000Z, value: 4 },\n    { orderdate: 2020-11-23T22:56:53.000Z, value: 187 }\n  ],\n  customer_id: 'tj@wheresmyemail.com'\n}"
                }
            ],
            "preview": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs the following operations:",
            "tags": "code example, node.js, analyze, aggregation",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "aggregation-tutorials/multi-field-join",
            "title": "Multi-Field Join",
            "headings": [
                "Introduction",
                "Aggregation Task Summary",
                "Before You Get Started",
                "Tutorial",
                "Add a lookup stage to link the collections and import fields",
                "Add a match stage for products ordered in 2020",
                "Add an unset stage to remove unneeded fields",
                "Run the aggregation pipeline",
                "Interpret results"
            ],
            "paragraphs": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs a multi-field join. A multi-field join occurs when there are\nmultiple corresponding fields in the documents of two collections that you use to\nmatch documents together. The aggregation matches these documents on the\nfield values and combines information from both into one document. A one-to-many join is a variety of a multi-field join. When you\nperform a one-to-many join, you select one field from a document that\nmatches a field value in multiple documents on the other side of the\njoin. To learn more about these data relationships,\nsee the Wikipedia entries about  One-to-many (data model)  and\n Many-to-many (data model) . This tutorial demonstrates how to combine data from a collection that\ndescribes product information with another collection that describes\ncustomer orders. The results show a list of products ordered in 2020\nthat also contains details about each order. This example uses two collections: An order can only contain one product, so the aggregation uses a\nmulti-field join to match a product document to documents representing orders of\nthat product. The collections are joined by the  name  and\n variation  fields in documents in the  products  collection, corresponding\nto the  product_name  and  product_variation  fields in documents in\nthe  orders  collection. products , which contains documents describing the products that\na shop sells orders , which contains documents describing individual orders\nfor products in a shop Before you start this tutorial, complete the\n Aggregation Template App  instructions to set up a working\nNode.js application. After you set up the app, access the  products  and  orders \ncollections by adding the following code to the application: Delete any existing data and insert sample data into\nthe  products  collection as shown in the following code: Delete any existing data and insert sample data into\nthe  orders  collection as shown in the following code: To view the complete code for this tutorial, see the  Completed Multi-field Join App \non GitHub. The first stage of the pipeline is a  $lookup  stage to join the\n orders  collection to the  products  collection by two\nfields in each collection. The lookup stage contains an\nembedded pipeline to configure the join. Within the embedded pipeline, add a  $match  stage to match the\nvalues of two fields on each side of the join. Note that the following\ncode uses aliases for the  name  and  variation  fields\nset when  creating the $lookup stage : Within the embedded pipeline, add another  $match  stage to match\norders placed in 2020: Within the embedded pipeline, add an  $unset  stage to remove\nunneeded fields from the  orders  collection side of the join: After the embedded pipeline is completed, add the\n $lookup  stage to the main aggregation pipeline.\nConfigure this stage to store the processed lookup fields in\nan array field called  orders : Next, add a  $match  stage to only show\nproducts for which there is at least one order in 2020,\nbased on the  orders  array calculated in the previous step: Finally, add an  $unset  stage. The\n $unset  stage removes the  _id  and  description \nfields from the result documents: Add the following code to the end of your application to perform\nthe aggregation on the  products  collection: Finally, run the following command in your shell to start your\napplication: The aggregated result contains two documents. The documents\nrepresent products for which there were orders placed in 2020.\nEach document contains an  orders  array field that lists details\nabout each order for that product: The result documents contain details from documents in the\n orders  collection and the  products  collection, joined by\nthe product names and variations.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const productsColl = aggDB.collection(\"products\");\nconst ordersColl = aggDB.collection(\"orders\");"
                },
                {
                    "lang": "javascript",
                    "value": "await productsColl.deleteMany({});\n\nconst productsData = [\n  {\n    name: \"Asus Laptop\",\n    variation: \"Ultra HD\",\n    category: \"ELECTRONICS\",\n    description: \"Great for watching movies\",\n  },\n  {\n    name: \"Asus Laptop\",\n    variation: \"Standard Display\",\n    category: \"ELECTRONICS\",\n    description: \"Good value laptop for students\",\n  },\n  {\n    name: \"The Day Of The Triffids\",\n    variation: \"1st Edition\",\n    category: \"BOOKS\",\n    description: \"Classic post-apocalyptic novel\",\n  },\n  {\n    name: \"The Day Of The Triffids\",\n    variation: \"2nd Edition\",\n    category: \"BOOKS\",\n    description: \"Classic post-apocalyptic novel\",\n  },\n  {\n    name: \"Morphy Richards Food Mixer\",\n    variation: \"Deluxe\",\n    category: \"KITCHENWARE\",\n    description: \"Luxury mixer turning good cakes into great\",\n  },\n];\n\nawait productsColl.insertMany(productsData);"
                },
                {
                    "lang": "javascript",
                    "value": "await ordersColl.deleteMany({});\n\nconst orderData = [\n  {\n    customer_id: \"elise_smith@myemail.com\",\n    orderdate: new Date(\"2020-05-30T08:35:52Z\"),\n    product_name: \"Asus Laptop\",\n    product_variation: \"Standard Display\",\n    value: 431.43,\n  },\n  {\n    customer_id: \"tj@wheresmyemail.com\",\n    orderdate: new Date(\"2019-05-28T19:13:32Z\"),\n    product_name: \"The Day Of The Triffids\",\n    product_variation: \"2nd Edition\",\n    value: 5.01,\n  },\n  {\n    customer_id: \"oranieri@warmmail.com\",\n    orderdate: new Date(\"2020-01-01T08:25:37Z\"),\n    product_name: \"Morphy Richards Food Mixer\",\n    product_variation: \"Deluxe\",\n    value: 63.13,\n  },\n  {\n    customer_id: \"jjones@tepidmail.com\",\n    orderdate: new Date(\"2020-12-26T08:55:46Z\"),\n    product_name: \"Asus Laptop\",\n    product_variation: \"Standard Display\",\n    value: 429.65,\n  },\n];\n\nawait ordersColl.insertMany(orderData);"
                },
                {
                    "lang": "javascript",
                    "value": "const embedded_pl = [];\n\nembedded_pl.push({\n  $match: {\n    $expr: {\n      $and: [\n        { $eq: [\"$product_name\", \"$$prdname\"] },\n        { $eq: [\"$product_variation\", \"$$prdvartn\"] },\n      ],\n    },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "embedded_pl.push({\n  $match: {\n    orderdate: {\n      $gte: new Date(\"2020-01-01T00:00:00Z\"),\n      $lt: new Date(\"2021-01-01T00:00:00Z\"),\n    },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "embedded_pl.push({\n  $unset: [\"_id\", \"product_name\", \"product_variation\"],\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $lookup: {\n    from: \"orders\",\n    let: {\n      prdname: \"$name\",\n      prdvartn: \"$variation\",\n    },\n    pipeline: embedded_pl,\n    as: \"orders\",\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $match: {\n    orders: { $ne: [] },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $unset: [\"_id\", \"description\"],\n});"
                },
                {
                    "lang": "bash",
                    "value": "node agg_tutorial.js"
                },
                {
                    "lang": "javascript",
                    "value": "const aggregationResult = await productsColl.aggregate(pipeline);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  name: 'Asus Laptop',\n  variation: 'Standard Display',\n  category: 'ELECTRONICS',\n  orders: [\n    {\n      customer_id: 'elise_smith@myemail.com',\n      orderdate: 2020-05-30T08:35:52.000Z,\n      value: 431.43\n    },\n    {\n      customer_id: 'jjones@tepidmail.com',\n      orderdate: 2020-12-26T08:55:46.000Z,\n      value: 429.65\n    }\n  ]\n}\n{\n  name: 'Morphy Richards Food Mixer',\n  variation: 'Deluxe',\n  category: 'KITCHENWARE',\n  orders: [\n    {\n      customer_id: 'oranieri@warmmail.com',\n      orderdate: 2020-01-01T08:25:37.000Z,\n      value: 63.13\n    }\n  ]\n}"
                }
            ],
            "preview": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app.",
            "tags": "code example, node.js, lookup, aggregation",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "aggregation-tutorials/one-to-one-join",
            "title": "One-to-One Join",
            "headings": [
                "Introduction",
                "Aggregation Task Summary",
                "Before You Get Started",
                "Tutorial",
                "Add a match stage for orders in 2020",
                "Add a lookup stage to link the collections",
                "Add set stages to create new document fields",
                "Add an unset stage to remove unneeded fields",
                "Run the aggregation pipeline",
                "Interpret results"
            ],
            "paragraphs": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs a one-to-one join. A one-to-one join occurs\nwhen a document in one collection has a field value that matches a\nsingle document in another collection that has the same field value. The\naggregation matches these documents on the field value and combines\ninformation from both sources into one result. A one-to-one join does not require the documents to have a\none-to-one relationship. To learn more about this data relationship,\nsee the Wikipedia entry about  One-to-one (data model) . This tutorial demonstrates how to combine data from a collection that\ndescribes product information with another collection that describes\ncustomer orders. The results show a list of all orders placed in 2020 that\nincludes the product details associated with each order. This example uses two collections: An order can only contain one product, so the aggregation uses a\none-to-one join to match an order document to the document for the\nproduct. The collections are joined by a field called  product_id \nthat exists in documents in both collections. orders : contains documents describing individual orders\nfor products in a shop products : contains documents describing the products that\na shop sells Before you start this tutorial, complete the\n Aggregation Template App  instructions to set up a working\nNode.js application. After you set up the app, access the  orders  and  products \ncollections by adding the following code to the application: Delete any existing data and insert sample data into\nthe  orders  collection as shown in the following code: Delete any existing data and insert sample data into\nthe  products  collection as shown in the following code: To view the complete code for this tutorial, see the  Completed One-to-one Join App \non GitHub. Add a  $match  stage that matches\norders placed in 2020: Next, add a  $lookup  stage. The\n $lookup  stage joins the  product_id  field in the  orders \ncollection to the  id  field in the  products  collection: Next, add two  $set \nstages to the pipeline. The first  $set  stage sets the  product_mapping  field\nto the first element in the  product_mapping  object\ncreated in the previous  $lookup  stage. The second  $set  stage creates two new fields,  product_name \nand  product_category , from the values in the\n product_mapping  object field: Because this is a one-to-one join, the  $lookup  stage\nadds only one array element to the input document. The pipeline\nuses the  $first \noperator to retrieve the data from this element. Finally, add an  $unset  stage. The\n $unset  stage removes unnecessary fields from the document: Add the following code to the end of your application to perform\nthe aggregation on the  orders  collection: Finally, run the following command in your shell to start your\napplication: The aggregated result contains three documents. The documents\nrepresent customer orders that occurred in 2020, with the\n product_name  and  product_category  of the ordered product: The result consists of documents that contain fields from\ndocuments in the  orders  collection and the  products \ncollection, joined by matching the  product_id  field present in\neach original document.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const ordersColl = aggDB.collection(\"orders\");\nconst productsColl = aggDB.collection(\"products\");"
                },
                {
                    "lang": "javascript",
                    "value": "await ordersColl.deleteMany({});\n\nconst orderData = [\n  {\n    customer_id: \"elise_smith@myemail.com\",\n    orderdate: new Date(\"2020-05-30T08:35:52Z\"),\n    product_id: \"a1b2c3d4\",\n    value: 431.43,\n  },\n  {\n    customer_id: \"tj@wheresmyemail.com\",\n    orderdate: new Date(\"2019-05-28T19:13:32Z\"),\n    product_id: \"z9y8x7w6\",\n    value: 5.01,\n  },\n  {\n    customer_id: \"oranieri@warmmail.com\",\n    orderdate: new Date(\"2020-01-01T08:25:37Z\"),\n    product_id: \"ff11gg22hh33\",\n    value: 63.13,\n  },\n  {\n    customer_id: \"jjones@tepidmail.com\",\n    orderdate: new Date(\"2020-12-26T08:55:46Z\"),\n    product_id: \"a1b2c3d4\",\n    value: 429.65,\n  },\n];\n\nawait ordersColl.insertMany(orderData);"
                },
                {
                    "lang": "javascript",
                    "value": "await productsColl.deleteMany({});\n\nconst productData = [\n  {\n    id: \"a1b2c3d4\",\n    name: \"Asus Laptop\",\n    category: \"ELECTRONICS\",\n    description: \"Good value laptop for students\",\n  },\n  {\n    id: \"z9y8x7w6\",\n    name: \"The Day Of The Triffids\",\n    category: \"BOOKS\",\n    description: \"Classic post-apocalyptic novel\",\n  },\n  {\n    id: \"ff11gg22hh33\",\n    name: \"Morphy Richardds Food Mixer\",\n    category: \"KITCHENWARE\",\n    description: \"Luxury mixer turning good cakes into great\",\n  },\n  {\n    id: \"pqr678st\",\n    name: \"Karcher Hose Set\",\n    category: \"GARDEN\",\n    description: \"Hose + nosels + winder for tidy storage\",\n  },\n];\n\nawait productsColl.insertMany(productData);"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $match: {\n    orderdate: {\n      $gte: new Date(\"2020-01-01T00:00:00Z\"),\n      $lt: new Date(\"2021-01-01T00:00:00Z\"),\n    },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $lookup: {\n    from: \"products\",\n    localField: \"product_id\",\n    foreignField: \"id\",\n    as: \"product_mapping\",\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push(\n    {\n      $set: {\n        product_mapping: { $first: \"$product_mapping\" },\n      },\n    },\n    {\n      $set: {\n        product_name: \"$product_mapping.name\",\n        product_category: \"$product_mapping.category\",\n      },\n    }\n  );"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({ $unset: [\"_id\", \"product_id\", \"product_mapping\"] });"
                },
                {
                    "lang": "bash",
                    "value": "node agg_tutorial.js"
                },
                {
                    "lang": "javascript",
                    "value": "const aggregationResult = await ordersColl.aggregate(pipeline);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  customer_id: 'elise_smith@myemail.com',\n  orderdate: 2020-05-30T08:35:52.000Z,\n  value: 431.43,\n  product_name: 'Asus Laptop',\n  product_category: 'ELECTRONICS'\n}\n{\n  customer_id: 'oranieri@warmmail.com',\n  orderdate: 2020-01-01T08:25:37.000Z,\n  value: 63.13,\n  product_name: 'Morphy Richardds Food Mixer',\n  product_category: 'KITCHENWARE'\n}\n{\n  customer_id: 'jjones@tepidmail.com',\n  orderdate: 2020-12-26T08:55:46.000Z,\n  value: 429.65,\n  product_name: 'Asus Laptop',\n  product_category: 'ELECTRONICS'\n}"
                }
            ],
            "preview": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app.",
            "tags": "code example, node.js, lookup, aggregation",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "aggregation-tutorials/unpack-arrays",
            "title": "Unpack Arrays and Group",
            "headings": [
                "Introduction",
                "Aggregation Task Summary",
                "Before You Get Started",
                "Tutorial",
                "Add an unwind stage to unpack the array of product orders",
                "Add a match stage for products that cost more than $15",
                "Add a group stage to group by product type",
                "Add a set stage to display the product ID",
                "Add an unset stage to remove unneeded fields",
                "Run the aggregation pipeline",
                "Interpret results"
            ],
            "paragraphs": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs the following operations: Unwinds an array field into separate documents Matches a subset of documents by a field value Groups documents by common field values Adds computed fields to each result document This tutorial demonstrates how to create insights from customer order\ndata. The results show the list of products ordered that cost more than\n$15, and each document contains the number of units sold and the total\nsale value for each product. This example uses one collection,  orders , which contains documents\ndescribing product orders. Since each order contains multiple products,\nthe first step of the aggregation is unpacking the  products  array\ninto individual product order documents. Before you start this tutorial, complete the\n Aggregation Template App  instructions to set up a working\nNode.js application. After you set up the app, access the  orders  collection by adding the\nfollowing code to the application: Delete any existing data and insert sample data into\nthe  orders  collection as shown in the following code: To view the complete code for this tutorial, see the  Completed Unpack Arrays App \non GitHub. First, add an  $unwind  stage to separate the\nentries in the  products  array into individual documents: Next, add a  $match  stage that matches\nproducts with a  products.price  value greater than  15 : Add a  $group  stage to group\norders by the value of the  prod_id  field. In this\nstage, add aggregation operations that create the\nfollowing fields in the result documents: product : the product name total_value : the total value of all the sales of the product quantity : the number of orders for the product Add a  $set  stage to recreate the\n product_id  field from the values in the  _id  field\nthat were set during the  $group  stage: Finally, add an  $unset  stage. The\n $unset  stage removes the  _id  field from the result\ndocuments: Add the following code to the end of your application to perform\nthe aggregation on the  orders  collection: Finally, run the following command in your shell to start your\napplication: The aggregation returns the following summary of customers' orders\nfrom 2020: The result documents contain details about the total value and\nquantity of orders for products that cost more than $15.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const ordersColl = aggDB.collection(\"orders\");"
                },
                {
                    "lang": "javascript",
                    "value": "await ordersColl.deleteMany({});\n\nconst orderData = [\n  {\n    order_id: 6363763262239,\n    products: [\n      {\n        prod_id: \"abc12345\",\n        name: \"Asus Laptop\",\n        price: 431,\n      },\n      {\n        prod_id: \"def45678\",\n        name: \"Karcher Hose Set\",\n        price: 22,\n      },\n    ],\n  },\n  {\n    order_id: 1197372932325,\n    products: [\n      {\n        prod_id: \"abc12345\",\n        name: \"Asus Laptop\",\n        price: 429,\n      },\n    ],\n  },\n  {\n    order_id: 9812343774839,\n    products: [\n      {\n        prod_id: \"pqr88223\",\n        name: \"Morphy Richards Food Mixer\",\n        price: 431,\n      },\n      {\n        prod_id: \"def45678\",\n        name: \"Karcher Hose Set\",\n        price: 21,\n      },\n    ],\n  },\n  {\n    order_id: 4433997244387,\n    products: [\n      {\n        prod_id: \"def45678\",\n        name: \"Karcher Hose Set\",\n        price: 23,\n      },\n      {\n        prod_id: \"jkl77336\",\n        name: \"Picky Pencil Sharpener\",\n        price: 1,\n      },\n      {\n        prod_id: \"xyz11228\",\n        name: \"Russell Hobbs Chrome Kettle\",\n        price: 16,\n      },\n    ],\n  },\n];\n\nawait ordersColl.insertMany(orderData);"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $unwind: {\n    path: \"$products\",\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $match: {\n    \"products.price\": {\n      $gt: 15,\n    },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $group: {\n    _id: \"$products.prod_id\",\n    product: { $first: \"$products.name\" },\n    total_value: { $sum: \"$products.price\" },\n    quantity: { $sum: 1 },\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({\n  $set: {\n    product_id: \"$_id\",\n  },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "pipeline.push({ $unset: [\"_id\"] });"
                },
                {
                    "lang": "bash",
                    "value": "node agg_tutorial.js"
                },
                {
                    "lang": "javascript",
                    "value": "const aggregationResult = await ordersColl.aggregate(pipeline);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  product: 'Asus Laptop',\n  total_value: 860,\n  quantity: 2,\n  product_id: 'abc12345'\n}\n{\n  product: 'Morphy Richards Food Mixer',\n  total_value: 431,\n  quantity: 1,\n  product_id: 'pqr88223'\n}\n{\n  product: 'Russell Hobbs Chrome Kettle',\n  total_value: 16,\n  quantity: 1,\n  product_id: 'xyz11228'\n}\n{\n  product: 'Karcher Hose Set',\n  total_value: 66,\n  quantity: 3,\n  product_id: 'def45678'\n}"
                }
            ],
            "preview": "In this tutorial, you can learn how to use the Node.js driver to\nconstruct an aggregation pipeline, perform the\naggregation on a collection, and print the results by completing and\nrunning a sample app. This aggregation performs the following operations:",
            "tags": "code example, node.js, analyze, array",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "aggregation-tutorials",
            "title": "Aggregation Tutorials",
            "headings": [
                "Overview",
                "Aggregation Template App",
                "Available Tutorials"
            ],
            "paragraphs": "Aggregation tutorials provide detailed explanations of common\naggregation tasks in a step-by-step format. The tutorials are adapted\nfrom examples in the  Practical MongoDB Aggregations book  by Paul Done. Each tutorial includes the following sections: At the end of each aggregation tutorial, you can find a link to a fully\nrunnable Node.js code file that you can run in your environment. Introduction , which describes the purpose and common use cases of the\naggregation type. This section also describes the example and desired\noutcome that the tutorial demonstrates. Before You Get Started , which describes the necessary databases,\ncollections, and sample data that you must have before building the\naggregation pipeline and performing the aggregation. Tutorial , which describes how to build and run the aggregation\npipeline. This section describes each stage of the completed\naggregation tutorial, and then explains how to run and interpret the\noutput of the aggregation. To learn more about performing aggregations, see the\n Aggregation  guide. Before you begin following an aggregation tutorial, you must set up a\nnew Node.js app. You can use this app to connect to a MongoDB\ndeployment, insert sample data into MongoDB, and run the aggregation\npipeline in each tutorial. Once you install the driver, create a file called\n agg_tutorial.js . Paste the following code in this file to create an\napp template for the aggregation tutorials: For every tutorial, you must replace the connection string placeholder with\nyour deployment's connection string. For example, if your connection string is\n \"mongodb+srv://mongodb-example:27017\" , your connection string assignment resembles\nthe following: To run the completed file after you modify the template for a\ntutorial, run the following command in your shell: To learn how to install the driver and connect to MongoDB,\nsee the  Download and Install  and\n Create a MongoDB Deployment  steps of the\nQuick Start guide. In the preceding code, read the code comments to find the sections of\nthe code that you must modify for the tutorial you are following. If you attempt to run the code without making any changes, you will\nencounter a connection error. To learn how to locate your deployment's connection string, see the\n Create a Connection String  step of the Quick Start guide. Filtered Subset Group and Total Unpack Arrays and Group One-to-One Join Multi-Field Join",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const uri = \"mongodb+srv://mongodb-example:27017\";"
                },
                {
                    "lang": "bash",
                    "value": "node agg_tutorial.js"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the placeholder with your connection string.\nconst uri = \"<connection string>\";\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const aggDB = client.db(\"agg_tutorials_db\");\n\n    // Get a reference to relevant collections.\n    // ... const someColl =\n    // ... const anotherColl =\n\n    // Delete any existing documents in collections.\n    // ... await someColl.deleteMany({});\n\n    // Insert sample data into the collection or collections.\n    // ... const someData = [ ... ];\n\n    // ... await someColl.insertMany(someData);\n\n    // Create an empty pipeline array.\n    const pipeline = [];\n\n    // Add code to create pipeline stages.\n    // ... pipeline.push({ ... })\n\n    // Run the aggregation.\n    // ... const aggregationResult = ...\n\n    // Print the aggregation results.\n    for await (const document of aggregationResult) {\n      console.log(document);\n    }\n  } finally {\n    await client.close();\n  }\n}\n\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "Aggregation tutorials provide detailed explanations of common\naggregation tasks in a step-by-step format. The tutorials are adapted\nfrom examples in the Practical MongoDB Aggregations book by Paul Done.",
            "tags": "node.js, code example, runnable app",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "compatibility",
            "title": "Compatibility",
            "headings": [
                "MongoDB Compatibility",
                "Compatibility Table Legend",
                "Language Compatibility",
                "Component Compatibility"
            ],
            "paragraphs": "The following compatibility table specifies the recommended versions of\nthe MongoDB Node.js driver for use with MongoDB. The first column lists the driver version. MongoDB ensures compatibility between the MongoDB Server and the drivers\nfor three years after the server version's end of life (EOL) date. To learn\nmore about the MongoDB release and EOL dates, see\n MongoDB Software Lifecycle Schedules . Icon Explanation \u2713 All features are supported. \u229b The Driver version will work with the MongoDB version, but not all\nnew MongoDB features are supported. No mark The Driver version is not tested with the MongoDB version. Node.js Driver Version MongoDB 7.0 MongoDB 6.0 MongoDB 5.0 MongoDB 4.4 MongoDB 4.2 MongoDB 4.0 MongoDB 3.6 MongoDB 3.4 MongoDB 3.2 MongoDB 3.0 MongoDB 2.6 6.0 to 6.8 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 5.7 to 5.9 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 5.0 to 5.6 \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 4.8 to 4.17 \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 4.2 to 4.7 \u229b \u229b \u2713 \u2713 \u2713 \u2713 \u2713 4.0 to 4.1 \u229b \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 3.7  \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 3.6 \u229b \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 3.3 to 3.5 \u229b \u229b \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 3.1 to 3.2 \u229b \u229b \u229b \u229b \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 3.0 \u2713 \u2713 \u2713 \u2713 \u2713 2.2.12 \u2713 \u2713 \u2713 \u2713 2.0.14 \u2713 \u2713 1.4.29 \u2713 \u2713 When using Node.js Driver version 3.7, you must set the  useUnifiedTopology  flag to  true  for certain features. The following compatibility table specifies the recommended versions of\nthe MongoDB Node.js driver for use with a specific version of Node.js. The first column lists the driver version. Node.js Driver Version Node.js v20.x.x Node.js v18.x.x Node.js v16.x.x Node.js v14.x.x Node.js v12.x.x Node.js v10.x.x Node.js v8.X.X Node.js v6.X.X Node.js v4.X.X Node.js v0.12.X Node.js v0.10.X Node.js v0.8.X 6.X  \u2713 \u2713 \u2713 5.6.X to 5.9.X \u2713 \u2713 \u2713 \u2713 5.0.0 to 5.5.X \u2713 \u2713 \u2713 4.X \u2713 \u2713 \u2713 \u2713 3.X \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 2.X \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 >= 1.4.18 \u2713 \u2713 \u2713 1.4.X \u2713 \u2713 Versions 6.0 and later of the Node.js driver require Node.js v16.20.1 or later. The following table describes add-on component version compatibility for\nversions of the MongoDB Node.js driver. Any other combination of packages might be\nunstable. For more information on how to read the compatibility tables, see our guide\nabout  MongoDB Compatibility Tables . Component Node.js Driver v6.x Node.js Driver v5.x Node.js Driver v4.x Node.js Driver v3.x bson ^6.0.0 ^5.0.0 ^4.0.0 ^1.0.0 bson-ext ^4.0.0 ^1.0.0 or ^2.0.0 kerberos ^2.0.1 ^1.0.0 or ^2.0.0 ^1.0.0 or ^2.0.0 ^1.0.0 mongodb-client-encryption ^6.0.0 ^2.3.0 ^1.0.0 or ^2.0.0 ^1.0.0 mongodb-legacy ^6.0.0 ^5.0.0 ^4.0.0 @mongodb-js/zstd ^1.1.0 ^1.0.0 ^1.0.0",
            "code": [],
            "preview": "Find the recommended versions of the Node.js driver that work with your version of MongoDB.",
            "tags": "node.js",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "connection-troubleshooting",
            "title": "Connection Troubleshooting",
            "headings": [
                "Connection Error",
                "Check Your Connection String",
                "Configure Your Firewall",
                "ECONNREFUSED Error",
                "Ensure MongoDB and Your Client Use the Same Protocol",
                "ECONNRESET Error",
                "Control the Number of File Descriptors",
                "Authentication Error",
                "Check Your Connection String",
                "Verify the User Is in the Authentication Database",
                "Error Sending Message",
                "Check the User Permissions",
                "Configure Your Firewall",
                "Check the Number of Connections",
                "Too Many Open Connections",
                "Check the Number of Connections",
                "Timeout Error",
                "Set connectTimeoutMS",
                "Check the Number of Connections"
            ],
            "paragraphs": "This page offers potential solutions to issues you might encounter when\nusing the MongoDB Node.js driver to connect to a MongoDB deployment. This page addresses only connection issues. If you encounter any other issues\nwith MongoDB or the driver, visit the following resources: The  Frequently Asked Questions (FAQ)  for the\nNode.js driver The  Issues & Help  page, which has\ninformation about reporting bugs, contributing to the driver, and\nfinding more resources The  MongoDB Community Forums  for\nquestions, discussions, or general technical support The following error message indicates that the driver cannot connect to a server\non the specified hostname or port. Multiple situations can generate this error\nmessage. In this sample error message, the hostname is  127.0.0.1  and the\nport is  27017 : The following sections describe actions you can take to potentially resolve the\nissue. Verify that the hostname and port number in the connection string are both\naccurate. The default port value for a MongoDB instance is\n 27017 , but you can configure MongoDB to communicate on another port. Verify that the ports your MongoDB deployment listens on are not blocked by a\nfirewall on the same network. MongoDB uses port  27017  by default. To learn\nmore about the default ports MongoDB uses and how to change them, see\n Default MongoDB Port . Do not open a port in your firewall unless you are sure it's the port\nused by your MongoDB deployment. If the connection is refused when the driver attempts to connect to the MongoDB\ninstance, it generates this error message: The following sections describe actions you can take to potentially resolve the\nissue. In Node.js v17 and later, the DNS resolver uses  IPv6  by default when both\nthe client and host support both. For example, if MongoDB uses IPv4 and your\nclient uses IPv6, the driver returns the previous error message. You can configure your MongoDB deployment to use  IPv6  mode when starting\nwith  mongod  or  mongos . For more information about how to specify\n IPv6  mode, see\n IP Binding  in the server\nmanual. As an alternative, you can explicitly use  IPv4  with your client by\nspecifying  family: 4  as an\n option to your MongoClient . If the connection is reset when the driver calls  client.connect() , it\ngenerates this error message: The following section describes a method that may help resolve the issue. A file descriptor is a unique identifier associated with an open process. In most\noperating systems, each open connection from the driver is associated with a\nfile descriptor. Operating systems typically have a limit on the number of file\ndescriptors used by a single process. An  ECONNRESET  error can occur\nif the number of connections exceeds this limit. You can set the maximum number of connections by setting  maxPoolSize . To\nresolve this error, you can decrease the number of maximum allowed connections\nby setting the value of  maxPoolSize . Alternatively, you could increase the\nfile descriptor limit in your operating system. Always be cautious when changing the configuration of your operating system. The Node.js driver can fail to connect to a MongoDB instance if\nthe authorization is not configured correctly. If you are using  SCRAM-SHA-256 \nfor authentication and the driver fails to connect, the driver might raise an\nerror message similar to one of the following messages: The following sections describe actions you can take to potentially resolve the\nissue. An invalid connection string is the most common cause of authentication\nissues when attempting to connect to MongoDB using  SCRAM-SHA-256 . If your connection string contains a username and password, ensure that they\nare in the correct format. If the username or password includes any of the\nfollowing characters, they must be\n percent encoded : The following example shows how to percent encode \"#MyP@assword?\": This results in the following output: For more information about connection strings,\nsee  Connection URI  in the Connection Guide. To successfully authenticate a connection by using a username and password with\n SCRAM-SHA-256 , the username must be defined in the authentication database.\nThe default authentication database is the  admin  database. To use a different\ndatabase for authentication, specify the  authSource  in the connection string.\nThe following example instructs the driver to use  users  as the authentication\ndatabase: You can check if this is the issue by attempting to connect to a MongoDB\ninstance hosted on the local machine with the same code. A deployment on\nthe same machine doesn't require any authorization to connect. When the driver fails to send a command after you make a request,\nit may display the following error message: The following sections describe actions you can take to potentially resolve the\nissue. Verify that you've accessed the MongoDB deployment with the correct user. The\nterm \"message\" in the error can be a command sent by the driver.\nIf you are using a user that doesn't have permissions to send the command, the\ndriver could generate this error. Also ensure that the user has the appropriate permissions for the message you\nare sending. MongoDB uses Role-Based Access Control (RBAC) to control access\nto a MongoDB deployment. For more information about how to configure RBAC in MongoDB,\nsee  Default MongoDB Port . The firewall needs to have an open port for communicating with the MongoDB\ninstance. For more information about configuring the firewall, see\n Configure Your Firewall  in\nthe Connection Error section. Each  MongoClient  instance supports a maximum number of concurrent open\nconnections in its connection pool. You can configure the parameter  maxPoolSize \nwhich defines this limit. The default value is  100 . If there are already a\nnumber of open connections equal to  maxPoolSize , the server waits until\na connection becomes available. If this wait time exceeds the  maxIdleTimeMS \nvalue, the driver responds with an error. For more information about how connection pooling works, see\n How Does Connection Pooling Work in the Node Driver? \nin the FAQ. The driver creates the following error message when it attempts to open a\nconnection, but it's reached the maximum number of connections: The following section describes a method that may help resolve the issue. To create more open connections, increase the value of  maxPoolSize . For more\ninformation about checking the number of connections, see\n Check the Number of Connections \nin the Error Sending Message section. When the network is not able to deliver a request from the driver to the server\nquickly enough, it can time out. When this happens, you might receive an error message\nsimilar to the following message: If you receive this error, try the following action to resolve the\nissue. The driver may hang when it's unable to establish a connection because it\ntakes too long attempting to reach unreachable replica set nodes. You can limit the\ntime the driver spends attempting to establish the connection by using the\n connectTimeMS  setting. To learn more about this setting, see the\n Timeout Options  in\nthe Server manual. Ensure the  connectTimeoutMS  setting is not lower than\nthe highest network latency you have for a member of the set. If one of the\nsecondary members has a latency of 10000 milliseconds, setting the\n connectTimeoutMS  to 9000 prevents the driver from ever connecting to that\nmember. The following example sets  connectTimeoutMS  to 10000 milliseconds. The number of connections to the server may exceed  maxPoolSize . For more\ninformation about checking the number of connections, see\n Check the Number of Connections \nin the Error Sending Message section.",
            "code": [
                {
                    "lang": "none",
                    "value": "Error: couldn't connect to server 127.0.0.1:27017"
                },
                {
                    "lang": "none",
                    "value": "MongoServerSelectionError: connect ECONNREFUSED <IPv6 address>:<port>"
                },
                {
                    "lang": "js",
                    "value": "const client = new MongoClient(uri, {\n  family: 4,\n});"
                },
                {
                    "lang": "none",
                    "value": "MongoServerSelectionError: connect ECONNRESET ::<IP address>:<port>"
                },
                {
                    "lang": "none",
                    "value": "Command failed with error 18 (AuthenticationFailed): 'Authentication\nfailed.' on server <hostname>:<port>."
                },
                {
                    "lang": "none",
                    "value": "connection() error occurred during connection handshake: auth error:\nsasl conversation error: unable to authenticate using mechanism\n\"SCRAM-SHA-256\": (AuthenticationFailed) Authentication failed."
                },
                {
                    "lang": "none",
                    "value": ": / ? # [ ] @"
                },
                {
                    "lang": "javascript",
                    "value": "console.log(encodeURIComponent('#MyP@assword?'));"
                },
                {
                    "lang": "none",
                    "value": "\"%23MyP%40assword%3F\""
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\nconst uri = \"mongodb://<username>:<password>@<hostname>:<port>/?authSource=users\";\nconst client = new MongoClient(uri);"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.MongoSocketWriteException: Exception sending message"
                },
                {
                    "lang": "none",
                    "value": "connection refused because too many open connections"
                },
                {
                    "lang": "none",
                    "value": "timed out while checking out a connection from connection pool: context canceled"
                },
                {
                    "lang": "javascript",
                    "value": "const client = new MongoClient(uri, {\n  connectTimeoutMS: 10000,\n});"
                }
            ],
            "preview": "This page offers potential solutions to issues you might encounter when\nusing the MongoDB Node.js driver to connect to a MongoDB deployment.",
            "tags": "code example, node.js, disconnected, help",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "faq",
            "title": "FAQ",
            "headings": [
                "Why Am I Getting Errors While Connecting to MongoDB?",
                "How Does Connection Pooling Work in the Node Driver?",
                "What Is the Difference Between \"connectTimeoutMS\", \"socketTimeoutMS\" and \"maxTimeMS\"?",
                "What Happens to Running Operations if the Client Disconnects?",
                "How Can I Confirm That the Driver Closed Unusable Sockets?",
                "How Can I Prevent Sockets From Timing Out Before They Become Active?",
                "What Does a Value of \"0\" Mean for \"connectTimeoutMS\" and \"socketTimeoutMS\"?",
                "How Can I Prevent Long-Running Operations From Slowing Down the Server?",
                "What Does the keepAlive Option Do?",
                "What Can I Do If I'm Experiencing Unexpected Network Behavior?",
                "How Can I Prevent a Slow Operation From Delaying Other Operations?",
                "How Can I Ensure my Connection String Is Valid for a Replica Set?"
            ],
            "paragraphs": "This page contains frequently asked questions and their corresponding answers. If you can't find an answer to your problem on this page,\nsee the  Issues & Help  page for next steps and more\nresources. If you have trouble connecting to a MongoDB deployment, see\nthe  Connection Troubleshooting Guide \nfor possible solutions. Every  MongoClient  instance has a built-in connection pool for each server\nin your MongoDB topology. Connection pools open sockets on demand to\nsupport concurrent requests to MongoDB in your application. The maximum size of each connection pool is set by the  maxPoolSize  option, which\ndefaults to  100 . If the number of in-use connections to a server reaches\nthe value of  maxPoolSize , the next request to that server will wait\nuntil a connection becomes available. In addition to the sockets needed to support your application's requests,\neach  MongoClient  instance opens two more sockets per server\nin your MongoDB topology for monitoring the server's state.\nFor example, a client connected to a three-node replica set opens six\nmonitoring sockets. If the application uses the default setting for\n maxPoolSize  and only queries the primary (default) node, then\nthere can be at most  106  total connections in the connection pool. If the\napplication uses a  read preference  to query the\nsecondary nodes, those connection pools grow and there can be\n 306  total connections. To support high numbers of concurrent MongoDB requests\nwithin one process, you can increase  maxPoolSize . Connection pools are rate-limited. The  maxConnecting  option\ndetermines the number of connections that the pool can create in\nparallel at any time. For example, if the value of  maxConnecting  is\n 2 , the third request that attempts to concurrently check out a\nconnection succeeds only when one the following cases occurs: You can set the minimum number of concurrent connections to\neach server with the  minPoolSize  option, which defaults to  0 .\nThe driver initializes the connection pool with this number of sockets. If\nsockets are closed, causing the total number\nof sockets (both in use and idle) to drop below the minimum, more\nsockets are opened until the minimum is reached. You can set the maximum number of milliseconds that a connection can\nremain idle in the pool by setting the  maxIdleTimeMS  option.\nOnce a connection has been idle for  maxIdleTimeMS , the connection\npool removes and replaces it. This option defaults to  0  (no limit). The following default configuration for a  MongoClient  works for most\napplications: MongoClient  supports multiple concurrent requests. For each process,\ncreate a client and reuse it for all operations in a process. This\npractice is more efficient than creating a client for each request. The driver does not limit the number of requests that\ncan wait for sockets to become available, and it is the application's\nresponsibility to limit the size of its pool to bound queuing\nduring a load spike. Requests wait for the amount of time specified in\nthe  waitQueueTimeoutMS  option, which defaults to  0  (no limit). A request that waits more than the length of time defined by\n waitQueueTimeoutMS  for a socket raises a connection error. Use this\noption if it is more important to bound the duration of operations\nduring a load spike than it is to complete every operation. When  MongoClient.close()  is called by any request, the driver\ncloses all idle sockets and closes all sockets that are in\nuse as they are returned to the pool. Calling  MongoClient.close() \ncloses only inactive sockets, so you cannot interrupt or terminate\nany ongoing operations by using this method. The driver closes these\nsockets only when the process completes. The connection pool finishes creating a connection and there are fewer\nthan  maxPoolSize  connections in the pool. An existing connection is checked back into the pool. The driver's ability to reuse existing connections improves due to\nrate-limits on connection creation. To specify the optional settings for your  MongoClient , declare one or\nmore available settings in the  options  object of the constructor as\nfollows: To see all the available settings, see the\n MongoClientOptions \nAPI Documentation. To specify  maxTimeMS , chain the  maxTimeMS()  method with a\ntimeout specification to an operation that returns a  Cursor : Setting Description connectTimeoutMS connectTimeoutMS  is a  connection option  that sets the time, in milliseconds,\nfor an individual connection from your connection pool to\nestablish a TCP connection to the MongoDB Server before\ntiming out. Default:  30000 To modify the allowed time for  MongoClient.connect  to establish a\nconnection to a MongoDB Server, use the  serverSelectionTimeoutMS  option instead. socketTimeoutMS socketTimeoutMS  specifies the amount of time the driver waits\nfor an inactive socket before closing it. The default value is to\nnever time out the socket. This option applies only to sockets that\nhave already been connected. maxTimeMS maxTimeMS \nspecifies the maximum amount of time that the server\nwaits for an operation to complete after it has reached the\nserver. If an operation runs over the specified time limit, it\nreturns a timeout error. You can pass  maxTimeMS  only to an\nindividual operation or to a cursor. Starting in MongoDB Server version 4.2, the server terminates\nrunning operations such as aggregations and find operations if the\nclient disconnects. To see a full list of operations affected by this\nbehavior, see the  Server version 4.2 release notes  in the Server manual. Other operations, such as write operations, continue to run on the\nMongoDB Server even if the client disconnects. This behavior can cause data\ninconsistencies if your application retries the operation after the\nclient disconnects. If you experience unexpected network behavior or if a MongoDB process\nfails with an error, you may not receive confirmation that the\ndriver correctly closed the corresponding socket. To make sure that the driver correctly closes the socket in these cases,\nset the  socketTimeoutMS  option. When a MongoDB process times out, the driver\nwill close the socket. We recommend that you select a value\nfor  socketTimeoutMS  that is two to three times longer than the\nexpected duration of the slowest operation that your application executes. Having a large connection pool does not always reduce reconnection\nrequests. Consider the following example: An application has a connection pool size of 5 sockets and has the\n socketTimeoutMS  option set to 5000 milliseconds. Operations occur,\non average, every 3000 milliseconds, and reconnection requests are\nfrequent. Each socket times out after 5000 milliseconds, which means\nthat all sockets must do something during those 5000 milliseconds to\navoid closing. One message every 3000 milliseconds is not enough to keep the sockets\nactive, so several of the sockets will time out after 5000 milliseconds.\nTo avoid excessive socket timeouts, reduce the number of connections\nthat the driver can maintain in the connection pool by specifying the\n maxPoolSize  option. To specify the optional  maxPoolSize  setting for your  MongoClient , declare\nit in the  options  object of the constructor as follows: If you set the value of  connectTimeoutMS  or  socketTimeoutMS  to\n 0 , your application will use the operating system's default socket\ntimeout value. You can prevent long-running operations from slowing down the server by\nspecifying a timeout value. You can chain the  maxTimeMS()  method to\nan operation that returns a  Cursor  to set a timeout on a specific action. The following example shows how you can chain the  maxTimeMS()  method\nto an operation that returns a  Cursor : The  keepAlive  connection option specifies whether to enable\n Transmission Control Protocol (TCP) keepalives  on a TCP socket. If you enable keepalives,\nthe driver checks whether the connection is active by sending periodic pings\nto your MongoDB deployment. This functionality only works if your\noperating system supports the  SO_KEEPALIVE  socket option. The  keepAliveInitialDelay  option specifies the number of\nmilliseconds that the driver waits before initiating a keepalive. The 5.3 driver version release deprecated these options. Starting in\nversion 6.0 of the driver, the  keepAlive  option is permanently set\nto  true , and the  keepAliveInitialDelay  is set to 300000\nmilliseconds (300 seconds). If your firewall ignores or drops the keepalive messages, you might\nnot be able to identify dropped connections. You might experience unexpected network behavior if the firewall between\nyour application and MongoDB is misconfigured. These firewalls can be\noverly aggressive in their removal of connections, which can lead to\nunexpected errors. Confirm that your firewall exhibits the following behavior: The firewall sends a  FIN  packet when closing a connection,\ninforming the driver that the socket is closed. The firewall allows keepalive messages. To learn more about keepalive messages, see the  What Does the\nkeepAlive Option Do?  FAQ entry. When you use the same  MongoClient  instance to run multiple MongoDB\noperations concurrently, a slow operation can cause delays to other\noperations. Slow operations keep a connection to MongoDB occupied,\nwhich can cause other operations to wait until an additional connection\nbecomes available. If you suspect that slow MongoDB operations are causing delays, you\ncan check the performance of all in-progress operations by using the\nfollowing methods: After you determine which operations are causing delays, try to improve\nthe performance of these operations. Read the  Best Practices\nGuide for MongoDB Performance  for possible solutions. If you implement performance best practices but still\nexperience delays, you can modify your connection settings to increase\nthe size of the connection pool. A connection pool is the group of\nconnections to the server that the driver maintains at any time. To specify the maximum size of a\nconnection pool, you can set the  maxPoolSize  option in the\n connection options  for your\n MongoClient  instance. The default value\nof  maxPoolSize  is  100 . If the number of in-use connections to a\nserver reaches  maxPoolSize , the next operation sent to the server\npauses until a connection to the driver becomes available. The following\ncode sets  maxPoolSize  to  150  when creating a new  MongoClient : Enable the database profiler on your deployment. To learn more, see\n Database Profiler \nin the Server manual. Run the  db.currentOp()  MongoDB Shell command. To learn more, see the\n db.currentOp() \ndocumentation in the Server manual. Enable connection pool monitoring. To learn more, see\n Connection Pool Monitoring . To learn more about connection pooling, see the  How Does Connection\nPooling Work in the Node Driver?  FAQ entry. The connection string passed to the driver must use exact hostnames for\nthe servers as set in the  Replica Set Config .\nGiven the following configuration settings for your Replica Set, in\norder for the Replica Set discovery and  failover  to work, the driver must have access\nto  server1 ,  server2 , and  server3 . If you are unable to find the answer to your question here, try our forums and\nsupport channels listed in the  Issues and Help \nsection.",
            "code": [
                {
                    "lang": "js",
                    "value": "const client = new MongoClient(\"<connection string>\");"
                },
                {
                    "lang": "javascript",
                    "value": "const client = new MongoClient(uri, {\n  connectTimeoutMS: <integer value>,\n  socketTimeoutMS: <integer value>\n});"
                },
                {
                    "lang": "javascript",
                    "value": "const cursor = myColl.find({}).maxTimeMS(50);"
                },
                {
                    "lang": "javascript",
                    "value": "const client = new MongoClient(uri, {\n  maxPoolSize: <integer value>,\n});"
                },
                {
                    "lang": "javascript",
                    "value": "// Execute a find command\nawait collection\n  .find({ $where: \"sleep(100) || true\" })\n  .maxTimeMS(50);\n"
                },
                {
                    "lang": "js",
                    "value": "const client = new MongoClient(uri, { maxPoolSize: 150 });"
                },
                {
                    "lang": "JSON",
                    "value": "{\n  \"_id\": \"testSet\",\n  \"version\": 1,\n  \"protocolVersion\": 1,\n  \"members\": [\n    {\n      \"_id\": 1,\n      \"host\": \"server1:31000\"\n    },\n    {\n      \"_id\": 2,\n      \"host\": \"server2:31001\"\n    },\n    {\n      \"_id\": 3,\n      \"host\": \"server3:31002\"\n    }\n  ]\n}"
                }
            ],
            "preview": "This page contains frequently asked questions and their corresponding answers.",
            "tags": null,
            "facets": {
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/aggregation",
            "title": "Aggregation",
            "headings": [
                "Overview",
                "Analogy",
                "Comparing Aggregation and Query Operations",
                "References",
                "Runnable Examples",
                "Aggregation Example",
                "Additional Examples"
            ],
            "paragraphs": "In this guide, you can learn how to use  aggregation operations  in\nthe MongoDB Node.js driver. Aggregation operations are expressions you can use to produce reduced\nand summarized results in MongoDB. MongoDB's aggregation framework\nallows you to create a pipeline that consists of one or more stages,\neach of which performs a specific operation on your data. You can think of the aggregation pipeline as similar to an automobile factory.\nAutomobile manufacturing requires the use of assembly stations organized\ninto assembly lines. Each station has specialized tools, such as\ndrills and welders. The factory transforms and\nassembles the initial parts and materials into finished products. The  aggregation pipeline  is the assembly line,  aggregation\nstages  are the assembly stations, and  expression operators  are the\nspecialized tools. Using query operations, such as the  find()  method, you can perform the following actions: Using aggregation operations, you can perform the following actions: Aggregation operations have some  limitations : Select  which documents  to return Select  which fields  to return Sort the results Perform all query operations Rename fields Calculate fields Summarize data Group values Returned documents must not violate the  BSON-document size limit \nof 16 megabytes. Pipeline stages have a memory limit of 100 megabytes by default. You can exceed this\nlimit by setting the  allowDiskUse  property of  AggregateOptions  to  true . See\nthe  AggregateOptions API documentation \nfor more details. The  $graphLookup  stage has a strict\nmemory limit of 100 megabytes and will ignore  allowDiskUse . To view a full list of expression operators, see  Aggregation\nOperators  in the Server manual. To learn about assembling an aggregation pipeline and view examples, see\n Aggregation Pipeline  in the\nServer manual. To learn more about creating pipeline stages, see  Aggregation\nStages  in the Server manual. The example uses sample data about restaurants. The following code\ninserts data into the  restaurants  collection of the  aggregation \ndatabase: For more information on connecting to your MongoDB deployment, see the  Connection Guide . To perform an aggregation, pass a list of aggregation stages to the\n collection.aggregate()  method. In the example, the aggregation pipeline uses the following aggregation stages: This example produces the following output: For more information, see the  aggregate() API documentation . A  $match  stage to filter for documents whose\n categories  array field contains the element  Bakery . A  $group  stage to group the matching documents by the  stars \nfield, accumulating a count of documents for each distinct value of  stars . To view step-by-step explanations of common aggregation tasks, see the\n Aggregation Tutorials . You can find another aggregation pipeline example in the  Aggregation\nFramework with Node.js Tutorial \nblog post on the MongoDB website.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const db = client.db(\"aggregation\");\nconst coll = db.collection(\"restaurants\");\n\n// Create sample documents\nconst docs = [\n    { stars: 3, categories: [\"Bakery\", \"Sandwiches\"], name: \"Rising Sun Bakery\" },\n    { stars: 4, categories: [\"Bakery\", \"Cafe\", \"Bar\"], name: \"Cafe au Late\" },\n    { stars: 5, categories: [\"Coffee\", \"Bakery\"], name: \"Liz's Coffee Bar\" },\n    { stars: 3, categories: [\"Steak\", \"Seafood\"], name: \"Oak Steakhouse\" },\n    { stars: 4, categories: [\"Bakery\", \"Dessert\"], name: \"Petit Cookie\" },\n];\n\n// Insert documents into the restaurants collection\nconst result = await coll.insertMany(docs);"
                },
                {
                    "lang": "json",
                    "value": "{ _id: 4, count: 2 }\n{ _id: 3, count: 1 }\n{ _id: 5, count: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Define an aggregation pipeline with a match stage and a group stage\nconst pipeline = [\n    { $match: { categories: \"Bakery\" } },\n    { $group: { _id: \"$stars\", count: { $sum: 1 } } }\n];\n\n// Execute the aggregation\nconst aggCursor = coll.aggregate(pipeline);\n\n// Print the aggregated results\nfor await (const doc of aggCursor) {\n    console.log(doc);\n}"
                }
            ],
            "preview": "In this guide, you can learn how to use aggregation operations in\nthe MongoDB Node.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/authentication/enterprise-mechanisms",
            "title": "Enterprise Authentication Mechanisms",
            "headings": [
                "Kerberos (GSSAPI/SSPI)",
                "LDAP (PLAIN)",
                "MONGODB-OIDC",
                "Azure IMDS",
                "GCP IMDS",
                "Custom Callback",
                "API Documentation"
            ],
            "paragraphs": "In this guide, you can find sample code for connection to MongoDB with each\nauthentication mechanism available in the MongoDB Enterprise Edition:\n Kerberos (GSSAPI/SSPI) ,  LDAP (PLAIN) , and  MONGODB-OIDC . The  GSSAPI  authentication mechanism uses your user principal to\nauthenticate to a Kerberos service. You can specify this authentication mechanism by performing the\nfollowing actions while specifying options on your\n connection string : The following code sample authenticates to Kerberos for UNIX using  GSSAPI . The Node.js driver supports Kerberos on UNIX using the MIT Kerberos library\nand on Windows using the SSPI API. Set the  authMechanism  parameter to  GSSAPI . Set the  SERVICE_NAME  value in the  authMechanismProperties \nparameter if using a value other than  mongodb . Specify a  SERVICE_REALM  value in the  authMechanismProperties \nparameter if a custom service realm is required. Specify a  CANONICALIZE_HOST_NAME  value in the  authMechanismProperties \nparameter if canonicalization of the hostname is required. This property can take\nthe following values: none : (Default) Does not perform hostname canonicalization forward : Performs a forward DNS lookup to canonicalize the hostname forwardAndReverse : Performs a forward DNS lookup and then a\nreverse lookup on that value to canonicalize the hostname The  gssapiServiceName  parameter is deprecated and may be removed\nin future versions of the driver. Use\n authMechanismProperties=SERVICE_NAME:<your service name>  in the\nconnection URI instead.\nSee the\n authMechanismProperties \nparameter documentation for more information. Always  URI encode  the principal using the  encodeURIComponent  method\nto ensure it is correctly parsed. The method refers to the  GSSAPI  authentication mechanism instead\nof  Kerberos  because the driver authenticates through\n GSSAPI RFC-4652 , the SASL\nmechanism. The  PLAIN  authentication mechanism uses your username and password to\nauthenticate to a Lightweight Directory Access Protocol (LDAP) server. You can specify this authentication mechanism by setting the  authMechanism \nparameter to  PLAIN  and including your LDAP username and password in the\n connection string  as shown\nin the following sample code. The authentication mechanism is named  PLAIN  instead of  LDAP  since it\nauthenticates using the  PLAIN Simple Authentication and Security Layer\n(SASL) defined in RFC-4616 . The following sections describe how to use the MONGODB-OIDC authentication mechanism to\nauthenticate from various platforms. For more information about the MONGODB-OIDC authentication mechanism, see\n OpenID Connect Authentication  and\n MongoDB Server Parameters \nin the MongoDB Server manual. The MONGODB-OIDC authentication mechanism requires MongoDB Server v7.0 or later running\non a Linux platform. If your application runs on an Azure VM, or otherwise uses the\n Azure Instance Metadata Service \n(IMDS), you can authenticate to MongoDB by using the Node.js driver's built-in Azure\nsupport. To specify Azure IMDS OIDC as the authentication mechanism, set the following options\nin your connection string: The following code example shows how to set the preceding connection options: username : If you're using an Azure managed identity, set this to the client ID\nof the managed identity. If you're using a service principal to represent an\nenterprise application, set this to the application ID of the service principal.\nOtherwise, omit this option. authMechanism : Set to  MONGODB-OIDC . authMechanismProperties : Set to\n ENVIRONMENT:azure,TOKEN_RESOURCE:<audience> .\nReplace the  <audience>  placeholder with the\nvalue of the  audience  parameter configured on your MongoDB deployment. If your application runs on a Google Compute Engine VM, or otherwise uses the\n GCP Instance Metadata Service ,\nyou can authenticate to MongoDB by using the Node.js driver's built-in GCP\nsupport. To specify GCP IMDS OIDC as the authentication mechanism, set the following options\nin your connection string: The following code example shows how to set the preceding connection options: authMechanism : Set to  MONGODB-OIDC . authMechanismProperties : Set to\n ENVIRONMENT:gcp,TOKEN_RESOURCE:<audience> .\nReplace the  <audience>  placeholder with the\nvalue of the  audience  parameter configured on your MongoDB deployment. The Node.js driver doesn't offer built-in support for all platforms, including\nAzure Functions and Azure Kubernetes Service (AKS). Instead, you\nmust define a custom callback to use OIDC to authenticate from these platforms. First, define a function that retrieves the access token to use for OIDC authentication.\nThis function must have the following signature: The  OIDCCallbackParams  parameter contains the following properties, which you can\naccess inside the function: The callback function must return an  OIDCResponse  object. This object contains the\nfollowing properties: The following example shows a callback function that retrieves an OIDC access token\nfrom a file named  access-token.dat  in the local file system: After you define your callback function, pass it to the  MongoClient  constructor\nas part of the  authMechanismProperties  parameter. The Node.js driver supports\nthe following authentication patterns: Property Value timeoutContext An  AbortSignal  that aborts the authentication workflow after 30 seconds version The current OIDC API version idpInfo The identity-provider information returned from the server username The username included in the connection string, if any refreshToken The refresh token to request a new access token from the issuer, if any Property Value accessToken The access token to use for authentication. expiresInSeconds Optional.  The number of seconds until the access token expires. refreshToken Optional.  The refresh token to request a new access token from the issuer. Machine authentication:  Used by web services and other applications that require\nno human interaction. Select the  Machine Callback  tab to see an example of\nthis syntax. Human authentication:  Used by database tools, command-line utilities, and other\napplications that involve direct human interaction. Select the  Human Callback \ntab to see an example of this syntax. For machine authentication, assign the callback function to the\n authMechanismProperties.OIDC_CALLBACK  property, as shown in the following\nexample: For human authentication, assign the callback function to the\n authMechanismProperties.OIDC_HUMAN_CALLBACK  property, as shown in the following\nexample: To learn more about the methods and types discussed in this\nguide, see the following API documentation: MongoClient OIDCCallbackParams OIDCResponse",
            "code": [
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// specify the placeholder values for your environment in the following lines\nconst clusterUrl = \"<MongoDB cluster URL>\";\nconst principal = encodeURIComponent(\"<Kerberos principal and realm>\");\nconst serviceRealm = \"<Kerberos service realm>\";\nconst canonicalizationSetting = \"<canonicalization setting>\";\nconst authMechanismProperties = `SERVICE_REALM:${serviceRealm},CANONICALIZE_HOST_NAME:${canonicalizationSetting}`;\n\nconst authMechanism = \"GSSAPI\";\n\n// Connection URI\nconst uri = `mongodb+srv://${principal}@${clusterUrl}/?authMechanism=${authMechanism}&authMechanismProperties=${authMechanismProperties}`;\n\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);"
                },
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// specify the placeholder values for your environment in the following lines\nconst clusterUrl = \"<MongoDB cluster URL>\";\nconst ldapUsername = \"<LDAP username>\";\nconst ldapPassword = \"<LDAP password>\";\nconst authMechanism = \"PLAIN\";\n\n// Connection URI\nconst uri = `mongodb+srv://${ldapUsername}:${ldapPassword}@${clusterUrl}/?authMechanism=${authMechanism}`;\n\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);"
                },
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\nconst uri = \"mongodb+srv://<username>@<hostname>:<port>/?authMechanism=MONGODB-OIDC\"\n            + \"&authMechanismProperties=ENVIRONMENT:azure,TOKEN_RESOURCE:<audience>\";\nconst client = new MongoClient(uri);"
                },
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\nconst uri = \"mongodb+srv://<host>:<port>/?authMechanism=MONGODB-OIDC\"\n            + \"&authMechanismProperties=ENVIRONMENT:gcp,TOKEN_RESOURCE:<audience>\";\nconst client = new MongoClient(uri);"
                },
                {
                    "lang": "js",
                    "value": "const myCallback = (params: OIDCCallbackParams): Promise<OIDCResponse> => { }"
                },
                {
                    "lang": "js",
                    "value": "const fs = require(\"node:fs\");\n\nconst myCallback = (params: OIDCCallbackParams): Promise<OIDCResponse> => {\n  const token = fs.readFileSync(\"access-token.dat\", \"utf8\");\n\n  return {\n    accessToken: token,\n    expiresInSeconds: 300,\n    refreshToken: token\n  };\n}"
                },
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\nconst uri = \"mongodb+srv://<host>:<port>/?authMechanism=MONGODB-OIDC\";\nconst client = new MongoClient(uri, {\n  authMechanismProperties: {\n    OIDC_CALLBACK: myCallback\n  }\n});"
                },
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\nconst uri = \"mongodb+srv://<host>:<port>/?authMechanism=MONGODB-OIDC\";\nconst client = new MongoClient(uri, {\n  authMechanismProperties: {\n    OIDC_HUMAN_CALLBACK: myCallback\n  }\n});"
                }
            ],
            "preview": "In this guide, you can find sample code for connection to MongoDB with each\nauthentication mechanism available in the MongoDB Enterprise Edition:\nKerberos (GSSAPI/SSPI), LDAP (PLAIN), and MONGODB-OIDC.",
            "tags": "ldap, encryption, principal, tls",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/authentication/mechanisms",
            "title": "Authentication Mechanisms",
            "headings": [
                "DEFAULT",
                "SCRAM-SHA-256",
                "SCRAM-SHA-1",
                "MONGODB-CR",
                "MONGODB-AWS",
                "X.509",
                "TLS Options"
            ],
            "paragraphs": "In this guide, you can find sample code for connection to MongoDB with each\nauthentication mechanism available in the MongoDB Community Edition:\n DEFAULT ,  SCRAM-SHA-256 ,  SCRAM-SHA-1 ,  MONGODB-CR ,\n MONGODB-AWS , and  X.509 . The  DEFAULT  authentication mechanism is a fallback setting that instructs\nthe driver to negotiate the first authentication mechanism supported by the\nserver in the following order of preference: If the  DEFAULT  option is specified, the driver first attempts to\nauthenticate using  SCRAM-SHA-256 . If the version of the MongoDB instance\ndoes not support that mechanism, the driver attempts to authenticate using\n SCRAM-SHA-1 . If the instance does not support that mechanism either,\nthe driver attempts to authenticate using  MONGODB-CR . You can specify this authentication mechanism by setting the  authMechanism \nparameter to  DEFAULT  in the\n connection string , or by omitting\nthe parameter since it is the default value. Also include your username and\npassword as shown in the code below. For more information on the challenge-response (CR) and salted\nchallenge-response authentication mechanisms (SCRAM) that MongoDB supports,\nsee the  SCRAM  section of the manual. SCRAM-SHA-256 SCRAM-SHA-1 MONGODB-CR Always  URI encode  the username and password using the\n encodeURIComponent  method to ensure they are correctly parsed. SCRAM-SHA-256  is a salted challenge-response authentication mechanism\n(SCRAM) that uses your username and password, encrypted with the  SHA-256 \nalgorithm to authenticate your user. You can specify this authentication mechanism by setting the  authMechanism \nto the value  SCRAM-SHA-256  in the\n connection string  as shown in the\nfollowing sample code. SCRAM-SHA-256  is the default authentication method for MongoDB starting\nin version 4.0 Always  URI encode  the username and password using the\n encodeURIComponent  method to ensure they are correctly parsed. SCRAM-SHA-1  is a salted challenge-response mechanism (SCRAM) that uses your\nusername and password, encrypted with the  SHA-1  algorithm to authenticate\nyour user. You can specify this authentication mechanism by setting the  authMechanism \nparameter to the value  SCRAM-SHA-1  in the\n connection string  as shown\nin the following sample code. SCRAM-SHA-1  is the default authentication method for MongoDB versions\n3.0, 3.2, 3.4, and 3.6. Always  URI encode  the username and password using the\n encodeURIComponent  method to ensure they are correctly parsed. MONGODB-CR  is a challenge-response authentication mechanism that uses your\nusername and password to authenticate your user. You can specify this option by setting the  authMechanism  parameter to value\n MONGODB-CR  in the\n connection string  as shown\nin the following sample code. Always  URI encode  the username and password using the\n encodeURIComponent  method to ensure they are correctly parsed. If you have  upgraded the authentication schema from MONGODB-CR to\nSCRAM , any  MONGODB-CR  user\nauthentication requests fail. The  MONGODB-AWS  authentication mechanism uses your Amazon Web Services\nIdentity and Access Management (AWS IAM) credentials to authenticate your\nuser. If you do not already have the  AWS signature library , use the following\n npm  command to install it: To connect to a MongoDB instance with  MONGODB-AWS  authentication\nenabled, specify the  MONGODB-AWS  authentication mechanism. The driver checks for your credentials in the following sources in order: The MONGODB-AWS authentication mechanism is available only in MongoDB\nversions 4.4 and later. Connection string Environment variables Web identity token file AWS ECS endpoint specified in  AWS_CONTAINER_CREDENTIALS_RELATIVE_URI AWS EC2 endpoint. For more information, see  IAM Roles for Tasks . The driver only reads the credentials from the first method that it detects\nin the order as given by the preceding list. For example, if you specify\nyour AWS credentials in the connection string, the driver ignores any\ncredentials that you specified in environment variables. To connect to your MongoDB instance with a connection string, pass\nyour  AWS_ACCESS_KEY_ID  and  AWS_SECRET_ACCESS_KEY \ncredentials to the driver when you attempt to connect. If your AWS\nlogin requires a session token, include your  AWS_SESSION_TOKEN  as well. The following code shows an example of specifying the  MONGODB-AWS \nauthentication mechanism and credentials with a connection string: Always  URI encode  the username and certificate file path using the\n encodeURIComponent  method to ensure they are correctly parsed. To authenticate to your MongoDB instance using AWS credentials stored in\nenvironment variables, set the following variables by using\na shell: After you've set the preceding environment variables, specify the  MONGODB-AWS \nauthentication mechanism in your connection string as shown in the following example: Omit the line containing  AWS_SESSION_TOKEN  if you don't need an AWS\nsession token for that role. You can use the OpenID Connect (OIDC) token obtained from a web identity\nprovider to authenticate to Amazon Elastic Kubernetes Service (EKS) or\nother services. To authenticate with your OIDC token you must first install\n @aws-sdk/credential-providers . You can\ninstall this dependency using the following  npm  command: Next, create a file that contains your OIDC token. Then\nset the absolute path to this file in an environment variable by using\na shell as shown in the following example: After you've set the preceding environment variable, specify the  MONGODB-AWS \nauthentication mechanism in your connection string as shown in the following example: Starting in version 4.11, when you install the optional\n aws-sdk/credential-providers  dependency, the driver uses the AWS SDK\nto retrieve credentials from the environment. As a result, if you\nhave a shared AWS credentials file or config file, the driver will\nuse those credentials by default. You can override this behavior by performing one of the following\nactions: Set  AWS_SHARED_CREDENTIALS_FILE  variable in your shell to point\nto your credentials file. Set the equivalent environment variable in your application to point\nto your credentials file. Create an AWS profile for your MongoDB credentials and set the\n AWS_PROFILE  environment variable to that profile name. The  X.509  authentication mechanism uses\n TLS  with X.509 certificates to\nauthenticate by retrieving the distinguished name (DN) from the\nclient certificate. You can specify this authentication mechanism by setting the following\nparameters of your  connection string : Pass the location of your client certificate file as the value of\n tlsCertificateKeyFile  as a parameter of the connection URI. The X.509 authentication mechanism is only available in MongoDB versions\n2.6 and later. Set the  authMechanism  parameter to  MONGODB-X509 Set the  tls  parameter to  true Always  URI encode  the certificate file path using the\n encodeURIComponent  method to ensure it is parsed correctly. To learn more about enabling TLS on a connection, see\n Enable TLS on a Connection . The following table describes the TLS options that you can set in a\nconnection URI. Parameter Name Type Default Value Description tls boolean false Specifies whether to enable TLS on the connection. tlsInsecure boolean false Specifies whether to allow invalid certificates and mismatched\nhostnames. When set to  true , this is equivalent to setting\n tlsAllowInvalidCertificates  and  tlsAllowInvalidHostnames  to\n true . tlsCAFile string Path to file that contains a single or bundle of trusted certificate\nauthorities used in a TLS connection. tlsCertificateKeyFile string Path to the client certificate file or the client private key file. If\nboth are required, the two must be concatenated into a single file. tlsCertificateKeyFilePassword buffer or string String or buffer that contains the password to decrypt the client\nprivate key. tlsAllowInvalidCertificates boolean false Specifies whether the driver permits an invalid certificate to be used\nto connect. tlsAllowInvalidHostnames boolean false Specifies whether the driver raises an error when there is a mismatch between the\nserver hostname and TLS certificate hostname.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with values for your environment.\nconst username = encodeURIComponent(\"<username>\");\nconst password = encodeURIComponent(\"<password>\");\nconst clusterUrl = \"<MongoDB cluster url>\";\n\nconst authMechanism = \"DEFAULT\";\n\n// Replace the following with your MongoDB deployment's connection string.\nconst uri =\n  `mongodb+srv://${username}:${password}@${clusterUrl}/?authMechanism=${authMechanism}`;\n\n// Create a new MongoClient\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with values for your environment.\nconst username = encodeURIComponent(\"<username>\");\nconst password = encodeURIComponent(\"<password>\");\nconst clusterUrl = \"<MongoDB cluster url>\";\n\nconst authMechanism = \"SCRAM-SHA-256\";\n\n// Replace the following with your MongoDB deployment's connection string.\nconst uri =\n  `mongodb+srv://${username}:${password}@${clusterUrl}/?authMechanism=${authMechanism}`;\n\n// Create a new MongoClient\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with values for your environment.\nconst username = encodeURIComponent(\"<username>\");\nconst password = encodeURIComponent(\"<password>\");\nconst clusterUrl = \"<MongoDB cluster url>\";\n\nconst authMechanism = \"SCRAM-SHA-1\";\n\n// Replace the following with your MongoDB deployment's connection string.\nconst uri =\n  `mongodb+srv://${username}:${password}@${clusterUrl}/?authMechanism=${authMechanism}`;\n\n// Create a new MongoClient\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with values for your environment.\nconst username = encodeURIComponent(\"<username>\");\nconst password = encodeURIComponent(\"<password>\");\nconst clusterUrl = \"<MongoDB cluster url>\";\n\n// Replace the following with your MongoDB deployment's connection string.\nconst uri =\n  `mongodb+srv://${username}:${password}@${clusterUrl}/?authMechanism=${authMechanism}&tls=true&tlsCertificateKeyFile=${clientPEMFile}`;\n\n// Create a new MongoClient\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "bash",
                    "value": "npm install aws4"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with values for your environment.\nconst accessKeyId = encodeURIComponent(\"<AWS_ACCESS_KEY_ID>\");\nconst secretAccessKey = encodeURIComponent(\"<AWS_SECRET_ACCESS_KEY>\");\nconst clusterUrl = \"<MongoDB cluster url>\";\n\nconst authMechanism = \"MONGODB-AWS\";\n\nlet uri =\n  `mongodb+srv://${accessKeyId}:${secretAccessKey}@${clusterUrl}/?authSource=%24external&authMechanism=${authMechanism}`;\n  \n// Uncomment the following lines if your AWS authentication setup requires a session token.\n// const sessionToken = encodeURIComponent(\"<AWS_SESSION_TOKEN>\");\n// uri = uri.concat(`&authMechanismProperties=AWS_SESSION_TOKEN:${sessionToken}`);\n\n// Create a new MongoClient.\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Establish and verify connection.\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server.\");\n  } finally {\n    // Ensure that the client closes when it finishes/errors.\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "bash",
                    "value": "export AWS_ACCESS_KEY_ID=<awsKeyId>\nexport AWS_SECRET_ACCESS_KEY=<awsSecretKey>\nexport AWS_SESSION_TOKEN=<awsSessionToken>"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Remember to specify your AWS credentials in environment variables.\nconst clusterUrl = \"<MongoDB deployment url>\";\nconst authMechanism = \"MONGODB-AWS\";\n\nlet uri =\n  `mongodb+srv://${clusterUrl}/?authSource=%24external&authMechanism=${authMechanism}`;\n\n// Create a new MongoClient.\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Establish and verify connection.\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server.\");\n  } finally {\n    // Ensure that the client closes when it finishes/errors.\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "bash",
                    "value": "npm install @aws-sdk/credential-providers"
                },
                {
                    "lang": "bash",
                    "value": "export AWS_WEB_IDENTITY_TOKEN_FILE=<absolute path to file containing your OIDC token>"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Remember to specify your AWS credentials in environment variables.\nconst clusterUrl = \"<MongoDB deployment url>\";\nconst authMechanism = \"MONGODB-AWS\";\n\nlet uri =\n  `mongodb+srv://${clusterUrl}/?authSource=%24external&authMechanism=${authMechanism}`;\n\n// Create a new MongoClient.\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Establish and verify connection.\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server.\");\n  } finally {\n    // Ensure that the client closes when it finishes/errors.\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with values for your environment.\nconst clusterUrl = \"<MongoDB cluster url>\";\nconst clientPEMFile = encodeURIComponent(\"<path to the client pem certificate file>\");\n\nconst authMechanism = \"MONGODB-X509\";\n\n// Replace the following with your MongoDB deployment's connection string.\nconst uri =\n  `mongodb+srv://${clusterUrl}/?authMechanism=${authMechanism}&tls=true&tlsCertificateKeyFile=${clientPEMFile}`;\n\n// Create a new MongoClient\nconst client = new MongoClient(uri);\n\n// Function to connect to the server\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully to server\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "In this guide, you can find sample code for connection to MongoDB with each\nauthentication mechanism available in the MongoDB Community Edition:\nDEFAULT, SCRAM-SHA-256, SCRAM-SHA-1, MONGODB-CR,\nMONGODB-AWS, and X.509.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/authentication",
            "title": "Authentication",
            "headings": [
                "Overview"
            ],
            "paragraphs": "These guides show you how to authenticate to a MongoDB instance using the\nNode.js driver. The  Authentication Mechanisms  guide contains\nsample connection code using each authentication mechanism supported in the\nMongoDB Community Edition which includes: The  Enterprise Authentication Mechanisms  guide contains sample\nconnection code using authentication mechanisms available only in MongoDB\nEnterprise Edition which includes: DEFAULT SCRAM-SHA-256 SCRAM-SHA-1 MONGODB-CR MONGODB-AWS X.509 Kerberos (GSSAPI/SSPI) LDAP (PLAIN) MONGODB-OIDC For instructions on MongoDB driver installation and deployment setup, see\nour  Connect to MongoDB guide . Select your\nMongoDB deployment type and the Node.js client.",
            "code": [],
            "preview": "These guides show you how to authenticate to a MongoDB instance using the\nNode.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/bson/undefined-values",
            "title": "Undefined Values",
            "headings": [
                "Overview",
                "Ignore Undefined Values",
                "Set the Scope for Serializing Undefined Values"
            ],
            "paragraphs": "In this guide, you can learn to control how the driver serializes\n undefined  values. By default, the driver serializes  undefined  values\nas  null  values during write operations. To make the driver ignore fields with\n undefined  values during serialization, set the\n ignoreUndefined  setting to  true . When you specify this setting,\nthe driver  does not  serialize fields with  undefined  values. The following example inserts two documents. The first insert operation has\nthe  ignoreUndefined  setting set to  true , so the driver does not\nserialize the  salesTax  field in that operation. The second operation\ninserts a document that has the  salesTax  field with a  null  value: The documents appear in the collection as follows: You can specify the  ignoreUndefined  setting at the following levels: The  ignoreUndefined  setting automatically applies to the scope of the\nobject instance in which you specified it and any other objects created\nfrom that instance. For example, if you set the  ignoreUndefined  setting when\ninstantiating a database object, any collection instance created from\nthat object inherits the setting. Furthermore, any operations that you\ncall on that collection instance also inherit the setting. The following example performs an find-and-update operation that\ninherits the  ignoreUndefined  setting from the  myDB  database\nobject. This operation does not produce any data changes because the\ndriver ignores the  gasTax  field: You can specify the  ignoreUndefined  setting again at any level to\noverride any inherited settings. For example, if you set  ignoreUndefined  to  true  on your\ncollection object, you can override the setting in individual write\noperations that you execute on that collection. The client level The database level The collection level The operation level",
            "code": [
                {
                    "lang": "javascript",
                    "value": "await myColl.insertOne(\n  {\n    state: \"Montana\",\n    salesTax: undefined,\n  },\n  { ignoreUndefined: true }\n);\n\nawait myColl.insertOne({\n  state: \"New Hampshire\",\n  salesTax: undefined,\n});"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  state: \"Montana\",\n},\n{\n  _id: ...,\n  state: \"New Hampshire\",\n  salesTax: null\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"test\", { ignoreUndefined: true });\n\n// The collection inherits the ignoreUndefined setting\nconst myColl = myDB.collection(\"states\");\n\n// Any write operation will not serialize undefined values\nawait myColl.findOneAndUpdate(\n  { state: \"Georgia\" },\n  { $set: { gasTax: undefined } }\n);"
                },
                {
                    "lang": "javascript",
                    "value": "const myColl = myDB.collection(\"states\", { ignoreUndefined: true });\n\n// The insert operation will not serialize undefined values\nawait myColl.insertOne({\n  state: \"South Dakota\",\n  capitalGainsTax: undefined,\n});\n\n// The insert operation will serialize undefined values\nawait myColl.insertOne(\n  { state: \"Texas\", capitalGainsTax: undefined },\n  { ignoreUndefined: false }\n);"
                }
            ],
            "preview": "In this guide, you can learn to control how the driver serializes\nundefined values. By default, the driver serializes undefined values\nas null values during write operations.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/bson/utf8-validation",
            "title": "UTF-8 Validation",
            "headings": [
                "Overview",
                "Specify the UTF-8 Validation Setting",
                "Set the Validation Scope"
            ],
            "paragraphs": "In this guide, you can learn how to enable or disable the Node.js driver's\n UTF-8  validation feature. UTF-8 is a character encoding specification\nthat ensures compatibility and consistent presentation across most operating\nsystems, applications, and language character sets. If you  enable  validation, the driver throws an error when it attempts to\nconvert data that contains invalid UTF-8 characters. The validation adds\nprocessing overhead since it needs to check the data. If you  disable  validation, your application avoids the validation processing\noverhead, but cannot guarantee consistent presentation of invalid UTF-8 data. The driver enables UTF-8 validation by default. It checks documents for any\ncharacters that are not encoded in a valid UTF-8 format when it transfers data\nbetween your application and MongoDB. Read the sections below to learn how to set UTF-8 validation using the\nNode.js driver. The current version of the Node.js driver automatically substitutes\ninvalid UTF-8 characters with alternate valid UTF-8 ones before\nvalidation when you send data to MongoDB. Therefore, the validation\nonly throws an error when the setting is enabled and the driver\nreceives invalid UTF-8 document data from MongoDB. You can specify whether the driver performs UTF-8 validation by\ndefining the  enableUtf8Validation  setting in the options parameter\nwhen you create a client, reference a database or collection, or call a\nCRUD operation. If you omit the setting, the driver enables UTF-8 validation. See the following for code examples that demonstrate how to disable UTF-8\nvalidation on the client, database, collection, or CRUD operation: If your application reads invalid UTF-8 from MongoDB while the\n enableUtf8Validation  option is enabled, it throws a  BSONError  that\ncontains the following message: The  enableUtf8Validation  setting automatically applies to the scope of the\nobject instance on which you included it, and any other objects created by\ncalls on that instance. For example, if you include the option on the call to instantiate a database\nobject, any collection instance you construct from that object inherits\nthe setting. Any operations you call on that collection instance also\ninherit the setting. You can override the setting at any level of scope by including it when\nconstructing the object instance or when calling an operation. For example, if you disable validation on the collection object, you can\noverride the setting in individual CRUD operation calls on that\ncollection.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "// disable UTF-8 validation on the client\nnew MongoClient('<connection uri>', { enableUtf8Validation: false });\n\n// disable UTF-8 validation on the database\nclient.db('<database name>', { enableUtf8Validation: false });\n\n// disable UTF-8 validation on the collection\ndb.collection('<collection name>', { enableUtf8Validation: false });\n\n// disable UTF-8 validation on a specific operation call\nawait myColl.findOne({ title: 'Cam Jansen'}, { enableUtf8Validation: false });"
                },
                {
                    "lang": null,
                    "value": "Invalid UTF-8 string in BSON document"
                },
                {
                    "lang": "javascript",
                    "value": "const database = client.db('books', { enableUtf8Validation: false });\n\n// The collection inherits the UTF-8 validation disabled setting from the database\nconst myColl = database.collection('mystery');\n\n// CRUD operation runs with UTF-8 validation disabled\nawait myColl.findOne({ title: 'Encyclopedia Brown' });"
                },
                {
                    "lang": "javascript",
                    "value": "const collection = database.collection('mystery', { enableUtf8Validation: false });\n\n// CRUD operation runs with UTF-8 validation enabled\nawait myColl.findOne({ title: 'Trixie Belden' }, { enableUtf8Validation: true });\n\n// CRUD operation runs with UTF-8 validation disabled\nawait myColl.findOne({ title: 'Enola Holmes' });"
                }
            ],
            "preview": "In this guide, you can learn how to enable or disable the Node.js driver's\nUTF-8 validation feature. UTF-8 is a character encoding specification\nthat ensures compatibility and consistent presentation across most operating\nsystems, applications, and language character sets.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/bson",
            "title": "BSON Settings",
            "headings": [
                "Overview"
            ],
            "paragraphs": "Learn how to configure your application's BSON serialization settings.\nThe guides in this section describe the following topics: Undefined Values : Control how the\ndriver serializes undefined values UTF-8 Validation : Enable or disable\nthe UTF-8 validation feature",
            "code": [],
            "preview": "Learn how to configure your application's BSON serialization settings.\nThe guides in this section describe the following topics:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/collations",
            "title": "Collations",
            "headings": [
                "Overview",
                "Usage",
                "Collation Parameters",
                "Collation Examples",
                "Set a Default Collation on a Collection",
                "Assign a Collation to an Index",
                "Collation Query Examples",
                "find() and sort() Example",
                "findOneAndUpdate() Example",
                "findOneAndDelete() Example",
                "Aggregation Example"
            ],
            "paragraphs": "Collations are available in MongoDB 3.4 and later. This guide shows you how to use  collations , a set of sorting rules, to\nrun operations using string ordering for specific languages and locales (a\ncommunity or region that shares common language idioms). MongoDB sorts strings using  binary collation  by default. This collation\nmethod uses the  ASCII standard \ncharacter values to compare and order strings. Languages and locales\nhave specific character ordering conventions that differ from the ASCII\nstandard. For example, in Canadian French, the right-most accented character determines\nthe ordering for strings when the other characters are the same. Consider the\nfollowing French words:  cote ,  cot\u00e9 ,  c\u00f4te , and  c\u00f4t\u00e9 . MongoDB sorts them in the following order using the default binary collation: MongoDB sorts them in the following order using the Canadian French collation: You can specify a collation when you create a new collection or new index. You\ncan also specify a collation for  CRUD operations \nand aggregations. When you create a new collection with a collation, you define the default\ncollation for any of the  operations that support collation  called on that\ncollection. You can override the collation for an operation by specifying a\ndifferent one. When you create an index with a collation, you specify the sort order for\noperations that use that index. To use the collation in the index, you\nmust provide a matching collation in the operation, and the operation must\nuse the index. While most index types support collation, the following\ntypes support only binary comparison: Currently, you cannot create a collation on an existing collection. To use\ncollations with an existing collection, create an index with the collation\nand specify the same collation in your operations on it. text 2d geoHaystack The collation object contains the following parameters: You must specify the  locale  field in the collation; all other fields\nare optional. For a complete list of supported locales and the default values\nfor the  locale  fields, see  Supported Languages and Locales .\nFor descriptions of each field, see the  Collation Document MongoDB\nmanual entry . In the following example, we create a new collection called  souvenirs  and\nassign a default collation with the  \"fr_CA\"  locale. The collation applies\nto all  operations that support collation  performed on that\ncollection. Any of the operations that support collations automatically apply the collation\ndefined on the collection. The query below searches the  souvenirs \ncollection and applies the  \"fr_CA\"  locale collation: You can specify a different collation as a parameter in an operation that\nsupports collations. The following query specifies the  \"is\"  Iceland locale\nand  caseFirst  optional parameter with the value  \"upper\" : In the following example, we create a new index on the  title  field of\na collection with a collation set to the  \"en_US\"  locale. The following query uses the index we created: The following queries  do not  use the index that we created. The first\nquery does not include a collation and the second contains a different\nstrength value than the collation on the index. Operations that read, update, and delete documents from a collection can use\ncollations. This section includes examples of a selection of these. See the\nMongoDB manual for a full list of  operations that support collation . The following example calls both  find()  and  sort()  on a collection\nthat uses the default binary collation. We use the German collation by\nsetting the value of the  locale  parameter to  \"de\" . The following example calls the  findOneAndUpdate()  operation on a\ncollection that uses the default binary collation. The collection contains the\nfollowing documents: Consider the following  findOneAndUpdate()  operation on this collection\nwhich  does not  specify a collation: Since \"Gunter\" is the first sorted result when using a binary collation, none\nof the documents come lexically before and match the  $lt  comparison\noperator in the query document. As a result, the operation does not update any\ndocuments. Consider the same operation with a collation specified with the locale set to\n de@collation=phonebook . This locale specifies the  collation=phonebook \noption which contains rules for prioritizing proper nouns, identified by\ncapitalization of the first letter. The  de@collation=phonebook  locale and\noption sorts characters with umlauts before the same characters without\numlauts. Since \"G\u00fcnter\" lexically comes before \"Gunter\" using the\n de@collation=phonebook  collation specified in  findOneAndUpdate() ,\nthe operation returns the following updated document: The following example calls the  findOneAndDelete()  operation on a\ncollection that uses the default binary collation and contains the following\ndocuments: In this example, we set the  numericOrdering  collation parameter to  true \nto sort numeric strings based on their numerical order instead of their\nlexical order. After you run the operation above, the collection contains the following\ndocuments: If you perform the same operation without collation on the original\ncollection of three documents, it matches documents based on the lexical value\nof the strings ( \"16\" ,  \"84\" , and  \"179\" ), and deletes the first\ndocument it finds that matches the query criteria. Since all the documents contain lexical values in the  a  field that\nmatch the criteria (greater than the lexical value of  \"100\" ), the operation\nremoves the first result. After you run the operation above, the collection\ncontains the following documents: To use collation with the  aggregate \noperation, pass the collation document in the options field, after the\narray of pipeline stages. The following example shows an aggregation pipeline on a collection that uses\nthe default binary collation. The aggregation groups the  first_name  field,\ncounts the total number of results in each group, and sorts the results by\nthe German phonebook ( \"de@collation=phonebook\"  locale) order. You can specify only one collation on an aggregation.",
            "code": [
                {
                    "lang": "none",
                    "value": "cote\ncot\u00e9\nc\u00f4te\nc\u00f4t\u00e9"
                },
                {
                    "lang": "none",
                    "value": "cote\nc\u00f4te\ncot\u00e9\nc\u00f4t\u00e9"
                },
                {
                    "lang": "javascript",
                    "value": "collation: {\n  locale: <string>,\n  caseLevel: <bool>,\n  caseFirst: <string>,\n  strength: <int>,\n  numericOrdering: <bool>,\n  alternate: <string>,\n  maxVariable: <string>,\n  backwards: <bool>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "db.createCollection(\"souvenirs\", {\n  collation: { locale: \"fr_CA\" },\n});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({type: \"photograph\"});"
                },
                {
                    "lang": "javascript",
                    "value": " myColl.find({type: \"photograph\"},\n   { collation: { locale: \"is\", caseFirst: \"upper\" } }\n );"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.createIndex(\n  { 'title' : 1 },\n  { 'collation' : { 'locale' : 'en_US' } });"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\"year\": 1980}, {\"collation\" : {\"locale\" : \"en_US\" }})\n  .sort({\"title\": -1});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\"year\": 1980}, {\"collation\" : {\"locale\" : \"en_US\", \"strength\": 2 }})\n  .sort({\"title\": -1});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\"year\": 1980})\n  .sort({\"title\": -1});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({ city: \"New York\" }, { collation: { locale: \"de\" } })\n  .sort({ name: 1 });"
                },
                {
                    "lang": "none",
                    "value": "{ \"_id\" : 1, \"first_name\" : \"Hans\" }\n{ \"_id\" : 2, \"first_name\" : \"Gunter\" }\n{ \"_id\" : 3, \"first_name\" : \"G\u00fcnter\" }\n{ \"_id\" : 4, \"first_name\" : \"J\u00fcrgen\" }"
                },
                {
                    "lang": "none",
                    "value": "{ lastErrorObject: { updatedExisting: true, n: 1 },\n  value: { _id: 3, first_name: 'G\u00fcnter' },\n  ok: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.findOneAndUpdate(\n  { first_name : { $lt: \"Gunter\" } },\n  { $set: { verified: true } }\n);"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.findOneAndUpdate(\n  { first_name: { $lt: \"Gunter\" } },\n  { $set: { verified: true } },\n  { collation: { locale: \"de@collation=phonebook\" } },\n);"
                },
                {
                    "lang": "none",
                    "value": "{ \"_id\" : 1, \"a\" : \"16\" }\n{ \"_id\" : 2, \"a\" : \"84\" }\n{ \"_id\" : 3, \"a\" : \"179\" }"
                },
                {
                    "lang": "none",
                    "value": "{ \"_id\" : 1, \"a\" : \"16\" }\n{ \"_id\" : 2, \"a\" : \"84\" }"
                },
                {
                    "lang": "none",
                    "value": "{ \"_id\" : 2, \"a\" : \"84\" }\n{ \"_id\" : 3, \"a\" : \"179\" }"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.findOneAndDelete(\n  { a: { $gt: \"100\" } },\n  { collation: { locale: \"en\", numericOrdering: true } },\n);"
                },
                {
                    "lang": "javascript",
                    "value": "await myColl.findOneAndDelete({ a: { $gt: \"100\" } });"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.aggregate(\n  [\n    { $group: { \"_id\": \"$first_name\", \"nameCount\": { \"$sum\": 1 } } },\n    { $sort: { \"_id\": 1 } },\n  ],\n  { collation: { locale: \"de@collation=phonebook\" } },\n);"
                }
            ],
            "preview": "Collations are available in MongoDB 3.4 and later.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/connection/connect",
            "title": "Connection Guide",
            "headings": [
                "Connection URI",
                "Atlas Connection Example",
                "Other Ways to Connect to MongoDB",
                "Connect to a MongoDB Server on Your Local Machine",
                "Connect to a Replica Set",
                "Direct Connection"
            ],
            "paragraphs": "This guide shows you how to connect to a\n MongoDB Atlas deployment ,\na MongoDB instance, or a replica set using the Node.js driver. The  connection URI  is the set of instructions that the driver uses to connect to a\nMongoDB deployment. It tells the driver how to connect to MongoDB and how to behave\nwhile connected. The following example shows each part of the connection URI: In this example, we connect to an Atlas MongoDB deployment that has a\nDNS SRV record. For more details, see the\n DNS Seed List Connection Format \ndocumentation. This format offers flexibility in deployment and the\nability to change the servers in rotation without reconfiguring clients. If you are connecting to an instance or replica set that does not have a\nDNS SRV address, you must use  mongodb  for the protocol, which specifies\nthe  Standard Connection String Format . After the protocol, the next part of the connection string contains credentials\nif you are using password-based authentication. Replace the value of  user \nwith your username and  pass  with your password. If you are using an\nauthentication mechanism that does not require a username and password, omit\nthis part of the connection URI. The next part of the connection string specifies the hostname or IP address of\nyour MongoDB instance, followed by the port number. In the example above, we use\n sample.host  as the hostname and  27017  as the port. Replace these values\nto point to your MongoDB instance. The last part of the connection string contains connection and authentication\noptions as parameters. In the example above, we set two connection options:\n maxPoolSize=20  and  w=majority . For more information on connection\noptions, skip to the  Connection Options  section. To learn how to retrieve your connection string in Atlas, see the\n Atlas driver connection guide . You must create a client to connect to a MongoDB deployment on Atlas. To create\na client, construct an instance of  MongoClient , passing in your\nURI and a  MongoClientOptions  object. Use the  serverApi  option in your  MongoClientOptions  object to\nenable the Stable API feature, which forces the server to run operations\nwith behavior compatible with the specified API version. The following code shows how you can specify the connection string and the\nStable API client option when connecting to a MongoDB deployment on Atlas and\nverify that the connection is successful: To learn more about the Stable\nAPI feature, see the  Stable API page . As each  MongoClient  represents a pool of connections to the\ndatabase, most applications only require a single instance of a\n MongoClient , even across multiple requests. To learn more about\nhow connection pools work in the driver, see the  FAQ page . The Node.js driver automatically calls the  MongoClient.connect() \nmethod when using the client to perform CRUD operations on your MongoDB deployment.\nCall the  MongoClient.connect()  method explicitly if you want to verify that the\nconnection is successful. If you are connecting to a single MongoDB Server instance or replica set\nthat is not hosted on Atlas, see the following sections to find out how to\nconnect. To test whether you can connect to your server, replace the connection\nstring in the  Connect to MongoDB  code\nexample and run it. To connect to a MongoDB deployment on your local machine, complete the following\nsteps: After you successfully start your MongoDB Server, specify your connection\nstring in your driver connection code. If your MongoDB Server is running locally, you can use the following\nconnection string: In this connection string,  <port>  is the port number on which you\nconfigured your server to listen for incoming connections. If you want to specify a different hostname or IP address, see our Server\nManual entry on  Connection Strings . Download the  Community \nor  Enterprise  version\nof MongoDB Server. Install and configure  MongoDB Server. Start the server. Always secure your MongoDB Server from malicious attacks. See our\n Security Checklist  for a\nlist of security recommendations. A MongoDB replica set deployment is a group of connected instances that\nstore the same set of data. This configuration of instances provides data\nredundancy and high data availability. To connect to a replica set deployment, specify the hostname and port numbers\nof each instance, separated by a comma, and the replica set name as the value\nof the  replicaSet  parameter in the connection string. When making a connection, the driver takes the following actions by default: Discovers all replica set members when given the address of any one member. Dispatches operations to the appropriate member, such as write against the  primary . To ensure connectivity if one host is unavailable, provide the full\nlist of hosts when connecting to a replica set. To force your operations to run on the host specified in your connection\nURI, you can specify the  directConnection  connection option. If you\nspecify this option, you must use the standard connection URI format. The\ndriver does not accept the DNS seedlist connection format (SRV) when you\nspecify this option. When you specify  directConnection  and connect to a secondary member of the\nreplica set, your write operations fail because the client isn't\nconnected to the primary member. To perform read operations, you must\nenable secondary reads. See the  read preference options \nfor more information.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const { MongoClient, ServerApiVersion } = require(\"mongodb\");\n\n// Replace the placeholder with your Atlas connection string\nconst uri = \"<connection string>\";\n\n// Create a MongoClient with a MongoClientOptions object to set the Stable API version\nconst client = new MongoClient(uri,  {\n        serverApi: {\n            version: ServerApiVersion.v1,\n            strict: true,\n            deprecationErrors: true,\n        }\n    }\n);\n\nasync function run() {\n  try {\n    // Connect the client to the server (optional starting in v4.7)\n    await client.connect();\n\n    // Send a ping to confirm a successful connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Pinged your deployment. You successfully connected to MongoDB!\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "none",
                    "value": "mongodb://localhost:<port>"
                },
                {
                    "lang": "none",
                    "value": "mongodb://host1:27017,host2:27017,host3:27017/?replicaSet=myRs"
                }
            ],
            "preview": "Learn how to connect to a MongoDB Atlas or local MongoDB deployment by using the Node.js driver.",
            "tags": "node.js, code example, connection string, local connection, Stable API, Atlas",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/connection/connection-options",
            "title": "Connection Options",
            "headings": [],
            "paragraphs": "This section explains the MongoDB connection and authentication options\nsupported by the driver. You can pass the connection options as\nparameters of the connection URI to specify the behavior of the client. Name Accepted Values Default Value Description appName string null Specifies the app name the driver passes to the server in the client\nmetadata as part of the connection handshake. The driver sends the\n appName  value to MongoDB when establishing the connection.\nThis value is recorded in the log file, the slow query logs, and\nprofile collections. authMechanism string null Specifies the authentication mechanism method to use for connection to the\nserver. If you do not specify a value, the driver uses the default mechanism,\neither  SCRAM-SHA-1  or  SCRAM-SHA-256  depending on the server version. See\n authentication mechanism  for available\nauthentication mechanisms. authMechanismProperties comma separated key:value pairs, for example, \"opt1:val1,opt2:val2\" null Specifies other options provided for authentication, such as the option to enable\nhostname canonicalization for GSSAPI. authSource string null Specifies the database that connections authenticate against. compressors comma separated list of strings, for example, \"snappy,zlib,zstd\" null Specifies the allowed compression types for wire protocol messages\nsent to or received from the server. See  Network Compression \nfor more information. connectTimeoutMS non-negative integer 30000 Specifies the amount of time, in milliseconds, to wait to establish a single TCP\nsocket connection to the server before raising an error. Specifying\n 0  disables the connection timeout. directConnection boolean false Specifies whether to force dispatch  all  operations to the host\nspecified in the connection URI. enableUtf8Validation boolean true Specifying  true  enables UTF-8 validation for the\nconnection. MongoDB throws an error when\nit attempts to serialize string data that contains invalid\nUTF-8 characters to BSON. This applies to both document keys and\ndocument values, this validation adds processing overhead. Specifying  false  disables UTF-8 validation for the\nconnection. MongoDB does not throw errors when\ndata contains invalid UTF-8 data. If you disable validation,\nyour application avoids the validation processing overhead.\n Editing data  while validation is disabled\ncan result in loss of data. Disabling UTF-8 validation is a\ntemporary workaround to query or export data only. You can also set UTF-8 validation in your  Node.js code . To learn more about UTF-8 characters,\nsee  UTF-8  on Wikipedia. heartbeatFrequencyMS integer greater than or equal to 500 null Specifies the interval, in milliseconds, between regular server monitoring checks. journal boolean null Specifies the journal write concern for the client. See\n write concern  for more information. loadBalanced boolean null Specifies whether the driver is connecting to a load balancer. localThresholdMS non-negative integer 15 Specifies the size of the latency window, in milliseconds, on round trip time for\nselecting between suitable servers. Specifying  0  means no wait,\nmeaning the fastest available server. maxIdleTimeMS non-negative integer 0 Specifies the amount of time, in milliseconds, a connection can be idle before it's closed.\nSpecifying  0  means no minimum. maxPoolSize non-negative integer 100 Specifies the maximum number of clients or connections the driver\ncan create in its connection pool. This count includes connections\nin use. maxConnecting non-negative integer 2 Specifies the maximum number of connections a driver's connection\npool may be establishing concurrently. maxStalenessSeconds -1, or an integer greater than or equal 90 null Specifies the maximum replication lag, in wall clock time, that\na secondary can experience and still be eligible for server selection.\nSpecifying  -1  means no maximum. minPoolSize non-negative integer 0 Specifies the number of connections the driver creates and\nmaintains in the connection pool even when no operations are occurring.\nThis count includes connections in use. proxyHost string null Specifies the SOCKS5 proxy IPv4 address, IPv6 address, or domain\nname. proxyPort non-negative integer null Specifies the TCP port number of the SOCKS5 proxy server. If you\nset the  proxyHost  option, the value of this option defaults\nto  1080 . proxyUsername string null Specifies the username for authentication to the SOCKS5\nproxy server. If you set\nthis option to a zero-length string, the driver ignores it. proxyPassword string null Specifies the password for authentication to the SOCKS5\nproxy server. If you set\nthis option to a zero-length string, the driver ignores it. readConcernLevel string null Specifies the default read concern for the client. See  read concern  for more information. readPreference string \"primary\" Specifies the default read preference for the client (excluding tags). See  read preference  for more information. readPreferenceTags comma-separated key:value pairs, for example, \"dc:ny,rack:1\" and \"dc:ny\ncan be specified multiple times, each instance of this key is a\nseparate tag set null Specifies the default read preference tags for the client. This option is\nvalid only if the read preference mode is not primary. The driver uses the order of the tags in the URI as the order\nfor the read preference. replicaSet string null Specifies the name of the replica set to connect to. retryReads boolean true Enables retryable reads. retryWrites boolean true Enables retryable writes. serverMonitoringMode auto ,  stream ,  poll auto Specifies the monitoring mode that the driver monitors use. When\nthis option is set to  auto , the monitoring mode is determined\nby the environment in which the driver is running. The driver\nuses polling mode in function-as-a-service (FaaS) environments\nand the streaming mode in other environments. serverSelectionTimeoutMS non-negative integer 30000 Specifies the timeout, in milliseconds, to block for server selection\nbefore raising an error. serverSelectionTryOnce boolean true Specifies to scan the topology only once after a server selection\nfailure instead of repeatedly until the server selection times out. socketTimeoutMS non-negative integer 0 Specifies the amount of time, in milliseconds, spent attempting to send or receive on a\nsocket before timing out. Specifying  0  means no timeout. srvMaxHosts non-negative integer 0 Specifies the maximum number of SRV results to randomly select when initially\npopulating the seedlist or, during SRV polling, adding new hosts to the\ntopology. srvServiceName a valid SRV service name according to  RFC 6335 \"mongodb\" Specifies the service name to use for SRV lookup in initial DNS seedlist discovery. ssl boolean false The  ssl  is an alias for the  tls  option. tls boolean false Specifies whether TLS is required for connections to the server.\nUsing a  srvServiceName  of   \"mongodb+srv\" , or specifying other\n tls -prefixed options implicitly sets the value of  tls  to\n true . tlsAllowInvalidCertificates boolean false Specifies whether the driver generates an error when the server's\nTLS certificate is invalid. Set this option to  true  for testing\npurposes only. tlsAllowInvalidHostnames boolean false Specifies whether the driver generates an error when there is a mismatch\nbetween the server's hostname and the hostname specified by the\nTLS certificate. Set this option to  true  for testing purposes only. tlsCAFile string null Specifies the path to a file with either a single or bundle of certificate\nauthorities to trust when making a TLS connection. To learn more\nabout setting this connection option, see the  Provide\nCertificate Filepaths  section of the TLS guide. tlsCertificateKeyFile string null Specifies the path to the client certificate file or the client\nprivate key file. If you need both, you must concatenate the\nfiles. To learn more about setting this connection option, see\nthe  Provide Certificate Filepaths \nsection of the TLS guide. tlsCertificateKeyFilePassword string null Specifies the password to decrypt the client private key to be used\nfor TLS connections. tlsInsecure boolean false Specifies to relax TLS constraints as much as possible, such as\nallowing invalid certificates or hostname mismatches. Set this option\nto  true  for testing purposes only. w non-negative integer or string null Specifies the default write concern  \"w\"  field for the client. waitQueueTimeoutMS non-negative integer 0 Specifies the amount of time, in milliseconds, spent attempting to check out a connection\nfrom a server's connection pool before timing out. wTimeoutMS non-negative integer null Specifies the default write concern timeout field for the client. zlibCompressionLevel integer between  -1  and  9  (inclusive) -1 Specifies the level of compression when using zlib to compress wire\nprotocol messages.  -1  signifies the default level,  0  signifies\nno compression,  1  signifies the fastest speed, and  9  signifies\nthe best compression. See  Network Compression  for more information.",
            "code": [],
            "preview": "This section explains the MongoDB connection and authentication options\nsupported by the driver. You can pass the connection options as\nparameters of the connection URI to specify the behavior of the client.",
            "tags": "node.js, customize",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/connection/network-compression",
            "title": "Network Compression",
            "headings": [
                "Specify Compression Algorithms",
                "Compression Algorithm Dependencies"
            ],
            "paragraphs": "You can enable a driver option to compress messages, which reduces the amount\nof data passed over the network between MongoDB and your application. The driver supports the following compression algorithms: If you specify multiple compression algorithms, the driver selects the\nfirst one in the list supported by your MongoDB instance. Snappy : available in MongoDB 3.6 and later. Zlib : available in MongoDB 3.6 and later. Zstandard : available in MongoDB 4.2 and later. When using the Snappy or Zstandard compression algorithm, you must\n add explicit dependencies . You can enable compression for the connection to your MongoDB instance\nby specifying the algorithms in one of two ways: Specify compression algorithms using the following strings: Adding the parameter to your connection string. Specifying the  compressors  option in your  MongoClientOptions . To enable compression using the connection string, add the\n compressors  parameter in the connection string. You can\nspecify one or more compression algorithms, separating them with\ncommas: To enable compression using the  MongoClientOptions ,\npass the  compressors  option and the compression\nalgorithm you want to use. You can specify one or more compression\nalgorithms, separating them with commas: \"snappy\" for  Snappy  compression \"zlib\" for  Zlib  compression \"zstd\" for  Zstandard  compression To add the Snappy compression algorithm to your application, run the\nfollowing code: To add the Zstandard compression algorithm to your application, run the\nfollowing code:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const uri =\n  \"mongodb+srv://<user>:<password>@<cluster-url>/?compressors=snappy,zlib\";\n\nconst client = new MongoClient(uri);"
                },
                {
                    "lang": "javascript",
                    "value": "const uri =\n  \"mongodb+srv://<user>:<password>@<cluster-url>\";\n\nconst client = new MongoClient(uri,\n  {\n    compressors: [\"snappy\"]\n  });"
                },
                {
                    "lang": "javascript",
                    "value": "npm install --save snappy"
                },
                {
                    "lang": "javascript",
                    "value": "npm install --save @mongodb-js/zstd"
                }
            ],
            "preview": "You can enable a driver option to compress messages, which reduces the amount\nof data passed over the network between MongoDB and your application.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/connection/socks",
            "title": "Enable SOCKS5 Proxy Support",
            "headings": [
                "Overview",
                "Install the socks Package",
                "SOCKS5 Client Options",
                "Example",
                "Additional Information",
                "API Documentation"
            ],
            "paragraphs": "In this guide, you can learn how to connect to MongoDB instances by using\na SOCKS5 proxy. SOCKS5 is a standardized protocol for connecting\nto network services through a proxy server. To learn more about the SOCKS5 protocol, see the Wikipedia entry on\n SOCKS . Starting in version 6.0 of the Node.js driver, you must install\nthe  socks  package to use SOCKS5 proxy support in your\napplication. You can install  socks  by running the following command\nin your shell: You can set options in your  MongoClientOptions  instance or\nin your connection URI to configure SOCKS5 proxy support for\nyour connection. The following table describes the client options\nrelated to SOCKS5: Name Accepted Values Default Value Description proxyHost string null Specifies the SOCKS5 proxy IPv4 address, IPv6 address, or domain\nname. proxyPort non-negative integer null Specifies the TCP port number of the SOCKS5 proxy server. If you\nset the  proxyHost  option, the value of this option defaults\nto  1080 . proxyUsername string null Specifies the username for authentication to the SOCKS5\nproxy server. If you set\nthis option to a zero-length string, the driver ignores it. proxyPassword string null Specifies the password for authentication to the SOCKS5\nproxy server. If you set\nthis option to a zero-length string, the driver ignores it. The driver throws an error if you set the  proxyPort ,\n proxyUsername , or  proxyPassword  options without setting the\n proxyHost  option. This example shows how to instantiate a  MongoClient  that uses SOCKS5\nproxy support. The following example code specifies proxy server options\nand connects to MongoDB: The preceding sample code uses placeholders for the connection URI\nand proxy server details. To run this code, you must replace these\nplaceholders with the information for your deployment and proxy server. For more information about SOCKS5 proxy support, see the\n MongoDB SOCKS5 specification . To learn more about the methods and types discussed in this\nguide, see the following API Documentation: MongoClientOptions MongoClient ProxyOptions",
            "code": [
                {
                    "lang": "bash",
                    "value": "npm i socks"
                },
                {
                    "lang": "javascript",
                    "value": "// Replace the placeholder with your connection string\nconst uri = \"<connection string uri>\";\n\n// Replace the placeholders with your SOCKS5 proxy server details\nconst socksOptions = {\n  proxyHost: \"<host>\",\n  proxyPort: 1080,\n  proxyUsername: \"<username>\",\n  proxyPassword: \"<password>\",\n};\n\n// Create a new client with the proxy server details\nconst client = new MongoClient(uri, socksOptions);"
                }
            ],
            "preview": "In this guide, you can learn how to connect to MongoDB instances by using\na SOCKS5 proxy. SOCKS5 is a standardized protocol for connecting\nto network services through a proxy server.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/connection/tls",
            "title": "Enable TLS on a Connection",
            "headings": [
                "Overview",
                "Enable TLS",
                "Configure Certificates",
                "Reference Certificates in a Client",
                "Create a SecureContext Object to Store Certificates",
                "Provide Certificate Filepaths",
                "Create Buffer Objects to Store Certificates",
                "SecureContext Example",
                "Additional Information",
                "API Documentation"
            ],
            "paragraphs": "In this guide, you can learn how to connect to MongoDB instances with\nthe TLS security protocol. To configure your connection to use TLS, enable\nthe TLS option and provide your certificates for validation. To learn more about TLS, see the Wikipedia entry on\n Transport Layer Security . You can enable TLS on a connection to your MongoDB instance\nin the following ways: In addition to the  tls  client option, the driver provides more\noptions to configure TLS on your connection. For  testing purposes ,\nyou can set the  tlsAllowInvalidHostnames ,\n tlsAllowInvalidCertificates , and  tlsInsecure  client options. Setting the  tlsAllowInvalidHostnames  option to  true  disables\nhostname verification, and setting the\n tlsAllowInvalidCertificates  to  true  disables certificate\nvalidation. Setting the  tlsInsecure  option to  true  disables\nboth certificate and hostname validation. For a full list of client options, see  Connection Options . Setting the  tls  option to  true  in your  MongoClientOptions  object Setting the  tls  option to  true  in your connection string A  MongoClient  instance can connect with TLS if you set  tls \nto  true  in your  MongoClientOptions  object: A  MongoClient  instance can connect with TLS if you set the\n tls  option to  true  in your connection string: If you use a DNS SRV record when connecting to MongoDB by specifying\nthe  +srv  modification in your connection string, you enable\nTLS on your connection by default. To disable it, set the  tls  or  ssl  parameter\nvalue to  false  in your connection string or  MongoClientOptions  object. To learn more about connection behavior when you use a DNS seedlist,\nsee the  SRV Connection Format \nsection in the Server manual. Specifying any of these options in a production environment makes\nyour application insecure and potentially\nvulnerable to expired certificates and to foreign processes posing\nas valid client instances. To successfully initiate a TLS request, an application must prove its\nidentity by referencing cryptographic certificates. To connect to\nMongoDB with TLS, your certificates must be stored as PEM\nfiles. The following list describes the components required to establish\na connection with TLS: For production use, we recommend that your MongoDB deployment use valid\ncertificates generated and signed by the same certificate authority.\nFor testing, you can use self-signed certificates. TLS Component Description Certificate Authority (CA) One or more certificate authorities to\ntrust when making a TLS connection. Client Certificate A digital certificate and key that allow the server to verify the identity\nof your application to establish an encrypted network connection. Certificate Key The client certificate private key file. This key is often\nincluded within the certificate file itself. Passphrase The password to decrypt the private client key if it is encrypted. To learn more about the PEM format, see the Wikipedia entry on\n Privacy-Enhanced Mail . You must reference your certificates in your  MongoClientOptions \nobject so that the server can validate them before the client connects.\nYou can reference your certificates in the following ways: Create a  SecureContext  object to store certificates  (Recommended) Provide filepath strings that point to your certificates Create  Buffer  objects to store certificates We recommend that you use the  secureContext  option to configure\nyour TLS connection.  SecureContext  objects are native to Node.js\nand allow you to keep all your TLS options in a single reusable object. To create a  SecureContext  object, import the  createSecureContext() \nmethod from the  tls  module. Next, call the  createSecureContext() \nmethod and pass the contents of your certificates in the options parameter.\nThis method returns a  SecureContext  object that you can use in your\n MongoClientOptions  object. The following code shows how to create a  SecureContext  object and\npass it to your client: To learn more about the  createSecureContext()  method and the\n tls  package, see the  Node.js TLS API documentation . For a runnable example that uses a  SecureContext  object, see\nthe  SecureContext Example . You can include the filepaths for your certificates as client options to\nretrieve your certificates while connecting with TLS. The driver reads\nthese files when you call the  connect()  method on your\n MongoClient  instance. The following code shows how to provide certificate filepaths as options\nin your  MongoClient : Your TLS configuration might require that you present a certificate\nrevocation list (CRL) when connecting to MongoDB. Starting in version\n6.0 of the driver, you can pass the filepath of your CRL file to the\n tlsCRLFile  option in your connection string or your\n MongoClientOptions  instance. You can pass the contents of your certificate files as  Buffer \nobjects in your client options to connect with TLS. The following code shows how to read the contents of your certificate\nfiles and pass the resulting  Buffer  objects as options in your\n MongoClient : This example shows how to create a  SecureContext  object and\na  MongoClient  instance that includes TLS options. The example\nconnects to MongoDB and executes a find query: For more information about enabling TLS on a connection, see the\nfollowing Server manual documentation: TLS/SSL (Transport Encryption) TLS/SSL Configuration for Clients MongoClientOptions MongoClient tlsAllowInvalidHostnames client option tlsAllowInvalidCertificates client option secureContext client option tlsCAFile client option tlsCertificateKeyFile client option ca client option cert client option key client option",
            "code": [
                {
                    "lang": "js",
                    "value": "const client = new MongoClient(uri, { tls: true });"
                },
                {
                    "lang": "js",
                    "value": "const uri = \"mongodb://<hostname>:<port>?tls=true\";\nconst client = new MongoClient(uri, myClientSettings);"
                },
                {
                    "lang": "js",
                    "value": "// Create a SecureContext object\nconst secureContext = tls.createSecureContext({\n  ca: fs.readFileSync(`<path to CA certificate>`),\n  cert: fs.readFileSync(`<path to public client certificate>`),\n  key: fs.readFileSync(`<path to private client key>`),\n});\n\n// Pass the SecureContext as a client option\nconst client = new MongoClient(uri, { tls: true, secureContext });"
                },
                {
                    "lang": "js",
                    "value": "// Pass filepaths as client options\nconst client = new MongoClient(uri, {\n  tls: true,\n  tlsCAFile: `<path to CA certificate>`,\n  tlsCertificateKeyFile: `<path to private client key>`,\n});"
                },
                {
                    "lang": "js",
                    "value": "// Read file contents\nconst ca = fs.readFileSync(`<path to CA certificate>`);\nconst cert = fs.readFileSync(`<path to public client certificate>`);\nconst key = fs.readFileSync(`<path to private client key>`);\n\n// Pass Buffers as client options\nconst client = new MongoClient(uri, { tls: true, ca, cert, key });"
                },
                {
                    "lang": "js",
                    "value": "import { MongoClient } from \"mongodb\";\nimport * as fs from \"fs\";\nimport * as tls from \"tls\";\n\n// Replace the uri string with your connection string.\nconst uri = \"<connection uri>\";\n\n// Replace the filepaths with your certificate filepaths.\nconst secureContext = tls.createSecureContext({\n  ca: fs.readFileSync(`<path to CA certificate>`),\n  cert: fs.readFileSync(`<path to public client certificate>`),\n  key: fs.readFileSync(`<path to private client key>`),\n});\n\n// Create a client with the secureContext option\nconst client = new MongoClient(uri, { tls: true, secureContext });\n\nasync function run() {\n  try {\n    const db = client.db(\"myDB\");\n    const myColl = db.collection(\"myColl\");\n    const doc = await myColl.findOne({});\n    console.log(doc);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);"
                }
            ],
            "preview": "In this guide, you can learn how to connect to MongoDB instances with\nthe TLS security protocol.",
            "tags": "code example, node.js, security, encrypt",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/connection",
            "title": "Connection",
            "headings": [
                "Overview",
                "Compatibility"
            ],
            "paragraphs": "Learn how to configure your application's connection to a MongoDB\ndeployment using the Node.js driver. In the following sections, you will\nlearn: How to Connect to MongoDB The Available Connection Options How to Enable Network Compression How to Enable TLS on a Connection How to Enable SOCKS5 Proxy Support How to Connect to MongoDB Atlas from AWS Lambda For information about authenticating to MongoDB,\nsee  Authentication  and\n Enterprise Authentication Mechanisms . You can use the Node.js driver to connect and  use the Node.js driver  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  using drivers to connect  for deployments hosted in MongoDB\nAtlas, see  Connect Your Application .",
            "code": [],
            "preview": "Learn how to configure your application's connection to a MongoDB deployment by using the Node.js driver.",
            "tags": "node.js",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/compound-operations",
            "title": "Compound Operations",
            "headings": [
                "Overview",
                "Built-in Methods",
                "includeResultMetadata Option"
            ],
            "paragraphs": "Most database requests either read data from a database or write data into\na database. However, there are instances where you may require a single\noperation that reads and writes data. Compound operations  combine read and write operations\nin a single atomic statement, so there's no chance of data changing in\nbetween a read and a subsequent write. If you execute each operation separately, another request may alter the\ndata between the read and write operations. These data changes may not\nprevent your operation from succeeding, but they can make error handling\nmore difficult. When your application handles potential errors at\nany stage of the process, it can become brittle and difficult\nto test. The Node.js driver provides the following methods to perform compound\noperations: These methods accept an optional  options  object with\nconfigurable  sort  and\n projection  options. You can also set the  includeResultMetadata \noption to specify the return type of each\nof these methods. To learn more about this option, see the\n includeResultMetadata Option \nsection of this guide. The  findOneAndUpdate()  and  findOneAndDelete()  methods take the\n returnDocument  setting, which specifies if the method returns the\npre-update or post-update version of the modified document. findOneAndDelete() findOneAndUpdate() findOneAndReplace() The  includeResultMetadata  option determines the return type of the\ncompound methods. This setting defaults to  false , which means that each method returns the matched\ndocument. If no document is matched, each method returns  null . If you set\n includeResultMetadata  to  true , the method returns a  ModifyResult  type that\ncontains the found document and metadata. Suppose a collection contains only the following document: The following table shows how the value of the\n includeResultMetadata  option changes the return type of\nthe  findOneAndDelete()  method: Option Value Syntax and Output Default:  false Document matched No document matched true",
            "code": [
                {
                    "lang": "json",
                    "value": "{ _id: 1, x: \"on\" }"
                },
                {
                    "lang": "js",
                    "value": "await coll.findOneAndDelete({ x: \"on\" });"
                },
                {
                    "lang": "js",
                    "value": "{ _id: 1, x: 'on' }"
                },
                {
                    "lang": "js",
                    "value": "await coll.findOneAndDelete({ x: \"off\" });"
                },
                {
                    "lang": "js",
                    "value": "null"
                },
                {
                    "lang": "js",
                    "value": "await coll.findOneAndDelete({ x: \"on\" }, { includeResultMetadata: true });"
                },
                {
                    "lang": "js",
                    "value": "{ lastErrorObject: { n: 1 }, value: { _id: 1, x: 'on' }, ok: 1, ... }"
                }
            ],
            "preview": "Most database requests either read data from a database or write data into\na database. However, there are instances where you may require a single\noperation that reads and writes data.",
            "tags": "node.js, atomic operation, read, write",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/query-document",
            "title": "Specify a Query",
            "headings": [
                "Overview",
                "Literal Value Queries",
                "Comparison Operators",
                "Logical Operators",
                "Element Operators",
                "Evaluation Operators"
            ],
            "paragraphs": "Most CRUD operations allow you to narrow the set of matched documents by\nspecifying matching criteria in a  query document . Query documents contain\none or more query operators that apply to specific fields which  determine which\ndocuments to include in the result set. In a query document, you can match fields against literal values, such as\n { title: 'The Room' } , or you can compose\n query operators  to express more\ncomplex matching criteria. In this guide, we cover the following categories\nof query operators in MongoDB and show examples on how to use them: To follow the examples in this guide, use the following code\nsnippet to insert documents that describe fruits into the  myDB.fruits  collection: Comparison Operators Logical Operators Element Operators Evaluation Operators Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . Literal value queries allow you to query for data that exactly matches\na value you provide in the query document. A literal value query has two\nparts: a field name and a value. Documents returned from such a query\nmust contain a field that has exactly the same name as the provided name\nand a value for that field that is exactly the same as the provided\nvalue. The following operation uses a literal query to search for\ndocuments containing a field called \"name\" that has a value of \"apples\": This code snippet returns the following results: Literal value queries are equivalent to the  $eq  comparison\noperator. As a result, the following two queries are equivalent: Comparison operators allow you to query for data based on comparisons\nwith values in a collection. Common comparison operators include\n $gt  for \"greater than\" comparisons,  $lt  for \"less than\" comparisons,\nand  $ne  for \"not equal to\" comparisons. The following operation uses\nthe comparison operator  $gt  to search for documents in which the  qty \nfield value is greater than  5  and prints them out: This code snippet returns the following results: Logical operators allow you to query for data using logic applied to the\nresults of field-level operators. For instance, you can use the  $or \nmethod to query for documents that match either a  $gt  comparison\noperator or a literal value query. The following operation uses the\nlogical operator  $not  to search for documents with a quantity value\nthat is not greater than 5 and prints them out: This code snippet returns the following results: For more information on comparison operators, see the reference manual\nentry for  Comparison Query Operators . Whenever a query document contains multiple elements, those elements\nare combined together with an implicit  $and  logical operator to\nfigure out which documents match the query. As a result, the following\ntwo queries are equivalent: Element operators allow you to query based on the presence, absence, or\ntype of a field. The following operation uses the element operator\n $exists  to search for documents containing the  color \nfield: This code snippet returns the following results: For more information on this operator, see the reference manual entry for\nthe  $exists operator . Evaluation operators allow you to execute higher level logic, like\nregex and text searches, when querying for documents in a collection.\nCommon evaluation operators include  $regex  and  $text .\nThe following operation uses the evaluation operator  $mod  to search\nfor documents in which the  qty  field value is divisible by 3 with\na remainder of 0: This code snippet returns the following results: For more information on this operator, see the reference manual entry for\nthe  $mod operator .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"fruits\");\n\nawait myColl.insertMany([\n  { \"_id\": 1, \"name\": \"apples\", \"qty\": 5, \"rating\": 3 },\n  { \"_id\": 2, \"name\": \"bananas\", \"qty\": 7, \"rating\": 1, \"color\": \"yellow\" },\n  { \"_id\": 3, \"name\": \"oranges\", \"qty\": 6, \"rating\": 2 },\n  { \"_id\": 4, \"name\": \"avocados\", \"qty\": 3, \"rating\": 5 },\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"name\": \"apples\" };\nconst cursor = myColl.find(query);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": 1, \"name\": \"apples\", \"qty\": 5, \"rating\": 3 }"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\n   rating: { $eq: 5 }\n});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\n   rating: 5\n});"
                },
                {
                    "lang": "javascript",
                    "value": "// $gt means \"greater than\"\nconst query = { qty: { $gt : 5 } };\nconst cursor = myColl.find(query);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": 2, \"name\": \"bananas\", \"qty\": 7, \"rating\": 1 }\n{ \"_id\": 3, \"name\": \"oranges\", \"qty\": 6, \"rating\": 2 }"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { qty: { $not: { $gt: 5 }}};\nconst cursor = myColl.find(query);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": 4, \"name\": \"avocados\", \"qty\": 3, \"rating\": 5 }\n{ \"_id\": 1, \"name\": \"apples\", \"qty\": 5, \"rating\": 3 }"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\n  rating: { $eq: 5 },\n  qty: { $gt: 4 }\n});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find({\n  $and: [\n     { rating: { $eq: 5 }},\n     { qty: { $gt: 4 }}\n  ]\n});"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { color: { $exists: true } };\nconst cursor = myColl.find(query);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": 2, \"name\": \"bananas\", \"qty\": 7, \"rating\": 1, \"color\": \"yellow\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// $mod means \"modulo\" and returns the remainder after division\nconst query = { qty: { $mod: [ 3, 0 ] } };\nconst cursor = myColl.find(query);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": 3, \"name\": \"oranges\", \"qty\": 6, \"rating\": 2 }\n{ \"_id\": 4, \"name\": \"avocados\", \"qty\": 3, \"rating\": 5 }"
                }
            ],
            "preview": "Most CRUD operations allow you to narrow the set of matched documents by\nspecifying matching criteria in a query document. Query documents contain\none or more query operators that apply to specific fields which  determine which\ndocuments to include in the result set.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/cursor",
            "title": "Access Data From a Cursor",
            "headings": [
                "Overview",
                "Cursor Paradigms",
                "Asynchronous Iteration",
                "Manual Iteration",
                "Return an Array of All Documents",
                "Stream API",
                "Event API",
                "Cursor Utility Methods",
                "Rewind",
                "Close"
            ],
            "paragraphs": "Read operations that return multiple documents do not immediately return all values\nmatching the query. Because a query can potentially match very large sets of documents,\nthese operations return an object called a cursor, which references documents identified\nby the query. A cursor fetches documents in batches to reduce both memory consumption and\nnetwork bandwidth usage. Cursors are highly configurable and offer multiple interaction\nparadigms for different use cases. The following functions directly return cursors: Other methods such as  Collection.findOne() \nand  Collection.watch()  use\ncursors internally, and return the results of the operations instead of\na cursor. Collection.find() Collection.aggregate() Collection.listIndexes() Collection.listSearchIndexes() Db.aggregate() Db.listCollections() You can use several different  cursor paradigms  to access data.\nMost cursor paradigms allow you to access query results one document at\na time, abstracting away network and caching logic. However, since use\ncases differ, other paradigms offer different access patterns, like\npulling all matching documents into a collection in process memory. Do not combine different cursor paradigms on a single cursor.\nOperations such as  hasNext()  and  toArray() \neach predictably modify the original cursor. If you mix these calls\non a single cursor, you may receive unexpected results. Because asynchronous calls directly modify the cursor, executing\nasynchronous calls on a single cursor simultaneously can also cause\nundefined behavior. Always wait for the previous\nasynchronous operation to complete before running another. When you reach the last result through iteration or through an at-once\nfetch, the cursor is exhausted which means it ceases to respond to methods\nthat access the results. Cursors implement the  AsyncIterator  interface, which\nallows you to use cursors in  for await...of  loops: You can use the  hasNext() \nmethod to check if a cursor can retrieve more data, and then use\nthe  next() \nmethod to retrieve the subsequent element of the cursor: For use cases that require all documents matched by a query to be held\nin memory at the same time, use the  toArray() \nmethod. Note that large numbers of matched documents can cause performance issues\nor failures if the operation exceeds memory constraints. Consider using\nthe  for await...of  syntax to iterate\nthrough results rather than returning all documents at once. Cursors expose the  stream()  method to convert them to Node Readable Streams. These streams operate in  Object\nMode , which passes JavaScript objects rather than Buffers or Strings through the pipeline. As Readable Streams, cursors also support the Event API's\n close ,  data ,  end , and  readable  events: To reset a cursor to its initial position in the set of returned\ndocuments, use  rewind() . Cursors consume memory and network resources both in the client\napplication and in the connected instance of MongoDB. Use\n close() \nto free up a cursor's resources in both the client application\nand the MongoDB Server:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "  const cursor = myColl.find({});\n  console.log(\"async\");\n  for await (const doc of cursor) {\n    console.log(doc);\n  }"
                },
                {
                    "lang": "javascript",
                    "value": "  const cursor = myColl.find({});\n\n  while (await cursor.hasNext()) {\n    console.log(await cursor.next());\n  }"
                },
                {
                    "lang": "javascript",
                    "value": "  const cursor = myColl.find({});\n  const allValues = await cursor.toArray();"
                },
                {
                    "lang": "javascript",
                    "value": "  const cursor = myColl.find({});\n  cursor.stream().on(\"data\", doc => console.log(doc));"
                },
                {
                    "lang": "javascript",
                    "value": "  const cursor = myColl.find({});\n  // the \"data\" event is fired once per document\n  cursor.on(\"data\", data => console.log(data));"
                },
                {
                    "lang": "javascript",
                    "value": "  const cursor = myColl.find({});\n  const firstResult = await cursor.toArray();\n  console.log(\"First count: \" + firstResult.length);\n  await cursor.rewind();\n  const secondResult = await cursor.toArray();\n  console.log(\"Second count: \" + secondResult.length);"
                },
                {
                    "lang": "javascript",
                    "value": "  await cursor.close();"
                }
            ],
            "preview": "Read operations that return multiple documents do not immediately return all values\nmatching the query. Because a query can potentially match very large sets of documents,\nthese operations return an object called a cursor, which references documents identified\nby the query. A cursor fetches documents in batches to reduce both memory consumption and\nnetwork bandwidth usage. Cursors are highly configurable and offer multiple interaction\nparadigms for different use cases.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/distinct",
            "title": "Retrieve Distinct Values",
            "headings": [
                "Overview",
                "Sample Documents",
                "Distinct",
                "Document Field Parameter",
                "Example",
                "Query Parameter",
                "Example",
                "Options Parameter",
                "Example",
                "Additional Information",
                "API Documentation"
            ],
            "paragraphs": "Use the  distinct()  method to retrieve all distinct values for a specified field\nacross a collection. To follow the examples in this guide, use the following code snippet to insert documents\nthat describe restaurants into the  myDB.restaurants  collection: Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . The  distinct()  method requires a document field as a parameter. You can specify the\nfollowing optional parameters to adjust the method output: A  query  parameter to refine your results An  options  parameter to set collation rules Pass the name of the document field to return a list of the field's unique values. The  \"Queens\"  and  \"Manhattan\"  borough values each appear more than\nonce in the sample documents. However, the following example retrieves the\nunique values of the  borough  field: This code outputs the following  borough  values: You can specify a query parameter to return unique values for documents that match\nyour query. Visit  Specify a Query  for more information on constructing a\nquery filter. The following example outputs the distinct values of the  cuisine  field but\nexcludes restaurants in  \"Brooklyn\" : In this case, the query filter matches every borough value except for  \"Brooklyn\" . This\nprevents  distinct()  from outputting one  cuisine  value,  \"Middle Eastern\" .\nThe code outputs the following values: You can specify the collation to the  distinct()  method by defining a\n collation  field as an  options  parameter. This field allows you to set\nregional rules for string ordering and comparisons. See  Collations  for instructions on applying collations. When using the  options  parameter, you must also specify a  query  parameter. If\nyou don't want to use a query filter, define the query as  {} . The following example uses a  collation  field to specify German language ordering\nconventions when outputting the distinct  restaurant  values: In this case, German string ordering conventions place words beginning with \"\u00c4\" before\nthose beginning with \"B\". The code outputs the following: If you do not specify the  collation  field, the output order follows default\nbinary collation rules. These rules place words beginning with \"\u00c4\" after the those\nwith unaccented first letters: For a runnable example of retrieving distinct values, see  Retrieve Distinct Values of a Field . To learn more about the  distinct()  method and its parameters, you can visit the\n API documentation .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"restaurants\");\n\nawait myColl.insertMany([\n  { \"_id\": 1, \"restaurant\": \"White Bear\", \"borough\": \"Queens\", \"cuisine\": \"Chinese\" },\n  { \"_id\": 2, \"restaurant\": \"Via Carota\", \"borough\": \"Manhattan\", \"cuisine\": \"Italian\" },\n  { \"_id\": 3, \"restaurant\": \"Borgatti's\", \"borough\": \"Bronx\", \"cuisine\": \"Italian\" },\n  { \"_id\": 4, \"restaurant\": \"Tanoreen\", \"borough\": \"Brooklyn\", \"cuisine\": \"Middle Eastern\" },\n  { \"_id\": 5, \"restaurant\": \"\u00c4pfel\", \"borough\": \"Queens\", \"cuisine\": \"German\" },\n  { \"_id\": 6, \"restaurant\": \"Samba Kitchen\", \"borough\": \"Manhattan\", \"cuisine\": \"Brazilian\" },\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "// specify \"borough\" as the field to return values for\nconst cursor = myColl.distinct(\"borough\");\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "[ \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\" ]"
                },
                {
                    "lang": "javascript",
                    "value": "// exclude Brooklyn restaurants from the output\nconst query = { borough: { $ne: \"Brooklyn\" }};\n\n// find the filtered distinct values of \"cuisine\"\nconst cursor = myColl.distinct(\"cuisine\", query);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "[ \"Brazilian\", \"Chinese\", \"German\", \"Italian\" ]"
                },
                {
                    "lang": "javascript",
                    "value": "// define an empty query document\nconst query = {};\n// specify German string ordering conventions\nconst options = { collation: { locale: \"de\" }};\n\nconst cursor = myColl.distinct(\"restaurant\", query, options);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "[ \"\u00c4pfel\", \"Borgatti's\", \"Samba Kitchen\", \"Tanoreen\", \"Via Carota\", \"White Bear\" ]"
                },
                {
                    "lang": "json",
                    "value": "[ \"Borgatti's\", \"Samba Kitchen\", \"Tanoreen\", \"Via Carota\", \"White Bear\", \"\u00c4pfel\" ]"
                }
            ],
            "preview": "Use the distinct() method to retrieve all distinct values for a specified field\nacross a collection.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/geo",
            "title": "Search Geospatially",
            "headings": [
                "Overview",
                "Coordinates on an Earth-like Sphere",
                "Coordinates on a 2D Plane",
                "Examples",
                "Query by Proximity",
                "Query Within a Range"
            ],
            "paragraphs": "You can query data based on geographical location using geospatial query\noperators. You can format geospatial queries using one of the following\ncoordinate systems: This section contains examples of geospatial queries using different\nquery operators that you can run against your Atlas sample dataset. Coordinates on an Earth-like Sphere Coordinates on a 2D Plane For geospatial queries using longitude and latitude coordinates\non an Earth-like sphere, use the  GeoJSON \nquery format. While GeoJSON has  multiple types , all GeoJSON data\ntypes use some form of the following structure: The object type determines the number of coordinates. For instance, a\n Point  requires only one coordinate: a longitude and a latitude.\nA  Line  uses two coordinates: a longitude and a latitude for each end.\nA  Polygon  consists of a list of coordinates in which the first and last\ncoordinate are the same, effectively closing the polygon. To learn more\nabout the GeoJSON shapes you can use in MongoDB, consult the\n GeoJSON manual entry . To enable querying GeoJSON data, you must add the field to a  2dsphere \nindex. The following snippet creates an index on the  location.geo  field in\nthe  theaters  collection using the  createIndex()  method: You can also express geospatial queries using  x  and  y  coordinates in\na two-dimensional Euclidean plane. Until MongoDB, this was the only format\ncompatible with geospatial queries, and are now referred to as\n\"legacy coordinate pairs\". Legacy coordinate pairs use the following structure: The field contains an array of two values in which the first represents\nthe  x  axis value and the second represents the  y  axis value. To enable querying using legacy coordinate pairs, create a  2d  index on\nthe field on the collection. The following snippet creates an index on the\n coordinates  field in the  shipwrecks  collection using the\n createIndex()  method: See the\n MongoDB Server manual page on legacy coordinate pairs \nfor more information. Spherical ( 2dsphere ) and flat ( 2d ) indexes support some, but\nnot all, of the same query operators. For a full list of operators\nand their index compatibility, consult the\n manual entry for geospatial queries . The following examples use the MongoDB Atlas sample dataset. You can learn how to set up your own free-tier Atlas cluster and how to load the sample dataset in our\n quick start guide . The examples use the  theaters  collection in the  sample_mflix  database\nfrom the sample dataset. The  theaters  collection contains a  2dsphere  index\non the  location.geo  field. The  $near \noperator accepts a set of longitude-latitude coordinates and returns\ndocuments ordered from nearest to farthest. To limit the results to a\nmaximum distance in meters, use the  $maxDistance  option. For a\ncomplete list of options, see the reference documentation for  $near .\nThe following example queries for theaters within  10,000  meters of\n [ -73.9667, 40.78 ] . The  $geoWithin  operator\nselects documents with geospatial data that exist within a specified\nshape. The following example searches for movie theaters in the New\nEngland area: See the  MongoDB Server manual page on geospatial query operators \nfor more information on the operators you can use in your query.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "<field> : {\n   type: <GeoJSON type>,\n   coordinates: [\n      [longitude_1, latitude_1],\n      ...\n      [longitude_n, latitude_n]\n   ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "db.theaters.createIndex({location.geo: \"2dsphere\"});"
                },
                {
                    "lang": "javascript",
                    "value": "<field> : [ x, y ]"
                },
                {
                    "lang": "javascript",
                    "value": "db.shipwrecks({coordinates: \"2d\"});"
                },
                {
                    "lang": "javascript",
                    "value": "// Find theaters within a certain proximity\nasync function proximity(theaters) {\n  // Define the query to find theaters near a specific location\n  const query = {\n    \"location.geo\": {\n      $near: {\n        $geometry: { type: \"Point\", coordinates: [-73.9667, 40.78] },\n        $maxDistance: 10000,\n      },\n    },\n  };\n  // Find documents based on our query\n  const cursor = theaters.find(query);"
                },
                {
                    "lang": "javascript",
                    "value": "// Find theaters within a specific geographic range\nasync function range(theaters) {\n  // Define the query to find theaters within a specified polygon\n  const query = {\n    \"location.geo\": {\n      $geoWithin: {\n        $geometry: {\n          type: \"Polygon\",\n          coordinates: [\n            [\n              [-72, 40], // Polygon coordinates defining the range\n              [-74, 41],\n              [-72, 39],\n              [-72, 40],\n            ],\n          ],\n        },\n      },\n    },\n  };\n\n  // Find documents based on our query\n  const cursor = theaters.find(query);"
                }
            ],
            "preview": "You can query data based on geographical location using geospatial query\noperators. You can format geospatial queries using one of the following\ncoordinate systems:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/limit",
            "title": "Limit the Number of Returned Results",
            "headings": [
                "Overview",
                "Sample Documents",
                "Limit",
                "Skip"
            ],
            "paragraphs": "Use  limit  to cap the number of documents that can be returned from a\nread operation.  limit  functions as a cap on the maximum number of\ndocuments that the operation can return, but the operation can return\na smaller number of documents if there are not enough documents present\nto reach the limit. If  limit  is used with the\n skip  method, the skip applies\nfirst and the limit only applies to the documents left over after\nthe skip. To follow the examples in this guide, use the following code snippet to insert documents\nthat describe books into the  myDB.books  collection: Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . The following example queries the collection to return the top three\nlongest books. It matches all documents because the query filter is\nempty. Then, it applies a descending  sort  on the  length  field to\nreturn longer books before shorter books and a  limit  to\nreturn only the  3  first results: The code example above outputs the following three documents, sorted by\nlength: You can also apply  sort  and  limit  by specifying them in an\n options  object in your call to the  find()  method. The following two\ncalls are equivalent: For more information on the  options  settings for the  find() \nmethod, see the\n API documentation on find() . The order in which you call  limit  and  sort  does not matter\nbecause the driver reorders the calls to apply the sort first and the\nlimit after it. The following two calls are equivalent: To see the next three books in the results, append the  skip()  method,\npassing the number of documents to bypass as shown below: This operation returns the documents that describe the fourth through sixth\nbooks in order of longest-to-shortest length: You can combine skip and limit in this way to implement paging for your\ncollection, returning only small \"slices\" of the collection at once.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"books\");\n\nawait myColl.insertMany([\n  { \"_id\": 1, \"name\": \"The Brothers Karamazov\", \"author\": \"Dostoyevsky\", \"length\": 824 },\n  { \"_id\": 2, \"name\": \"Les Mis\u00e9rables\", \"author\": \"Hugo\", \"length\": 1462 },\n  { \"_id\": 3, \"name\": \"Atlas Shrugged\", \"author\": \"Rand\", \"length\": 1088 },\n  { \"_id\": 4, \"name\": \"Infinite Jest\", \"author\": \"Wallace\", \"length\": 1104 },\n  { \"_id\": 5, \"name\": \"Cryptonomicon\", \"author\": \"Stephenson\", \"length\": 918 },\n  { \"_id\": 6, \"name\": \"A Dance With Dragons\", \"author\": \"Martin\", \"length\": 1104 },\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "// define an empty query document\nconst query = {};\n// sort in descending (-1) order by length\nconst sort = { length: -1 };\nconst limit = 3;\nconst cursor = myColl.find(query).sort(sort).limit(limit);\nfor await (const doc of cursor) {\n    console.dir;\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 2, \"title\": \"Les Mis\u00e9rables\", \"author\": \"Hugo\", \"length\": 1462 }\n{ \"_id\": 6, \"title\": \"A Dance With Dragons\", \"author\": \"Martin\", \"length\": 1104 }\n{ \"_id\": 4, \"title\": \"Infinite Jest\", \"author\": \"Wallace\", \"length\": 1104 }"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find(query).sort({ length: -1 }).limit(3);\nmyColl.find(query, { sort: { length: -1 }, limit: 3 });"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find(query).sort({ length: -1 }).limit(3);\nmyColl.find(query).limit(3).sort({ length: -1 });"
                },
                {
                    "lang": "javascript",
                    "value": "// define an empty query document\nconst query = {};\n// sort in descending (-1) order by length\nconst sort = { length: -1 };\nconst limit = 3;\nconst skip = 3;\nconst cursor = myColl.find(query).sort(sort).limit(limit).skip(skip);\nfor await (const doc of cursor) {\n    console.dir;\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 3, \"title\": \"Atlas Shrugged\", \"author\": \"Rand\", \"length\": 1088 }\n{ \"_id\": 5, \"title\": \"Cryptonomicon\", \"author\": \"Stephenson\", \"length\": 918 }\n{ \"_id\": 1, \"title\": \"The Brothers Karamazov\", \"author\": \"Dostoyevsky\", \"length\": 824 }"
                }
            ],
            "preview": "Use limit to cap the number of documents that can be returned from a\nread operation. limit functions as a cap on the maximum number of\ndocuments that the operation can return, but the operation can return\na smaller number of documents if there are not enough documents present\nto reach the limit. If limit is used with the\nskip method, the skip applies\nfirst and the limit only applies to the documents left over after\nthe skip.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/project",
            "title": "Specify Which Fields to Return",
            "headings": [
                "Overview",
                "Sample Documents",
                "Single Field",
                "Multiple Fields"
            ],
            "paragraphs": "Use a projection to control which fields appear in the documents\nreturned by read operations. Many requests only require certain fields,\nso projections can help you limit unnecessary network bandwidth usage.\nProjections work in two ways: These two methods of projection are mutually exclusive: if you\nexplicitly include fields, you cannot explicitly exclude fields, and\nvice versa. Explicitly include fields with a value of  1 . This has the\nside-effect of implicitly excluding all unspecified fields. Implicitly exclude fields with a value of  0 . This has the\nside-effect of implicitly including all unspecified fields. To follow the examples in this guide, use the following code snippet to insert documents\nthat describe fruits into the  myDB.fruits  collection: Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . In the following query, pass the projection to only return the  name \nfield of each document: The projection document specifies a value of  1  for  name . This instructs\nthe operation to  include  the  name  field of each returned document in\nthe results and  exclude  the  qty  and  rating  fields. Passing this projection\nto  find()  with an empty query document and no sort document yields the following\nresults: Although this projection only explicitly included the  name  field, the query returned\nthe  _id  field as well. The  _id  field is a special case because it is always included in every query unless\nexplicitly specified otherwise. This is because  _id  is a unique identifier for each\ndocument, a property that is often used when constructing queries. The  movies \ncollection data demonstrates why this property is necessary: two or more movies can share\nthe same title, such as movie remakes. Because of this, you need a unique  _id  value to\nreliably reference a specific movie.  _id  is the only exception to the mutually\nexclusive include-exclude behavior in projections: you  can  explicitly exclude  _id \neven when explicitly including other fields if you do not want  _id  to be present in\nreturned documents. The projection document specifies a value of  1  for  name  and  0  for\n _id . This instructs the operation to  include  the  name  field of each\nreturned document in the results and  exclude  the  _id ,  qty , and  rating \nfields. Passing this projection to  find()  with an empty query document and\nno sort document yields the following results: You can also specify multiple fields to include in your projection. Note: the\norder in which you specify the fields in the projection does not alter the\norder in which they are returned. This example that identifies two fields to include in the projection yields\nthe following results: For more projection examples, see the\n MongoDB Manual page on Project Fields to Return from Query .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"fruits\");\n\nawait myColl.insertMany([\n  { \"_id\": 1, \"name\": \"apples\", \"qty\": 5, \"rating\": 3 },\n  { \"_id\": 2, \"name\": \"bananas\", \"qty\": 7, \"rating\": 1 },\n  { \"_id\": 3, \"name\": \"oranges\", \"qty\": 6, \"rating\": 2 },\n  { \"_id\": 4, \"name\": \"avocados\", \"qty\": 3, \"rating\": 5 },\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "// return only* the name field\nconst projection = { name: 1 };\nconst cursor = myColl.find().project(projection);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 1, \"name\": \"apples\" }\n{ \"_id\": 2, \"name\": \"bananas\" }\n{ \"_id\": 3, \"name\": \"oranges\" }\n{ \"_id\": 4, \"name\": \"avocados\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// return only the name field\nconst projection = { _id: 0, name: 1 };\nconst cursor = myColl.find().project(projection);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"name\": \"apples\" }\n{ \"name\": \"bananas\" }\n{ \"name\": \"oranges\" }\n{ \"name\": \"avocados\" }"
                },
                {
                    "lang": "javascript",
                    "value": "const projection = { _id: 0, rating: 1, name: 1 };\nconst cursor = myColl.find().project(projection);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "  { \"name\": \"apples\", \"rating\": 3 }\n  { \"name\": \"bananas\", \"rating\": 1 }\n  { \"name\": \"oranges\", \"rating\": 2 }\n  { \"name\": \"avocados\", \"rating\": 5 }"
                }
            ],
            "preview": "Use a projection to control which fields appear in the documents\nreturned by read operations. Many requests only require certain fields,\nso projections can help you limit unnecessary network bandwidth usage.\nProjections work in two ways:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/retrieve",
            "title": "Retrieve Data",
            "headings": [
                "Overview",
                "Find Documents",
                "Additional Information",
                "Aggregate Data from Documents",
                "Additional Information",
                "Monitor Data Changes",
                "Additional Information"
            ],
            "paragraphs": "You can perform find operations to retrieve data from your MongoDB database.\nYou can perform a find operation to match documents on a set of criteria\nby calling the  find()  or  findOne()  method. You can also further specify the information that the find operation\nreturns by specifying optional parameters or by chaining other methods,\nas shown in the following guides: You can also use an aggregation operation to retrieve data. This type of\noperation allows you to apply an ordered pipeline of transformations to the\nmatched data. If you want to monitor the database for incoming data that matches a set of\ncriteria, you can use the watch operation to be notified in real-time when\nmatching data is inserted. This page includes a short interactive lab that demonstrates how to\nretrieve data by using the  find()  method. You can complete this lab\ndirectly in your browser window without installing MongoDB or a code editor. To start the lab, click the  Open Interactive Tutorial  button at the\ntop of the page. To expand the lab to a full-screen format, click the\nfull-screen button ( \u26f6 ) in the top-right corner of the lab pane. Sort Results Skip Returned Results Limit the Number of Returned Results Specify Which Fields to Return Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . You can use the Node.js driver to connect and  perform read operations  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  performing read operations in the Atlas UI  for deployments hosted in MongoDB\nAtlas, see  View, Filter, and Sort Documents . You can call the  find()  method on a  Collection  object. The\nmethod accepts a query document that describes the documents you want to\nretrieve. For more information on how to specify your query document,\nsee the  Specify a Query  guide. The  find()  method returns a  Cursor  instance from which you can\naccess the matched documents. The  findOne()  method returns a  Promise \ninstance, which you can resolve to access either the matching document or\na  null  value if there are no matches. To execute a find operation that has no query criteria, you can\npass an empty query or omit the query document in your find\nmethod parameters. The following operations both return all documents in the\n myColl  collection: If you don't pass a query or pass an empty query\nto the  findOne()  method, the operation returns a single\ndocument from a collection. You can specify options in a find operation even when you pass an\nempty query. For example, the following code shows how you can\nspecify a projection as an option while executing a find operation\nthat receives an empty query parameter: For more information about projecting document fields, see the\n Specify Which Fields to Return  guide. A pizza restaurant wants to find all pizzas ordered by Lemony Snicket\nyesterday. They run the following  find()  query on the\n orders  collection: Once the operation returns, the  findResult  variable references a\n Cursor . You can print the documents retrieved using the  for await...of \nsyntax as shown below: The output might resemble the following: For runnable code examples that demonstrate find operations, see the following\nusage examples: For more information about the  findOne()  and  find()  methods, see the\nfollowing Server manual documentation: Find a Document Find Multiple Documents findOne() find() If you want to run a custom processing pipeline to retrieve data from your\ndatabase, you can use the  aggregate()  method. This method accepts\naggregation expressions to run in sequence. These expressions let you filter,\ngroup, and arrange the result data from a collection. A pizza restaurant wants to run a status report on-demand to\nsummarize pizza orders over the past week. They run the following\n aggregate()  query on the  orders  collection to fetch the\ntotals for each distinct \"status\" field: Once the operation returns, the  aggregateResult  variable references a\n Cursor . You can print the documents retrieved using the  for await...of \nsyntax as shown below: The output might resemble the following: For more information on how to construct an aggregation pipeline, see\nthe  Aggregation  guide or  Aggregation Operations \nin the Server manual. You can use the  watch()  method to monitor a collection for changes to\na collection that match certain criteria. These changes include inserted,\nupdated, replaced, and deleted documents. You can pass this method\na pipeline of aggregation commands that sequentially runs on the changed\ndata whenever write operations are executed on the collection. A pizza restaurant wants to receive a notification whenever a new pizza\norder comes in. To accomplish this, they create an aggregation pipeline\nto filter on insert operations and return specific fields. They pass\nthis pipeline to the  watch()  method called on the  orders \ncollection as shown below: For a runnable example of the  watch()  method, see the\n Watch for Changes  usage example.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "myColl.find(); // no query\nmyColl.find({}); // empty query"
                },
                {
                    "lang": "javascript",
                    "value": "const options = {\n  projection: { _id: 0, field1: 1 },\n};\n\nconst findResult = await myColl.findOne({}, options);"
                },
                {
                    "lang": "javascript",
                    "value": "for await (const doc of findResult) {\n  console.log(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "[\n  { name: \"Lemony Snicket\", type: \"horseradish pizza\", qty: 1, status: \"delivered\", date: ... },\n  { name: \"Lemony Snicket\", type: \"coal-fired oven pizza\", qty: 3, status: \"canceled\", date: ...},\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "    // Search for orders by name and within a specific date range\n    const findResult = orders.find({\n      name: \"Lemony Snicket\",\n      date: {\n        $gte: new Date(new Date().setHours(00, 00, 00)),\n        $lt: new Date(new Date().setHours(23, 59, 59)),\n      },\n    });"
                },
                {
                    "lang": "javascript",
                    "value": "for await (const doc of aggregateResult) {\n  console.log(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "[\n  { _id: 'delivering', count: 5 },\n  { _id: 'delivered', count: 37 },\n  { _id: 'created', count: 9 }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "    // Group orders by status within the last week\n    const aggregateResult = orders.aggregate([\n      {\n        $match: {\n          date: {\n            $gte: new Date(new Date().getTime() - 1000 * 3600 * 24 * 7),\n            $lt: new Date(),\n          },\n        },\n      },\n      {\n        $group: {\n          _id: \"$status\",\n          count: {\n            $sum: 1,\n          },\n        },\n      },\n    ]);"
                },
                {
                    "lang": "javascript",
                    "value": "    // Set up a change stream to listen for new order insertions\n    const changeStream = orders.watch([\n      { $match: { operationType: \"insert\" } },\n      {\n        $project: {\n          \"fullDocument.name\": 1,\n          \"fullDocument.address\": 1,\n        },\n      },\n    ]);\n    changeStream.on(\"change\", change => {\n      const { name, address } = change.fullDocument;\n      console.log(`New order for ${name} at ${address}.`);\n    });"
                }
            ],
            "preview": "Learn how to retrieve data, aggregate data, and monitor data changes in MongoDB by using the Node.js driver.",
            "tags": "node.js, code example, find one, find many",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/skip",
            "title": "Skip Returned Results",
            "headings": [
                "Overview",
                "Sample Documents",
                "Example"
            ],
            "paragraphs": "Use  skip  to omit documents from the beginning of the list of\nreturned documents for a read operation. You can combine  skip  with\n sort  to omit the top\n(for descending order) or bottom (for ascending order) results for a\ngiven query. Since the  order of documents returned  is not guaranteed in\nthe absence of a sort, using  skip  without using  sort  omits\narbitrary documents. If the value of  skip  exceeds the number of matched documents for\na query, then that query returns no documents. To follow the examples in this guide, use the following code snippet to insert documents\nthat describe fruits into the  myDB.fruits  collection: Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . In the following example, we query the collection with a filter that\nmatches all the documents and pass options that specifies  sort  and\n skip  commands as query options. The sort option specifies that fruit\ndocuments that have higher  rating  values are returned before ones with lower\nratings. The skip option specifies that the first 2 documents are\nomitted from the result: Since we specified that query skip the first  2  documents, the third and fourth highest\nrating documents are printed by the code snippet above: The  sort  and  skip  options can also be specified as methods chained to\nthe  find  method. The following two commands are equivalent:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"fruits\");\n\nawait myColl.insertMany([\n  { \"_id\": 1, \"name\": \"apples\", \"qty\": 5, \"rating\": 3 },\n  { \"_id\": 2, \"name\": \"bananas\", \"qty\": 7, \"rating\": 1 },\n  { \"_id\": 3, \"name\": \"oranges\", \"qty\": 6, \"rating\": 2 },\n  { \"_id\": 4, \"name\": \"avocados\", \"qty\": 3, \"rating\": 5 },\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "// define an empty query document\nconst query = {};\nconst options = {\n   // sort in descending (-1) order by rating\n   sort : { rating: -1 },\n   // omit the first two documents\n   skip : 2,\n}\n\nconst cursor = myColl.find(query, options);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 3, \"name\": \"oranges\", \"qty\": 6, \"rating\": 2 }\n{ \"_id\": 2, \"name\": \"bananas\", \"qty\": 7, \"rating\": 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.find(query, { sort: { rating: -1}, skip: 2});\nmyColl.find(query).sort({rating: -1}).skip(2);"
                }
            ],
            "preview": "Use skip to omit documents from the beginning of the list of\nreturned documents for a read operation. You can combine skip with\nsort to omit the top\n(for descending order) or bottom (for ascending order) results for a\ngiven query. Since the order of documents returned is not guaranteed in\nthe absence of a sort, using skip without using sort omits\narbitrary documents.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/sort",
            "title": "Sort Results",
            "headings": [
                "Overview",
                "Sample Documents",
                "Example"
            ],
            "paragraphs": "Use  sort  to change the order in which read operations return\ndocuments.  Sort  tells MongoDB to order returned documents by the\nvalues of one or more fields in a certain direction. To sort returned\ndocuments by a field in ascending (lowest first) order, use a value of\n 1 . To sort in descending (greatest first) order instead, use  -1 .\nIf you do not specify a sort, MongoDB does not guarantee the order of\nquery results. Follow the instructions in the examples below to insert data into\nthe  myDB.books  collection and perform a sort on the results of a query.\nConsider a collection containing documents that describe books. To\ninsert this data into a collection, run the following operation: Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . Pass the following sort document to a read operation to ensure that the\noperation returns books with longer lengths before books with shorter\nlengths: In this case, the number  -1  tells the read operation to sort the\nbooks in descending order by length.  find()  returns the following\ndocuments when this sort is used with an empty query: Sometimes, the order of two or more documents is ambiguous using a\nspecified sort. In the above case, both \"A Dance with Dragons\" and\n\"Infinite Jest\" have  1104  pages, so the order in which they are\nreturned is not guaranteed. To resolve ties in your sorted results in a\nrepeatable way, add more fields to the sort document: With the addition of the  author  field to the sort document, the read operation sorts\nmatching documents first by  length  then, if there is a tie, by  author . Matched\ndocument fields are compared in the same order as fields are specified in the sort\ndocument.  find()  returns the following ordering of documents when this sort is used on\nthe documents matching the query, sorting  \"Martin\"  before  \"Wallace\"  for the two books with\nthe same length:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"books\");\n\nawait myColl.insertMany([\n  { \"_id\": 1, \"name\": \"The Brothers Karamazov\", \"author\": \"Dostoyevsky\", \"length\": 824 },\n  { \"_id\": 2, \"name\": \"Les Mis\u00e9rables\", \"author\": \"Hugo\", \"length\": 1462 },\n  { \"_id\": 3, \"name\": \"Atlas Shrugged\", \"author\": \"Rand\", \"length\": 1088 },\n  { \"_id\": 4, \"name\": \"Infinite Jest\", \"author\": \"Wallace\", \"length\": 1104 },\n  { \"_id\": 5, \"name\": \"Cryptonomicon\", \"author\": \"Stephenson\", \"length\": 918 },\n  { \"_id\": 6, \"name\": \"A Dance with Dragons\", \"author\": \"Martin\", \"length\": 1104 },\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "// define an empty query document\nconst query = {};\n// sort in descending (-1) order by length\nconst sort = { length: -1 };\nconst cursor = myColl.find(query).sort(sort);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 2, \"title\": \"Les Mis\u00e9rables\", \"author\": \"Hugo\", \"length\": 1462 }\n{ \"_id\": 4, \"title\": \"Infinite Jest\", \"author\": \"Wallace\", \"length\": 1104 }\n{ \"_id\": 6, \"title\": \"A Dance with Dragons\", \"author\": \"Martin\", \"length\": 1104 }\n{ \"_id\": 3, \"title\": \"Atlas Shrugged\", \"author\": \"Rand\", \"length\": 1088 }\n{ \"_id\": 5, \"title\": \"Cryptonomicon\", \"author\": \"Stephenson\", \"length\": 918 }\n{ \"_id\": 1, \"title\": \"The Brothers Karamazov\", \"author\": \"Dostoyevsky\", \"length\": 824 }"
                },
                {
                    "lang": "javascript",
                    "value": "// define an empty query document\nconst query = {};\n// sort in ascending (1) order by length\nconst sort = { length: 1, author: 1 };\nconst cursor = myColl.find(query).sort(sort);\nfor await (const doc of cursor) {\n  console.dir(doc);\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 1, \"title\": \"The Brothers Karamazov\", \"author\": \"Dostoyevsky\", \"length\": 824 }\n{ \"_id\": 5, \"title\": \"Cryptonomicon\", \"author\": \"Stephenson\", \"length\": 918 }\n{ \"_id\": 3, \"title\": \"Atlas Shrugged\", \"author\": \"Rand\", \"length\": 1088 }\n{ \"_id\": 6, \"title\": \"A Dance with Dragons\", \"author\": \"Martin\", \"length\": 1104 }\n{ \"_id\": 4, \"title\": \"Infinite Jest\", \"author\": \"Wallace\", \"length\": 1104 }\n{ \"_id\": 2, \"title\": \"Les Mis\u00e9rables\", \"author\": \"Hugo\", \"length\": 1462 }"
                }
            ],
            "preview": "Use sort to change the order in which read operations return\ndocuments. Sort tells MongoDB to order returned documents by the\nvalues of one or more fields in a certain direction. To sort returned\ndocuments by a field in ascending (lowest first) order, use a value of\n1. To sort in descending (greatest first) order instead, use -1.\nIf you do not specify a sort, MongoDB does not guarantee the order of\nquery results.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations/text",
            "title": "Search Text",
            "headings": [
                "Overview",
                "Examples",
                "Query for Words",
                "Query By Phrase",
                "Query with Negations",
                "Sort by Relevance"
            ],
            "paragraphs": "Text searches let you search string type fields in your collection for specified words or\nphrases. You can perform a text search by using the  $text  operator, which performs a\nlogical  OR  on each term separated by a space in the search string. You can also\nspecify more options to the operator to handle case sensitivity, stop words, and word\nstemming (such as plural forms or other tenses) for a supported language.\nThis is often used for unstructured text such as transcripts, essays, or web pages. The  $text  query operator requires that you specify the search field in\na  text index  on your collection. See the examples below for sample\ncode for creating a text index and using the  $text  query operator. Atlas Search  helps you build fast,\nrelevance-based search capabilities on top of your MongoDB data. Try it today on\n MongoDB Atlas , our\nfully managed database as a service. The following examples use sample data from the  movies  collection in the\n sample_mflix  database. To enable text searches on the  title   field, create a\n text index  by using the following command: We use a single field text index for the examples in this guide, but you can\ncreate a compound text index that broadens your text queries to multiple\nfields. The following command creates a text index on two fields in the\n movies  collection: You can only create  one  text index per collection. Every text search\nqueries all the fields specified in that index for matches. To learn more about text indexes, see  Text Indexes  in the Server manual. When creating a compound text index, you can specify a weight option to\nprioritize certain text fields in your index. When you execute a text\nsearch, the field weights influence how MongoDB calculates the\n text search score  for each matching\ndocument. To learn more about specifying field weights when creating a text\nindex, see the  Text Indexes \nsection in the Indexes guide. This example queries for Star Trek movies by searching for titles\ncontaining the word \"trek\". If you want to query using multiple words,\nseparate your words with spaces to query for documents that match any of\nthe search terms (logical  OR ). This operation returns the following documents: Success! The query found every document in the  movies  collection\nwith a title including the word \"trek\". Unfortunately, the search included\none unintended item: \"Trek Nation,\" which is a movie about Star Trek and not\npart of the Star Trek movie series. To solve this, we can query with a more\nspecific  phrase . To make your query more specific, try using the phrase \"star trek\"\ninstead of just the word \"trek\". To search by phrase, surround your\nmulti-word phrase with escaped quotes ( \\\"<term>\\\" ): Querying by the phrase  \"star trek\"  instead of just the term  \"trek\" \nmatches the following documents: These results include all movies in the database that contain the phrase\n \"star trek\" , which in this case results in only fictional Star Trek\nmovies. Unfortunately, this query returned  \"Star Trek Into\nDarkness\" , a movie that was not part of the original series of movies. To\nresolve this issue, we can omit that document with a  negation . To use a negated term, place a negative sign,  - , in front of the term\nyou to omit from the result set. The query operation omits any\ndocuments that contain this term from the search result. Since this query\nincludes two distinct terms, separate them with a space. Querying with the negated term yields the following documents: Your query operation may return a reference to a\ncursor that contains matching documents. To learn how to\nexamine data stored in the cursor, see the\n Cursor Fundamentals page . Now that the result set reflects the desired results, you can use the\ntext search  textScore , accessed using the  $meta  operator in the query\nprojection, to order the results by relevance: Querying in this way returns the following documents in the following\norder. In general, text relevance increases as a string matches more\nterms and decreases as the unmatched portion of the string lengthens. For more information about the $text operator and its options, see the\n manual entry .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "db.movies.createIndex({ title: \"text\" });"
                },
                {
                    "lang": "javascript",
                    "value": "db.movies.createIndex({ title: \"text\", plot: \"text\" });"
                },
                {
                    "lang": "javascript",
                    "value": "{ title: 'Trek Nation' }\n{ title: 'Star Trek' }\n{ title: 'Star Trek Into Darkness' }\n{ title: 'Star Trek: Nemesis' }\n{ title: 'Star Trek: Insurrection' }\n{ title: 'Star Trek: Generations' }\n{ title: 'Star Trek: First Contact' }\n{ title: 'Star Trek: The Motion Picture' }\n{ title: 'Star Trek VI: The Undiscovered Country' }\n{ title: 'Star Trek V: The Final Frontier' }\n{ title: 'Star Trek IV: The Voyage Home' }\n{ title: 'Star Trek III: The Search for Spock' }\n{ title: 'Star Trek II: The Wrath of Khan' }"
                },
                {
                    "lang": "javascript",
                    "value": "  // Create a query that searches for the string \"trek\"\n  const query = { $text: { $search: \"trek\" } };\n\n  // Return only the `title` of each matched document\n  const projection = {\n    _id: 0,\n    title: 1,\n  };\n\n  // Find documents based on our query and projection\n  const cursor = movies.find(query).project(projection);"
                },
                {
                    "lang": "javascript",
                    "value": "{ title: 'Star Trek' }\n{ title: 'Star Trek Into Darkness' }\n{ title: 'Star Trek: Nemesis' }\n{ title: 'Star Trek: Insurrection' }\n{ title: 'Star Trek: Generations' }\n{ title: 'Star Trek: First Contact' }\n{ title: 'Star Trek: The Motion Picture' }\n{ title: 'Star Trek VI: The Undiscovered Country' }\n{ title: 'Star Trek V: The Final Frontier' }\n{ title: 'Star Trek IV: The Voyage Home' }\n{ title: 'Star Trek III: The Search for Spock' }\n{ title: 'Star Trek II: The Wrath of Khan' }"
                },
                {
                    "lang": "javascript",
                    "value": "  // Create a query that searches for the phrase \"star trek\"\n  const query = { $text: { $search: \"\\\"star trek\\\"\" } };\n\n  // Return only the `title` of each matched document\n  const projection = {\n    _id: 0,\n    title: 1,\n  };\n\n  // Find documents based on the query and projection\n  const cursor = movies.find(query).project(projection);"
                },
                {
                    "lang": "javascript",
                    "value": "{ title: 'Star Trek' }\n{ title: 'Star Trek: Nemesis' }\n{ title: 'Star Trek: Insurrection' }\n{ title: 'Star Trek: Generations' }\n{ title: 'Star Trek: First Contact' }\n{ title: 'Star Trek: The Motion Picture' }\n{ title: 'Star Trek VI: The Undiscovered Country' }\n{ title: 'Star Trek V: The Final Frontier' }\n{ title: 'Star Trek IV: The Voyage Home' }\n{ title: 'Star Trek III: The Search for Spock' }\n{ title: 'Star Trek II: The Wrath of Khan' }"
                },
                {
                    "lang": "javascript",
                    "value": "  // Create a query that searches for the phrase \"star trek\" while omitting \"into darkness\"\n  const query = { $text: { $search: \"\\\"star trek\\\"  -\\\"into darkness\\\"\" } };\n\n  // Include only the `title` field of each matched document\n  const projection = {\n    _id: 0,\n    title: 1,\n  };\n\n  // Find documents based on the query and projection\n  const cursor = movies.find(query).project(projection);"
                },
                {
                    "lang": "javascript",
                    "value": "{ title: 'Star Trek', score: 1.5 }\n{ title: 'Star Trek: Generations', score: 1.3333333333333333 }\n{ title: 'Star Trek: Insurrection', score: 1.3333333333333333 }\n{ title: 'Star Trek: Nemesis', score: 1.3333333333333333 }\n{ title: 'Star Trek: The Motion Picture', score: 1.25 }\n{ title: 'Star Trek: First Contact', score: 1.25 }\n{ title: 'Star Trek II: The Wrath of Khan', score: 1.2 }\n{ title: 'Star Trek III: The Search for Spock', score: 1.2 }\n{ title: 'Star Trek IV: The Voyage Home', score: 1.2 }\n{ title: 'Star Trek V: The Final Frontier', score: 1.2 }\n{ title: 'Star Trek VI: The Undiscovered Country', score: 1.2 }"
                },
                {
                    "lang": "javascript",
                    "value": "  // Create a query that searches for the phrase \"star trek\" while omitting \"into darkness\"r\n  const query = { $text: { $search: \"\\\"star trek\\\"  -\\\"into darkness\\\"\" } };\n\n  // Sort returned documents by descending text relevance score\n  const sort = { score: { $meta: \"textScore\" } };\n\n  // Include only the `title` and `score` fields in each returned document\n  const projection = {\n    _id: 0,\n    title: 1,\n    score: { $meta: \"textScore\" },\n  };\n\n  // Find documents based on the query, sort, and projection\n  const cursor = movies\n    .find(query)\n    .sort(sort)\n    .project(projection);"
                }
            ],
            "preview": "Text searches let you search string type fields in your collection for specified words or\nphrases. You can perform a text search by using the $text operator, which performs a\nlogical OR on each term separated by a space in the search string. You can also\nspecify more options to the operator to handle case sensitivity, stop words, and word\nstemming (such as plural forms or other tenses) for a supported language.\nThis is often used for unstructured text such as transcripts, essays, or web pages.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-operations",
            "title": "Read Operations",
            "headings": [],
            "paragraphs": "Retrieve Data Access Data From a Cursor Retrieve Distinct Values Sort Results Skip Returned Results Limit the Number of Returned Results Specify Which Fields to Return Search Geospatially Search Text",
            "code": [],
            "preview": "Learn about the commands for running MongoDB read operations by using the MongoDB Node.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/read-write-pref",
            "title": "Specify How CRUD Operations Run on Replica Sets",
            "headings": [
                "Overview",
                "Write Concern",
                "Example: Set the Write Concern for a Single Write Operation",
                "Example: Retrieve and Apply an Existing Write Concern",
                "Read Concern",
                "Example: Set the Read Concern Level of an Aggregation",
                "Example: Change the Read Concern of a Database",
                "Read Preference",
                "Example: Set Read Preference and Concerns for a Transaction",
                "Example: Set the Read Preference of a Cluster in the Connection String",
                "API Documentation"
            ],
            "paragraphs": "In this guide, you can learn how to use the  write concern ,  read concern , and\n read preference  configurations to modify the way that MongoDB runs\ncreate, read, update, and delete (CRUD) operations on replica sets. You can set write concern, read concern, and read preference options at the following\nlevels: This list also indicates the increasing order of precedence of the option settings. For\nexample, if you set a read concern level for a transaction, it will override a read\nconcern level set for the client. These options allow you to customize the causal consistency and availability of the data\nin your replica sets. Client, which sets the  default for all operation executions  unless overridden Session Transaction Database Collection The write concern specifies the level of acknowledgement requested from MongoDB for write\noperations, such as an insert or update, before the operation successfully returns.\nOperations that do not specify an explicit write concern inherit the global default write\nconcern settings. For more information, see  Write Concern  in the\nServer manual. For detailed API documentation, see the  WriteConcern API documentation . The following table describes the  WriteConcern  parameters: Parameter Type Description w   (optional) W Requests acknowledgment that the write operation has propagated to a specified\nnumber of  mongod  instances or to  mongod  instances that are labelled specified tags wtimeoutMS   (optional) number Specifies a time limit to prevent write operations from blocking indefinitely journal   (optional) boolean Requests acknowledgment that the write operation has been written to the on-disk journal This code uses custom  WriteConcern  settings while creating new a document: This code uses the  fromOptions()  method to construct a  WriteConcern  from the\noptions of an existing database reference,  myDB . Note that  myDB  could be replaced\nwith a reference to any entity that accepts a write concern option. Then the new write\nconcern is applied to a document,  myDoc . The read concern specifies the following behaviors: You can specify the read concern setting by using the  level  parameter. The default\nread concern level is  local . This means that the client returns the data from the\nreplica set member that the client is connected to, with no guarantee that the data has\nbeen written to all replica set members. Note that lower read concern level requirements\nmay reduce latency. For more information about read concerns or read concern levels, see\n Read Concern  in the Server manual. For more detail on\nthe  ReadConcern  type and definitions of the read concern levels, see the  ReadConcern  in\nthe API documentation. Level of  causal consistency  across replica sets Isolation guarantees  maintained during a query This code sets the read concern level of an an aggregation to  \"majority\" : For more information about aggregates, see the  Aggregation  page. This code changes the read concern level of a database to  \"local\" : The read preference determines which member of a replica set MongoDB reads when running a\nquery. You can also customize how the server evaluates members. For more detailed API documentation, see the  ReadPreference API\ndocumentation . The following table describes the  ReadPreference  parameters: Parameter Type Description mode ReadPreferenceMode Specifies a requirement or preference for which replica set\nmember the server reads from. The default mode,  primary , specifies that\noperations read from the primary member of the replica set. tags   (optional) TagSet List Assigns tags to secondary replica set members to customize how the server evaluates\nthem. Tags cannot be used with the  primary  read preference mode setting. options   (optional) ReadPreferenceOptions Sets various options, including  hedge \nand  maxStalenessSeconds  that can be\napplied to your read preference. This code sets the read preference, read concern, and write concern for the operations in\na transaction: For more information about transactions, see  Transactions . This code example creates a MongoClient that uses the \"secondary\" read preference mode\nwhen performing queries on a cluster: This example also sets the  maxStalenessSeconds  option. For more information about connection string options, see the  Connection String Options \nsection in the manual. To learn more about the methods and types mentioned in this guide, see the following API\ndocumentation: API WriteConcern API ReadConcern API ReadPreference",
            "code": [
                {
                    "lang": "js",
                    "value": "myDB.myCollection.insertOne(\n   { name: \"anotherDocumentName\" },\n   { writeConcern:\n     { w: 2, wtimeoutMS: 5000 }\n   }\n);"
                },
                {
                    "lang": "js",
                    "value": "const newWriteConcern = WriteConcern.fromOptions(myDB);\nconst myDoc = { name: \"New Document\" };\nWriteConcern.apply(myDoc,newWriteConcern);"
                },
                {
                    "lang": "js",
                    "value": "const pipeline = [\n   {\"$match\": {\n     category: \"KITCHENWARE\",\n   }},\n   {\"$unset\": [\n     \"_id\",\n     \"category\",\n   ]}\n ];\n\nresult = await myDB.collection(\"mycollection\")\n   .aggregate(\n     pipeline,\n     { readConcern:\n       { level: \"available\" }\n     }\n   );"
                },
                {
                    "lang": "js",
                    "value": "const options = { readConcern: { level: \"local\" } };\nconst myDB = client.db(\"mydb\", options);"
                },
                {
                    "lang": "js",
                    "value": "const transactionOptions = {\n   readPreference: \"primary\",\n   readConcern: { level: \"local\" },\n   writeConcern: { w: \"majority\" },\n};\n\nconst session = client.startSession();\nsession.startTransaction(transactionOptions);\n// ...\nawait session.commitTransaction();\nawait session.endSession();"
                },
                {
                    "lang": "js",
                    "value": "const uri = \"mongodb+srv://<user>:<password>@<cluster-url>?readPreference=secondary&maxStalenessSeconds=120\";\nconst client = new MongoClient(uri);"
                }
            ],
            "preview": "In this guide, you can learn how to use the write concern, read concern, and\nread preference configurations to modify the way that MongoDB runs\ncreate, read, update, and delete (CRUD) operations on replica sets.",
            "tags": "node.js, customize, preferences, replica set, consistency",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations/delete",
            "title": "Delete Documents",
            "headings": [
                "Overview",
                "Delete"
            ],
            "paragraphs": "In this section, we show you how to call the write operations to  remove \ndocuments from a collection in your MongoDB database. If you want to remove existing documents from a collection, you can\nuse  deleteOne()  to remove one document or  deleteMany()  for one or\nmore documents. These methods accept a query document that matches the\ndocuments you want to delete. You can specify the document or documents to be deleted by the\n deleteOne()  or  deleteMany()  write operations in a JSON object as\nfollows: To delete the first matching document using the  deleteOne()  method or\nto delete all matching documents using the  deleteMany()  method, pass the\ndocument as the method parameter: You can print the number of documents deleted by the operation by\naccessing the  deletedCount  field of the result for each of the\nmethod calls above as follows: If the delete operation is successful, these statements print the number of documents\ndeleted by the associated operation. To see fully runnable examples and more information on the available options, see the usage\nexamples for  deleteOne()  and\n deleteMany() .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const doc = {\n  pageViews: {\n    $gt: 10,\n    $lt: 32768\n  }\n};"
                },
                {
                    "lang": "javascript",
                    "value": "const deleteResult = await myColl.deleteOne(doc);\nconst deleteManyResult = await myColl.deleteMany(doc);"
                },
                {
                    "lang": "javascript",
                    "value": "console.dir(deleteResult.deletedCount);\nconsole.dir(deleteManyResult.deletedCount);"
                }
            ],
            "preview": "In this section, we show you how to call the write operations to remove\ndocuments from a collection in your MongoDB database.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations/embedded-arrays",
            "title": "Update Arrays in a Document",
            "headings": [
                "Overview",
                "Specifying Array Elements",
                "The First Matching Array Element",
                "Example",
                "Matching All Array Elements",
                "Example",
                "Matching Multiple Array Elements",
                "Usage",
                "Example"
            ],
            "paragraphs": "In this guide, you can learn how to use the following array update\noperators to modify an array embedded within a document: For a list of array update operators, see  Update Operators  in the Server\nManual documentation. Positional Operator :  $ All Positional Operator :  $[] Filtered Positional Operator :  $[<identifier>] Positional operators specify which array elements to update. You can use these operators to apply updates to the first element, all elements, or\ncertain elements of an array that match a criteria. To specify elements in an array with positional operators, use  dot\nnotation . Dot notation is a property access syntax for navigating BSON\nobjects. To learn more, see  dot notation . To update the first array element of each document that matches your\nquery, use the positional operator  $ . The positional operator  $  references the array matched by the query.\nYou cannot use this operator to reference a nested array. If you want to\naccess a nested array, use the  filtered positional operator . Do not use the  $  operator in an  upsert  call because the\ndriver treats  $  as a field name in the insert document. This example uses the following sample document to show how to update\nthe first matching array element: The following code shows how to increment a value in the first array\nelement that matches a query. The query matches elements in the  entries  array where the value of\n x  is a  string  type. The update increases the  y  value by\n 33  in the first matching element. After you run the update operation, the document resembles the\nfollowing: The example includes the  entries.x  field in the\nquery to match the array that the  $  operator applies an update to. If you\nomit the  entries.x  field from the query while using the\n $  operator in an update, the driver is unable to identify the\nmatching array and raises the following error: To perform the update on all array elements of each document that\nmatches your query, use the all positional operator  $[] . This example uses the following sample documents, which describe phone\ncall logs, to show how to update all matching array elements: The following code shows how to remove the  duration  field from\nall  calls  array entries in the document whose  date  is\n \"5/15/2023\" : After you run the update operation, the documents resemble the following: To perform an update on all embedded array elements of each document\nthat matches your query, use the filtered positional operator\n $[<identifier>] . The filtered positional operator  $[<identifier>]  specifies the\nmatching array elements in the update document. To identify which array\nelements to match, pair this operator with  <identifier>  in an\n arrayFilters  object. The  <identifier>  placeholder represents an element of the array\nfield. You must select a value for  <identifier>  that starts with a\nlowercase letter and contains only alphanumeric characters. You can use a filtered positional operator in an update operation.\nAn update operation takes a query, an update document, and\noptionally, an options object as its parameters. The following steps describe how to use a filtered positional operator\nin an update operation: Format your update document as follows: This update document contains the following placeholders: $<operator> : The array update operator <array> : The array in the document to update <identifier> : The identifier for the filtered positional operator <arrayField> : The field in the  <array>  array element to update <updateParameter> : The value that describes the update Add the matching criteria in the  arrayFilters  object. This object\nis an array of queries that specify which array elements to include\nin the update. Set this object in an  options  parameter: Pass the query, the update document, and options to an\nupdate method. The following sample code shows how to call the\n updateOne()  method with these parameters: This example uses the following sample documents, which describe\nshopping lists for specific recipes, to show how to update certain matching array elements: Suppose you want to increase the quantity of items you purchase for a\nrecipe on your  \"11/12/2023\"  grocery trip. You want to double the quantity if\nthe item meets all the following criteria: To double the  quantity  value in the matching array\nentries, use the filtered positional operator as shown in the following\ncode: The update multiplied the  quantity  value by  2  for\nitems that matched the criteria. The item  \"Sesame oil\"  did not match\nthe criteria in the  arrayFilters  object and therefore was excluded\nfrom the update. The following documents reflect these changes: The item is for the  \"Fried rice\"  recipe. The item name does not include the word  \"oil\" .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  entries: [\n    { x: false, y: 1 },\n    { x: \"hello\", y: 100 },\n    { x: \"goodbye\", y: 1000 }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  entries: [\n    { x: false, y: 1 },\n    { x: \"hello\", y: 133 },\n    { x: \"goodbye\", y: 1000 }\n  ]\n}"
                },
                {
                    "lang": "none",
                    "value": "MongoServerError: The positional operator did not find the match needed from the query."
                },
                {
                    "lang": "javascript",
                    "value": "// Query for all elements in entries array where the value of x is a string\nconst query = { \"entries.x\": { $type : \"string\" } };\n\n// On first matched element, increase value of y by 33\nconst updateDocument = {\n  $inc: { \"entries.$.y\": 33 }\n};\n\n// Execute the update operation\nconst result = await myColl.updateOne(query, updateDocument);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  date: \"5/15/2023\",\n  calls: [\n    { time: \"10:08 AM\", caller: \"Mom\", duration: 67 },\n    { time: \"04:11 PM\", caller: \"Dad\", duration: 121 },\n    { time: \"06:36 PM\", caller: \"Grandpa\", duration: 13 }\n  ]\n},\n{\n  _id: ...,\n  date: \"5/16/2023\",\n  calls: [\n    { time: \"11:47 AM\", caller: \"Mom\", duration: 4 },\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  date: \"5/15/2023\",\n  calls: [\n    { time: \"10:08 AM\", caller: \"Mom\" },\n    { time: \"04:11 PM\", caller: \"Dad\" },\n    { time: \"06:36 PM\", caller: \"Grandpa\" }\n  ]\n},\n{\n  _id: ...,\n  date: \"5/16/2023\",\n  calls: [\n    { time: \"11:47 AM\", caller: \"Mom\", duration: 4 },\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// Query for all documents where date is the string \"5/15/2023\"\nconst query = { date: \"5/15/2023\" };\n\n// For each matched document, remove duration field from all entries in calls array \nconst updateDocument = {\n  $unset: { \"calls.$[].duration\": \"\" }\n};\n\n// Execute the update operation\nconst result = await myColl.updateOne(query, updateDocument);"
                },
                {
                    "lang": "javascript",
                    "value": "{ $<operator>: { \"<array>.$[<identifier>].<arrayField>\": <updateParameter> } }"
                },
                {
                    "lang": "javascript",
                    "value": "arrayFilters: [\n  { \"<identifier>.<arrayField1>\": <updateParameter1> },\n  { \"<identifier>.<arrayField2>\": <updateParameter2> },\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "await myColl.updateOne(query, updateDocument, options);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  date: \"11/12/2023\",\n  items: [\n    { item: \"Scallions\", quantity: 3, recipe: \"Fried rice\" },\n    { item: \"Mangos\", quantity: 4, recipe: \"Salsa\" },\n    { item: \"Pork shoulder\", quantity: 1, recipe: \"Fried rice\" },\n    { item: \"Sesame oil\", quantity: 1, recipe: \"Fried rice\" }\n  ]\n},\n{\n  _id: ...,\n  date: \"11/20/2023\",\n  items: [\n    { item: \"Coffee beans\", quantity: 1, recipe: \"Coffee\" }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: ...,\n  date: \"11/12/2023\",\n  items: [\n    { item: \"Scallions\", quantity: 6, recipe: \"Fried rice\" },\n    { item: \"Mangos\", quantity: 4, recipe: \"Salsa\" },\n    { item: \"Pork shoulder\", quantity: 2, recipe: \"Fried rice\" },\n    { item: \"Sesame oil\", quantity: 1, recipe: \"Fried rice\" }\n  ]\n},\n{\n  _id: ...,\n  date: \"11/20/2023\",\n  items: [\n    { item: \"Coffee beans\", quantity: 1, recipe: \"Coffee\" }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// Query for all documents where date is the string \"11/12/2023\"\nconst query = { date: \"11/12/2023\" };\n\n// For each matched document, change the quantity of items to 2 \nconst updateDocument = {\n  $mul: { \"items.$[i].quantity\": 2 }\n};\n\n// Update only non-oil items used for fried rice \nconst options = {\n  arrayFilters: [\n    {\n      \"i.recipe\": \"Fried rice\",\n      \"i.item\": { $not: { $regex: \"oil\" } },\n    }\n  ]\n};\n\n// Execute the update operation\nconst result = await myColl.updateOne(query, updateDocument, options);"
                }
            ],
            "preview": "In this guide, you can learn how to use the following array update\noperators to modify an array embedded within a document:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations/insert",
            "title": "Insert Documents",
            "headings": [
                "Overview",
                "A Note About _id",
                "Insert a Single Document",
                "Example",
                "Insert Multiple Documents",
                "Example"
            ],
            "paragraphs": "In this guide, you can learn how to insert documents into MongoDB. You can use MongoDB to retrieve, update, and delete information that is already stored\nin MongoDB. To store information, use an  insert operation . An insert operation inserts one or more documents into a MongoDB collection.\nThe Node.js driver provides the following methods to perform insert\noperations: The following sections focus on  insertOne()  and  insertMany() . For an\nexample on how to use the  bulkWrite()  method, see our runnable  Bulk\nOperations Example . insertOne() insertMany() bulkWrite() This page includes a short interactive lab that demonstrates how to\ninsert data by using the  insertOne()  method. You can complete this lab\ndirectly in your browser window without installing MongoDB or a code editor. To start the lab, click the  Open Interactive Tutorial  button at the\ntop of the page. To expand the lab to a full-screen format, click the\nfull-screen button ( \u26f6 ) in the top-right corner of the lab pane. When inserting a document, MongoDB enforces one constraint on your\ndocuments by default. Each document  must  contain a unique  _id \nfield. There are two ways to manage this field: Unless you have provided strong guarantees for uniqueness, we recommend\nyou let the driver automatically generate  _id  values. For more information about  _id , see the Server manual entry on\n Unique Indexes . You can manage this field yourself, ensuring each value you use is unique. You can let the driver automatically generate unique  ObjectId  values\nwith the  primary key factory . Duplicate  _id  values violate unique index constraints, resulting\nin a  WriteError . Use the  insertOne()  method when you want to insert a single\ndocument. On successful insertion, the method returns an\n InsertOneResult  instance representing the  _id  of\nthe new document. The following example uses the  insertOne()  method to insert a new\ndocument into the  myDB.pizzaMenu  collection: Your output looks similar to the following text: For more information on the classes and methods mentioned in this\nsection, see the following resources: API Documentation on  insertOne() API Documentation on  InsertOneResult Server manual entry on  insertOne() Runnable  Insert a Document Example Use the  insertMany()  method when you want to insert multiple\ndocuments. This method inserts documents in the order specified until an\nexception occurs, if any. For example, assume you want to insert the following documents: If you attempt to insert these documents, a  WriteError  occurs when the third\ndocument is processed, but the documents before the error are inserted into your\ncollection. On successful insertion, the method returns an\n InsertManyResult  instance representing the number of\ndocuments inserted and the  _id  of the new document. Use a try-catch block to get an acknowledgment for successfully\nprocessed documents before the error occurs: The output consists of documents MongoDB can process and looks similar to the\nfollowing: If you look inside your collection, you see the following documents: The following example uses the  insertMany()  method to insert three new\ndocuments into the  myDB.pizzaMenu  collection: Your output looks similar to the following: For more information on the classes and methods mentioned in this\nsection, see the following resources: API Documentation on  insertMany() API Documentation on  InsertManyResult API Documentation on  PkFactory Server manual entry on  insertMany() Runnable  Insert Multiple Documents Example",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"pizzaMenu\");\n\nconst doc = { name: \"Neapolitan pizza\", shape: \"round\" };\nconst result = await myColl.insertOne(doc);\nconsole.log(\n   `A document was inserted with the _id: ${result.insertedId}`,\n);"
                },
                {
                    "lang": null,
                    "value": "A document was inserted with the _id: 60c79c0f4cc72b6bb31e3836"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 1, \"color\": \"red\" }\n{ \"_id\": 2, \"color\": \"purple\" }\n{ \"_id\": 1, \"color\": \"yellow\" }\n{ \"_id\": 3, \"color\": \"blue\" }"
                },
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"colors\");\n\ntry {\n   const docs = [\n      { \"_id\": 1, \"color\": \"red\"},\n      { \"_id\": 2, \"color\": \"purple\"},\n      { \"_id\": 1, \"color\": \"yellow\"},\n      { \"_id\": 3, \"color\": \"blue\"}\n   ];\n\n   const insertManyresult = await myColl.insertMany(docs);\n   let ids = insertManyresult.insertedIds;\n\n   console.log(`${insertManyresult.insertedCount} documents were inserted.`);\n   for (let id of Object.values(ids)) {\n      console.log(`Inserted a document with id ${id}`);\n   }\n} catch(e) {\n   console.log(`A MongoBulkWriteException occurred, but there are successfully processed documents.`);\n   let ids = e.result.result.insertedIds;\n   for (let id of Object.values(ids)) {\n      console.log(`Processed a document with id ${id._id}`);\n   }\n   console.log(`Number of documents inserted: ${e.result.result.nInserted}`);\n}"
                },
                {
                    "lang": null,
                    "value": "A MongoBulkWriteException occurred, but there are successfully processed documents.\nProcessed a document with id 1\nProcessed a document with id 2\nProcessed a document with id 1\nProcessed a document with id 3\nNumber of documents inserted: 2"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": 1, \"color\": \"red\" }\n{ \"_id\": 2, \"color\": \"purple\" }"
                },
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"pizzaMenu\");\n\nconst docs = [\n   { name: \"Sicilian pizza\", shape: \"square\" },\n   { name: \"New York pizza\", shape: \"round\" },\n   { name: \"Grandma pizza\", shape: \"square\" }\n];\n\nconst insertManyresult = await myColl.insertMany(docs);\nlet ids = insertManyresult.insertedIds;\n\nconsole.log(`${insertManyresult.insertedCount} documents were inserted.`);\n\nfor (let id of Object.values(ids)) {\n   console.log(`Inserted a document with id ${id}`);\n}"
                },
                {
                    "lang": null,
                    "value": "3 documents were inserted.\nInserted a document with id 60ca09f4a40cf1d1afcd93a2\nInserted a document with id 60ca09f4a40cf1d1afcd93a3\nInserted a document with id 60ca09f4a40cf1d1afcd93a4"
                }
            ],
            "preview": "In this guide, you can learn how to insert documents into MongoDB.",
            "tags": "code example, node.js, add data",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations/modify",
            "title": "Modify Documents",
            "headings": [
                "Overview",
                "Update Documents",
                "Example",
                "Replace a Document",
                "Example"
            ],
            "paragraphs": "You can modify documents in a MongoDB collection by using  update \nand  replace  operations. Update operations modify the fields and\nvalues of a document while keeping other fields and values\nunchanged. Replace operations substitute all fields and values\nin an existing document with specified fields and values while keeping\nthe  _id  field value unchanged. The Node.js driver provides the following methods to change documents: updateOne() updateMany() replaceOne() This page includes a short interactive lab that demonstrates how to\nmodify data by using the  updateMany()  method. You can complete this lab\ndirectly in your browser window without installing MongoDB or a code editor. To start the lab, click the  Open Interactive Tutorial  button at the\ntop of the page. To expand the lab to a full-screen format, click the\nfull-screen button ( \u26f6 ) in the top-right corner of the lab pane. To perform an update to one or more documents, create an  update\ndocument  that specifies the  update operator  (the type of update to\nperform) and the fields and values that describe the change. Update\ndocuments use the following format: The top level of an update document contains one or more of the following\nupdate operators: See the MongoDB Server manual for a  complete list of update operators\nand their usage . The update operators apply only to the fields associated with them in your\nupdate document. $set : replaces the value of a field with a specified one $inc : increments or decrements field values $rename : renames fields $unset : removes fields $mul : multiplies a field value by a specified number If you are using MongoDB Version 4.2 or later, you can use aggregation\npipelines made up of a subset of aggregation stages in update operations. For\nmore information on the aggregation stages MongoDB supports in\naggregation pipelines used in update operations, see our tutorial on building\n updates with aggregation pipelines . Consider a document in the  myDB.items  collection with fields\ndescribing an item for sale, its price, and the quantity available: If you apply the  $set  update operator with a new value for\n quantity , you can use the following update document: The updated document resembles the following, with an updated value in\nthe  quantity  field and all other values unchanged: If an update operation fails to match any documents in a collection, it\ndoes not make any changes. Update operations can be configured to perform\nan  upsert  which\nattempts to perform an update, but if no documents are matched, inserts\na new document with the specified fields and values. You cannot modify the  _id  field of a document nor change a field to\na value that violates a unique index constraint. See the MongoDB Server manual\nfor more information on  unique indexes . To perform a replacement operation, create a  replacement document  that\nconsists of the fields and values that you want to use in your\n replace  operation. Replacement documents use the following format: Replacement documents are the documents that you want to take the place of\nexisting documents that match the query filters. Consider a document in the  myDB.items  collection with fields\ndescribing an item for sale, its price, and the quantity available: Suppose you wanted to replace this document with one that contains a\ndescription for an entirely different item. Your replacement operation might\nresemble the following: The replaced document contains the contents of the replacement document\nand the immutable  _id  field as follows: If a replace operation fails to match any documents in a collection, it\ndoes not make any changes. Replace operations can be configured to perform\nan  upsert  which\nattempts to perform the replacement, but if no documents are matched, it\ninserts a new document with the specified fields and values. You cannot modify the  _id  field of a document nor change a field to\na value that violates a unique index constraint. See the MongoDB Server manual\nfor more information on  unique indexes .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{\n   <update operator>: {\n      <field> : {\n         ...\n      },\n      <field> : {\n      }\n   },\n   <update operator>: {\n      ...\n   }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   _id: 465,\n   item: \"Hand-thrown ceramic plate\",\n   price: 32.50,\n   quantity: 7,\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"items\");\n\nconst filter = { _id: 465 };\n\n// update the value of the 'quantity' field to 5\nconst updateDocument = {\n   $set: {\n      quantity: 5,\n   },\n};\nconst result = await myColl.updateOne(filter, updateDocument);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   _id: 465,\n   item: \"Hand-thrown ceramic plate\",\n   price: 32.50,\n   quantity: 5,\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   <field>: {\n      <value>\n   },\n   <field>: {\n      ...\n   }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   _id: 501,\n   item: \"3-wick beeswax candle\",\n   price: 18.99,\n   quantity: 10,\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"items\");\n\nconst filter = { _id: 501 };\n\n// replace the matched document with the replacement document\nconst replacementDocument = {\n   item: \"Vintage silver flatware set\",\n   price: 79.15,\n   quantity: 1,\n};\nconst result = await myColl.replaceOne(filter, replacementDocument);"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   _id: 501,\n   item: \"Vintage silver flatware set\",\n   price: 79.15,\n   quantity: 1,\n}"
                }
            ],
            "preview": "You can modify documents in a MongoDB collection by using update\nand replace operations. Update operations modify the fields and\nvalues of a document while keeping other fields and values\nunchanged. Replace operations substitute all fields and values\nin an existing document with specified fields and values while keeping\nthe _id field value unchanged.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations/pkFactory",
            "title": "Generate Custom Values for _id",
            "headings": [
                "Overview",
                "Specify a Primary Key Factory",
                "Additional Information"
            ],
            "paragraphs": "In this guide, you can learn how to use the MongoDB Node.js driver to generate your\nown  _id  values using the  primary key factory . The primary key factory allows you to create unique identifiers in your\ndocuments when you choose not to specify an  _id  during an\n insert operation . The\ndefault primary key factory generates  ObjectId  values. The driver doesn't use the primary key factory for\n upsert operations  because it's\nunable to determine whether to apply the primary key factory. If you\nspecified the primary key factory in an upsert operation and it\nperforms an insert operation, the server autogenerates an\n ObjectId  for that document. If you want to use your specified primary key factory, perform a\n find operation , then an\n update  or\n insert  operation. To specify a primary key factory, apply the  pkFactory  option to your\n MongoClient  instance. The following code snippet applies the  pkFactory  option to\ngenerate  _id  values of type  uuid : If you insert a document with an  _id  field with a different\ntype than the type specified by the primary key factory, then you\nwill have inconsistent data. For example, if you run the following insert operation on a primary\nkey factory that generates  uuid  types, your  _id  values will\ncontain both the  uuid  and  string  types: To learn more about the types, interfaces, and classes discussed in this\nsection, see the following resources: pkFactory The _id Field Insert or Update in a Single Operation Retrieve Data Modify Documents Insert Documents",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const { UUID } = require('bson');\n...\nconst client = new MongoClient(uri, {\n  pkFactory: { createPk: () =>  new UUID().toBinary() }\n});"
                },
                {
                    "lang": "javascript",
                    "value": "myColl.insertOne({ _id: \"user1388\", ... });"
                }
            ],
            "preview": "In this guide, you can learn how to use the MongoDB Node.js driver to generate your\nown _id values using the primary key factory.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations/upsert",
            "title": "Insert or Update in a Single Operation",
            "headings": [
                "Overview",
                "Performing an Update",
                "Performing an Upsert"
            ],
            "paragraphs": "If your application stores and modifies data in MongoDB, you probably use\ninsert and update operations. In certain workflows, whether you perform\nan insert or update operation depends on whether the document exists.\nIn these cases, you can streamline your application logic by using the\n upsert  option available in the following methods: If the query filter passed to these methods does not find any matches and\nyou set the  upsert  option to  true , MongoDB inserts the update\ndocument. Let's go through an example. updateOne() replaceOne() updateMany() Suppose your application tracks the current location of food trucks,\nstoring the nearest address data in the  myDB.foodTrucks  collection,\nwhich resembles the following: As an application user, you read about a food truck changing its regular\nlocation and want to apply the update. This update might resemble the\nfollowing: If a food truck named \"Deli Llama\" exists, the method call above updates\nthe document in the collection. However, if there are no food trucks named\n\"Deli Llama\" in your collection, no changes are made. Consider the case in which you want to add information about the food\ntruck even if it does not yet exist in your collection. Rather than\nfirst querying whether it exists to determine whether to insert or\nupdate the document, we can set  upsert  to  true  in our call to\n updateOne()  as follows: After you run the operation above, your collection looks similar to the\nfollowing, even if the  \"Deli Llama\"  document did not exist in your collection\nbefore the operation:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "[\n  { name: \"Haute Skillet\", address: \"42 Avenue B\" },\n  { name: \"Lady of the Latke\", address: \"35 Fulton Rd\" },\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "const myDB = client.db(\"myDB\");\nconst myColl = myDB.collection(\"foodTrucks\");\n\nconst query = { name: \"Deli Llama\" };\nconst update = { $set: { name: \"Deli Llama\", address: \"3 Nassau St\" }};\nconst options = {};\nmyColl.updateOne(query, update, options);"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { name: \"Deli Llama\" };\nconst update = { $set: { name: \"Deli Llama\", address: \"3 Nassau St\" }};\nconst options = { upsert: true };\nmyColl.updateOne(query, update, options);"
                },
                {
                    "lang": "javascript",
                    "value": "[\n  { name: \"Haute Skillet\", address: \"42 Avenue B\" },\n  { name: \"Lady of the Latke\", address: \"35 Fulton Rd\" },\n  { name: \"Deli Llama\", address: \"3 Nassau St\" },\n  ...\n]"
                }
            ],
            "preview": "If your application stores and modifies data in MongoDB, you probably use\ninsert and update operations. In certain workflows, whether you perform\nan insert or update operation depends on whether the document exists.\nIn these cases, you can streamline your application logic by using the\nupsert option available in the following methods:",
            "tags": "code example, node.js, write, add data",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud/write-operations",
            "title": "Write Operations",
            "headings": [],
            "paragraphs": "Insert Documents Generate Custom Values for  _id Delete Documents Modify Documents Update Arrays in a Document Insert or Update in a Single Operation",
            "code": [],
            "preview": "Learn about the commands for running MongoDB write operations by using the MongoDB Node.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/crud",
            "title": "CRUD Operations",
            "headings": [
                "Compatibility"
            ],
            "paragraphs": "CRUD (Create, Read, Update, Delete) operations allow you to work with\nthe data stored in MongoDB. The CRUD operation documentation is categorized in two sections: Some operations combine aspects of read and write operations. See our\nguide on  compound operations \nto learn more about these hybrid methods. Read Operations  find and return\ndocuments stored within your MongoDB database. Write Operations  insert, modify,\nor delete documents in your MongoDB database. You can use the Node.js driver to connect and  perform CRUD operations  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  performing CRUD operations in the Atlas UI  for deployments hosted in MongoDB\nAtlas, see  Create, View, Update, and Delete Documents . To learn more about performing CRUD operations, see the following posts on the  MongoDB\nDeveloper Hub : Learn how to apply  CRUD Operations \nwith an example scenario. Analyze data in MongoDB Atlas using the  Aggregation Pipeline .",
            "code": [],
            "preview": "Learn how to perform create, read, update, and delete (CRUD) operations to work with the data stored in MongoDB by using the Node.js driver.",
            "tags": "node.js",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/encrypt-fields",
            "title": "In-Use Encryption",
            "headings": [
                "Overview",
                "Queryable Encryption",
                "Client-side Field Level Encryption"
            ],
            "paragraphs": "You can use the Node.js driver to encrypt specific document fields by using a\nset of features called  in-use encryption . In-use encryption allows\nyour application to encrypt data  before  sending it to MongoDB\nand query documents with encrypted fields. In-use encryption prevents unauthorized users from viewing plaintext\ndata as it is sent to MongoDB or while it is in an encrypted database. To\nenable in-use encryption in an application and authorize it to decrypt\ndata, you must create encryption keys that only your application can\naccess. Only applications that have access to your encryption\nkeys can access the decrypted, plaintext data. If an attacker gains\naccess to the database, they can only see the encrypted ciphertext data\nbecause they lack access to the encryption keys. You might use in-use encryption to encrypt fields in your MongoDB\ndocuments that contain the following types of sensitive data: MongoDB offers the following features to enable in-use encryption: Credit card numbers Addresses Health information Financial information Any other sensitive or personally identifiable information (PII) Queryable Encryption Client-side Field Level Encryption Queryable Encryption is the next-generation in-use encryption feature,\nfirst introduced as a preview feature in MongoDB Server version 6.0 and\nas a generally available (GA) feature in MongoDB 7.0. Queryable\nEncryption supports searching encrypted fields for equality and encrypts\neach value uniquely. To learn more about Queryable Encryption, see  Queryable\nEncryption  in the Server manual. The implementation of Queryable Encryption in MongoDB 6.0 is incompatible with the GA version introduced in MongoDB 7.0. The Queryable Encryption preview feature is no longer supported. Client-side Field Level Encryption (CSFLE) was introduced in MongoDB\nServer version 4.2 and supports searching encrypted fields for equality.\nCSFLE differs from Queryable Encryption in that you can select either a\ndeterministic or random encryption algorithm to encrypt fields. You can only\nquery encrypted fields that use a deterministic encryption algorithm when\nusing CSFLE. When you use a random encryption algorithm to encrypt\nfields in CSFLE, they can be decrypted, but you cannot perform equality\nqueries on those fields. When you use Queryable Encryption, you cannot\nspecify the encryption algorithm, but you can query all encrypted\nfields. When you deterministically encrypt a value, the same input value\nproduces the same output value. While deterministic encryption allows\nyou to perform queries on those encrypted fields, encrypted data with\nlow cardinality is susceptible to code breaking by frequency analysis. To learn more about CSFLE, see  CSFLE  in the\nServer manual. To learn more about these concepts, see the following Wikipedia\nentries: Cardinality Frequency Analysis",
            "code": [],
            "preview": "You can use the Node.js driver to encrypt specific document fields by using a\nset of features called in-use encryption. In-use encryption allows\nyour application to encrypt data before sending it to MongoDB\nand query documents with encrypted fields.",
            "tags": "node.js",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/gridfs",
            "title": "GridFS",
            "headings": [
                "Overview",
                "How GridFS Works",
                "Create a GridFS Bucket",
                "Upload Files",
                "Retrieve File Information",
                "Download Files",
                "Rename Files",
                "Delete Files",
                "Delete a GridFS Bucket",
                "Additional Resources"
            ],
            "paragraphs": "In this guide, you can learn how to store and retrieve large files in\nMongoDB using  GridFS . GridFS is a specification that describes how\nto split files into chunks during storage\nand reassemble them during retrieval. The driver implementation of\nGridFS manages the operations and organization of\nthe file storage. Use GridFS if the size of your file exceeds the BSON-document\nsize limit of 16 megabytes. For more detailed information on whether GridFS is\nsuitable for your use case, see the  GridFS Server manual page . Navigate the following sections to learn more about GridFS operations\nand implementation: Create a GridFS Bucket Upload Files Retrieve File Information Download Files Rename Files Delete Files Delete a GridFS Bucket GridFS organizes files in a  bucket , a group of MongoDB collections\nthat contain the chunks of files and descriptive information.\nBuckets contain the following collections, named using the convention\ndefined in the GridFS specification: When you create a new GridFS bucket, the driver creates the  chunks \nand  files  collections, prefixed with the default bucket name  fs , unless\nyou specify a different name. The driver also creates an index on each\ncollection to ensure efficient retrieval of files and related\nmetadata. The driver only creates the GridFS bucket on the first write\noperation if it does not already exist. The driver only creates indexes if\nthey do not exist and when the bucket is empty. For more information on\nGridFS indexes, see the Server manual page on  GridFS Indexes . When storing files with GridFS, the driver splits the files into smaller\npieces, each represented by a separate document in the  chunks  collection.\nIt also creates a document in the  files  collection that contains\na unique file id, file name, and other file metadata. You can upload the file from\nmemory or from a stream. The following diagram describes how GridFS splits\nfiles when uploading to a bucket: When retrieving files, GridFS fetches the metadata from the  files \ncollection in the specified bucket and uses the information to reconstruct\nthe file from documents in the  chunks  collection. You can read the file\ninto memory or output it to a stream. The  chunks  collection stores the binary file chunks. The  files  collection stores the file metadata. Create a bucket or get a reference to an existing one to begin storing\nor retrieving files from GridFS. Create a  GridFSBucket \ninstance, passing a database as the parameter. You can then use the\n GridFSBucket  instance to call read and write operations on the files\nin your bucket: Pass your bucket name as the second parameter to the  create()  method\nto create or reference a bucket with a custom name other than the\ndefault name  fs , as shown in the following example: For more information, see the  GridFSBucket API documentation . Use the  openUploadStream()  method from  GridFSBucket  to create an upload\nstream for a given file name. You can use the  pipe()  method to\nconnect a Node.js read stream to the upload stream. The\n openUploadStream()  method allows you to specify configuration information\nsuch as file chunk size and other field/value pairs to store as metadata. The following example shows how to pipe a Node.js read stream, represented by the\nvariable  fs , to the  openUploadStream()  method of a  GridFSBucket  instance: See the  openUploadStream() API documentation  for more information. In this section, you can learn how to retrieve file metadata stored in the\n files  collection of the GridFS bucket. The metadata contains information\nabout the file it refers to, including: Call the  find()  method on the  GridFSBucket  instance to retrieve\nfiles from a GridFS bucket. The method returns a  FindCursor  instance\nfrom which you can access the results. The following code example shows you how to retrieve and print file metadata\nfrom all your files in a GridFS bucket. Among the different ways that you can\ntraverse the retrieved results from the  FindCursor  iterable, the\nfollowing example uses the  for await...of  syntax to display the results: The  find()  method accepts various query specifications and can be\ncombined with other methods such as  sort() ,  limit() , and  project() . For more information on the classes and methods mentioned in this section,\nsee the following resources: The  _id  of the file The name of the file The length/size of the file The upload date and time A  metadata  document in which you can store any other information find() API documentation FindCursor API documentation Cursor Fundamentals page Read Operations page You can download files from your MongoDB database by using the\n openDownloadStreamByName()  method from  GridFSBucket  to create a\ndownload stream. The following example shows you how to download a file referenced\nby the file name, stored in the  filename  field, into your working\ndirectory: Alternatively, you can use the  openDownloadStream() \nmethod, which takes the  _id  field of a file as a parameter: For more information on the  openDownloadStreamByName()  method, see\nits  API documentation . If there are multiple documents with the same  filename  value,\nGridFS will stream the most recent file with the given name (as\ndetermined by the  uploadDate  field). The GridFS streaming API cannot load partial chunks. When a download\nstream needs to pull a chunk from MongoDB, it pulls the entire chunk\ninto memory. The 255 kilobyte default chunk size is usually\nsufficient, but you can reduce the chunk size to reduce memory\noverhead. Use the  rename()  method to update the name of a GridFS file in your\nbucket. You must specify the file to rename by its  _id  field\nrather than its file name. The following example shows how to update the  filename  field to\n\"newFileName\" by referencing a document's  _id  field: For more information on this method, see the  rename() \nAPI documentation. The  rename()  method only supports updating the name of one file at\na time. To rename multiple files, retrieve a list of files matching the\nfile name from the bucket, extract the  _id  field from the files you\nwant to rename, and pass each value in separate calls to the  rename() \nmethod. Use the  delete()  method to remove a file from your bucket. You must\nspecify the file by its  _id  field rather than its file name. The following example shows you how to delete a file by referencing its  _id  field: For more information on this method, see the  delete() \nAPI documentation. The  delete()  method only supports deleting one file at a time. To\ndelete multiple files, retrieve the files from the bucket, extract\nthe  _id  field from the files you want to delete, and pass each value\nin separate calls to the  delete()  method. Use the  drop()  method to remove a bucket's  files  and  chunks \ncollections, which effectively deletes the bucket. The following\ncode example shows you how to delete a GridFS bucket: For more information on this method, see the  drop() \nAPI documentation. MongoDB GridFS specification",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const db = client.db(dbName);\nconst bucket = new mongodb.GridFSBucket(db);"
                },
                {
                    "lang": "javascript",
                    "value": "const bucket = new mongodb.GridFSBucket(db, { bucketName: 'myCustomBucket' });"
                },
                {
                    "lang": "javascript",
                    "value": "fs.createReadStream('./myFile').\n     pipe(bucket.openUploadStream('myFile', {\n         chunkSizeBytes: 1048576,\n         metadata: { field: 'myField', value: 'myValue' }\n     }));"
                },
                {
                    "lang": "javascript",
                    "value": "const cursor = bucket.find({});\nfor await (const doc of cursor) {\n   console.log(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "bucket.openDownloadStreamByName('myFile').\n     pipe(fs.createWriteStream('./outputFile'));"
                },
                {
                    "lang": "javascript",
                    "value": "bucket.openDownloadStream(ObjectId(\"60edece5e06275bf0463aaf3\")).\n     pipe(fs.createWriteStream('./outputFile'));"
                },
                {
                    "lang": "javascript",
                    "value": "bucket.rename(ObjectId(\"60edece5e06275bf0463aaf3\"), \"newFileName\");"
                },
                {
                    "lang": "javascript",
                    "value": "bucket.delete(ObjectId(\"60edece5e06275bf0463aaf3\"));"
                },
                {
                    "lang": "javascript",
                    "value": "bucket.drop();"
                }
            ],
            "preview": "In this guide, you can learn how to store and retrieve large files in\nMongoDB using GridFS. GridFS is a specification that describes how\nto split files into chunks during storage\nand reassemble them during retrieval. The driver implementation of\nGridFS manages the operations and organization of\nthe file storage.",
            "tags": "node.js, code example, file storage",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/indexes",
            "title": "Indexes",
            "headings": [
                "Overview",
                "Query Coverage and Performance",
                "Operational Considerations",
                "List Indexes",
                "Index Types",
                "Single Field Indexes",
                "Compound Indexes",
                "Multikey Indexes (Indexes on Array Fields)",
                "Clustered Indexes",
                "Text Indexes",
                "Geospatial Indexes",
                "Unique Indexes",
                "Search Indexes",
                "Create a Search Index",
                "List Search Indexes",
                "Update a Search Index",
                "Drop a Search Index"
            ],
            "paragraphs": "Indexes are data structures that support the efficient execution of queries in\nMongoDB. They contain copies of parts of the data in documents to make\nqueries more efficient. Without indexes, MongoDB must scan  every  document in a collection to find\nthe documents that match each query. These collection scans are slow and can\nnegatively affect the performance of your application. By using an index to\nlimit the number of documents MongoDB scans, queries can be more efficient\nand therefore return faster. When you execute a query against MongoDB, your query can include three\nparts: When all the fields specified in the query criteria and projection of a\nquery are indexed, MongoDB returns results directly from the index\nwithout scanning any documents in the collection or loading them into\nmemory. For more information on how to ensure your index covers your query\ncriteria and projection, see the MongoDB manual articles on\n query coverage \nand  index intersection . Query criteria that specify one or more fields and values that you are looking for Options that affect the query's execution, such as read concern Projection criteria to specify the fields you want MongoDB to return (optional) To improve query performance, build indexes on fields that appear often in your\napplication's queries and operations that return sorted results. Each index that you add\nconsumes disk space and memory when active, so it might be necessary to track index memory\nand disk usage for capacity planning. In addition, when a write operation updates an\nindexed field, MongoDB also updates the related index. For more information on designing your data model and choosing indexes\nappropriate for your application, see the MongoDB Server documentation on\n Indexing Strategies  and\n Data Modeling and Indexes . You can use the  listIndexes()  method to list all the indexes\nfor a collection. The  listIndexes()  method takes an\noptional  ListIndexesOptions  parameter. The  listIndexes()  method returns an\nobject of type  ListIndexesCursor . The following code uses the  listIndexes()  method to list all the\nindexes in a collection: MongoDB supports several different index types to support querying\nyour data. The following sections describe the most common index types\nand provide sample code for creating each index type. Single field indexes  are indexes that improve performance for queries\nthat specify ascending or descending sort order on a single field of a\ndocument. The following example uses the  createIndex()  method to create an\nascending order index on the  title  field in the  movies  collection in\nthe  sample_mflix  database. The following is an example of a query that is covered by the index\ncreated above. To learn more, see   Single Field Indexes . Compound indexes  are indexes that improve performance for queries that\nspecify ascending or descending sort order for  multiple  fields of\na document. You must specify the direction (ascending or descending) for\neach field in the index. The following example uses the  createIndex()  method to create a compound\nindex on the  type  and  genre  fields in the  movies  collection in the\n sample_mflix  database. The following is an example of a query that is covered by the index\ncreated above. To learn more, see   Compound Indexes . Multikey indexes  are indexes that improve the performance of queries on\nfields that contain array values. You can create a multikey index on a field with an array value by\ncalling the  createIndex()  method. The following code creates an ascending\nindex on the  cast  field in the  movies  collection of the\n sample_mflix  database: The following code queries the multikey index to find\ndocuments in which the  cast  field value contains  \"Viola Davis\" : Multikey indexes behave differently from non-multikey indexes in terms of\nquery coverage, index bound computation, and sort behavior. For a full\nexplanation of multikey indexes, including a discussion of their behavior\nand limitations, see the  Multikey Indexes page  in the MongoDB Server manual. Clustered indexes  are indexes that improve the performance of\ninsert, update, and delete operations on  clustered collections .\nClustered collections store documents ordered by the clustered index key\nvalue. To create a clustered index, specify the  clusteredIndex  option in\nthe  CollectionOption . The  clusteredIndex  option must specify the\n _id  field as the key and the unique field as  true . The following example uses the  createCollection()  method to create a\nclustered index on the  _id  field in the  vendors  collection of the\n tea  database. To learn more, see\n Clustered Indexes  and\n Clustered Collections . Text indexes  support text search queries on string content. These indexes\ncan include any field whose value is a string or an array of string elements. MongoDB supports text search for various languages, so you can specify the\ndefault language as an option when creating the index. You can also\nspecify a weight option to prioritize certain text fields in your\nindex. These weights denote the significance of fields relative to the\nother indexed fields. To learn more about text searches, see our guide on  text search queries . The following example uses the  createIndex()  method to perform the\nfollowing actions: The following query uses the text index created in the preceding code: To learn more about text indexes, see  Text Indexes  in the Server manual. Create a  text  index on the  title  and  body  fields in the\n blogPosts  collection Specify  english  as the default language Set the field weight of  body  to  10  and  title  to  3 MongoDB supports queries of geospatial coordinate data using  2dsphere\nindexes . With a 2dsphere index, you can query the geospatial data for\ninclusion, intersection, and proximity. For more information on querying\ngeospatial data with the MongoDB Node.js driver, read our\n Search Geospatial  guide. To create a 2dsphere index, you must specify a field that contains\nonly  GeoJSON objects . For more details on this type, see the MongoDB\nServer manual page on  GeoJSON objects . The  location.geo  field in following sample document from the\n theaters  collection in the  sample_mflix  database is a GeoJSON Point\nobject that describes the coordinates of the theater: The following example uses the  createIndexes()  method to create a\n 2dsphere  index on the  location.geo  field in the  theaters \ncollection in the  sample_mflix  database to enable geospatial searches. MongoDB also supports  2d  indexes for calculating distances on a\nEuclidean plane and for working with the \"legacy coordinate pairs\" syntax\nused in MongoDB 2.2 and earlier. To learn more, see\n Geospatial Queries . Unique indexes  ensure that the indexed fields do not store duplicate\nvalues. By default, MongoDB creates a unique index on the  _id  field\nduring the creation of a collection. To create a unique index, specify the\nfield or combination of fields that you want to prevent duplication on and\nset the  unique  option to  true . The following example uses the  createIndex()  method to create a unique\nindex on the  theaterId  field in the  theaters  collection of the\n sample_mflix  database. If you attempt to perform a write operation that stores a duplicate value\nthat violates the unique index, MongoDB will throw an error that resembles\nthe following: To learn more, see  Unique Indexes . Atlas Search is a feature that allows you to perform full-text\nsearches. To learn more, see the  Atlas Search \ndocumentation. Before you can perform a search on an Atlas collection, you must first\ncreate an Atlas Search index on the collection. An Atlas Search\nindex is a data structure that categorizes data in a searchable format. You can use the following methods to manage your Search indexes: The following sections provide code samples that use each of the preceding\nmethods to manage Search indexes. createSearchIndex() createSearchIndexes() listSearchIndexes() updateSearchIndex() dropSearchIndex() You can use the  createSearchIndex()  and\n createSearchIndexes() \nmethods to create new Search indexes. The following code shows how to\nuse the  createSearchIndex()  method to create an index called\n search1 : When connecting to MongoDB Server v6.0.11 and later v6 versions, or\nv7.0.2 and later v7 versions, you can use the driver to create an Atlas\nVector Search index on a collection. Learn more about this feature in\nthe  Atlas Vector Search documentation . The following code shows how to use the  createSearchIndex()  method\nto create a search index in which the  type  field is\n vectorSearch : You can use the  listSearchIndexes() \nmethod to return a cursor that contains the Search indexes of a given\ncollection. The  listSearchIndexes()  method takes an optional string\nparameter,  name , to return only the indexes with matching names. It\nalso takes an optional  aggregateOptions  parameter. The following code uses the  listSearchIndexes()  method to list the\nSearch indexes in a collection: You can use the  updateSearchIndex()  method to update a Search\nindex. The following code shows how to\nuse the  updateSearchIndex()  method to update an index called\n search1  to specify a string type for the  description  field: You can use the  dropSearchIndex()  method to remove a Search\nindex. The following code shows how to\nuse the  dropSearchIndex()  method to remove an index called\n search1 :",
            "code": [
                {
                    "lang": "javascript",
                    "value": "// List the indexes on the collection and output them as an array\nconst result = await collection.listIndexes().toArray();\n\n// Print the list of indexes\nconsole.log(\"Existing indexes:\\n\");\nfor(const doc in result){\n    console.log(doc);\n}"
                },
                {
                    "lang": "js",
                    "value": "const database = client.db(\"sample_mflix\");\nconst movies = database.collection(\"movies\");\n\n// Create an ascending index on the \"title\" field in the\n// \"movies\" collection.\nconst result = await movies.createIndex({ title: 1 });\nconsole.log(`Index created: ${result}`);"
                },
                {
                    "lang": "js",
                    "value": "// Define the query parameters\nconst query = { title: \"Batman\" }\nconst sort = { title: 1 };\nconst projection = { _id: 0, title: 1 };\n// Execute the query using the defined parameters\nconst cursor = movies\n  .find(query)\n  .sort(sort)\n  .project(projection);"
                },
                {
                    "lang": "js",
                    "value": "// Connect to the \"sample_mflix\" database\nconst database = client.db(\"sample_mflix\");\n// Access the database's \"movies\" collection\nconst movies = database.collection(\"movies\");\n\n// Create an ascending index on the \"type\" and \"genre\" fields\n// in the \"movies\" collection.\nconst result = await movies.createIndex({ type: 1, genre: 1 });\nconsole.log(`Index created: ${result}`);"
                },
                {
                    "lang": "js",
                    "value": "// Define a query to find movies in the \"Drama\" genre\nconst query = { type: \"movie\", genre: \"Drama\" };\n// Define sorting criteria for the query results\nconst sort = { type: 1, genre: 1 };\n// Include only the type and genre fields in the query results\nconst projection = { _id: 0, type: 1, genre: 1 };\n\n// Execute the query using the defined criteria and projection\nconst cursor = movies\n  .find(query)\n  .sort(sort)\n  .project(projection);"
                },
                {
                    "lang": "js",
                    "value": "const database = client.db(\"sample_mflix\");\nconst movies = database.collection(\"movies\");\n\n// Create a multikey index on the \"cast\" field in the \"movies\" collection\nconst result = await movies.createIndex({ cast: 1 });"
                },
                {
                    "lang": "js",
                    "value": "const query = { cast: \"Viola Davis\" };\nconst projection = { _id: 0, cast: 1 , title: 1 };\n\n// Perform a find operation with the preceding filter and projection\nconst cursor = movies\n  .find(query)\n  .project(projection);"
                },
                {
                    "lang": "javascript",
                    "value": "const db = client.db('tea');\nawait db.createCollection('ratings', {\n  clusteredIndex: {\n    key: { _id: 1 },\n    unique: true\n  }\n});"
                },
                {
                    "lang": "js",
                    "value": "// Get the database and collection on which to create the index \nconst myDB = client.db(\"testDB\");\nconst myColl = myDB.collection(\"blogPosts\");\n\n// Create a text index on the \"title\" and \"body\" fields\nconst result = await myColl.createIndex(\n  { title: \"text\", body: \"text\" },\n  { default_language: \"english\" },\n  { weights: { body: 10, title: 3 } }\n);"
                },
                {
                    "lang": "js",
                    "value": "// Query for documents where body or title contain \"life ahead\"\nconst query = { $text: { $search: \"life ahead\" } };\n\n// Show only the title field\nconst projection = { _id: 0, title: 1 };\n\n// Execute the find operation\nconst cursor = myColl.find(query).project(projection);"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"_id\" : ObjectId(\"59a47286cfa9a3a73e51e75c\"),\n   \"theaterId\" : 104,\n   \"location\" : {\n      \"address\" : {\n         \"street1\" : \"5000 W 147th St\",\n         \"city\" : \"Hawthorne\",\n         \"state\" : \"CA\",\n         \"zipcode\" : \"90250\"\n      },\n      \"geo\" : {\n         \"type\" : \"Point\",\n         \"coordinates\" : [\n            -118.36559,\n            33.897167\n         ]\n      }\n   }\n}"
                },
                {
                    "lang": "js",
                    "value": "const database = client.db(\"sample_mflix\");\nconst movies = database.collection(\"movies\");\n\n/* Create a 2dsphere index on the \"location.geo\" field in the\n\"movies\" collection */\nconst result = await movies.createIndex({ \"location.geo\": \"2dsphere\" });\n\n// Print the result of the index creation\nconsole.log(`Index created: ${result}`);"
                },
                {
                    "lang": "none",
                    "value": "E11000 duplicate key error index"
                },
                {
                    "lang": "js",
                    "value": "const database = client.db(\"sample_mflix\");\nconst movies = database.collection(\"movies\");\n\n// Create a unique index on the \"theaterId\" field in the \"theaters\" collection.\nconst result = await movies.createIndex({ theaterId: 1 }, { unique: true });\nconsole.log(`Index created: ${result}`);"
                },
                {
                    "lang": "javascript",
                    "value": "// Create a search index\nconst index1 = {\n    name: \"search1\",\n    definition: {\n        \"mappings\": {\n            \"dynamic\": true\n        }\n    }\n}\nawait collection.createSearchIndex(index1);"
                },
                {
                    "lang": "javascript",
                    "value": "// Create a Vector Search index\nconst vectorSearchIdx = {\n    name: \"vsidx1\",\n    type: \"vectorSearch\",\n    definition: {\n        fields: [{\n            type: \"vector\",\n            numDimensions: 384,\n            path: \"summary\",\n            similarity: \"dotProduct\"\n        }]\n    }\n}\n\nawait collection.createSearchIndex(vectorSearchIdx);"
                },
                {
                    "lang": "javascript",
                    "value": "// List search indexes\nconst result = await collection.listSearchIndexes().toArray();\nconsole.log(\"Existing search indexes:\\n\");\nfor (const doc in result) {\n    console.log(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// Update a search index\nconst index2 = {\n    \"mappings\": {\n        \"dynamic\": true,\n        \"fields\": {\n            \"description\": {\n                \"type\": \"string\"\n            }\n        }\n    }\n}\nawait collection.updateSearchIndex(\"search1\", index2);"
                },
                {
                    "lang": "javascript",
                    "value": "// Dropping (deleting) a search index\nawait collection.dropSearchIndex(\"search1\");"
                }
            ],
            "preview": "Indexes are data structures that support the efficient execution of queries in\nMongoDB. They contain copies of parts of the data in documents to make\nqueries more efficient.",
            "tags": "node.js, code example, Atlas search",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/logging",
            "title": "Logging",
            "headings": [
                "Temporary Alternative"
            ],
            "paragraphs": "The driver doesn't use the logger in versions 4.0 and later.\nAttempting to use prior logger settings in this version won't print\nanything in the log. Instead, see our monitoring guides: Command Monitoring Cluster Monitoring Connection Pool Monitoring We are developing a new logging framework. In the meantime, you can output monitor events\nby using the following snippet:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const uri = \"mongodb+srv://<user>:<password>@<cluster-url>?writeConcern=majority\";\nconst client = new MongoClient(uri, { monitorCommands:true });\n\nclient.on('commandStarted', (event) => console.debug(event));\nclient.on('commandSucceeded', (event) => console.debug(event));\nclient.on('commandFailed', (event) => console.debug(event));"
                }
            ],
            "preview": "We are developing a new logging framework. In the meantime, you can output monitor events\nby using the following snippet:",
            "tags": "code example, deprecated, replace",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/monitoring/cluster-monitoring",
            "title": "Cluster Monitoring",
            "headings": [
                "Overview",
                "Event Subscription Example",
                "Event Descriptions",
                "Example Event Documents",
                "serverDescriptionChanged",
                "serverHeartbeatStarted",
                "serverHeartbeatSucceeded",
                "serverHeartbeatFailed",
                "serverOpening",
                "serverClosed",
                "topologyOpening",
                "topologyClosed",
                "topologyDescriptionChanged"
            ],
            "paragraphs": "This guide shows you how to monitor topology events in a MongoDB instance,\nreplica set, or sharded cluster. The driver creates topology events, also\nknown as Server Discovery and Monitoring (SDAM) events, when there is\na change in the state of the instance or cluster that you connected to.\nFor example, the driver creates an event when you establish a new connection\nor if the cluster elects a new primary. The following sections demonstrate how to record topology changes in your application\nand explore the information provided in these events. You can access one or more SDAM events using the driver by subscribing to them\nin your application. The following example demonstrates connecting to a\nreplica set and subscribing to one of the SDAM events created by the MongoDB\ndeployment: You can subscribe to any of the following SDAM events: Event Name Description serverOpening Created when a connection to an instance opens. serverClosed Created when a connection to an instance closes. serverDescriptionChanged Created when an instance state changes (such as from secondary to\nprimary). topologyOpening Created before attempting a connection to an instance. topologyClosed Created after all instance connections in the topology close. topologyDescriptionChanged Created when the topology changes, such as an election of a new\nprimary or a  mongos  proxy disconnecting. serverHeartbeatStarted Created before issuing a  hello  command to a MongoDB instance. serverHeartbeatSucceeded Created when the  hello  command returns successfully from a\nMongoDB instance. serverHeartbeatFailed Created when a  hello  command issued to a specific MongoDB\ninstance fails to return a successful response. The following sections show sample output for each type of SDAM event. The  type  field of the  ServerDescription  object in this event contains\none of the following possible values: Type Description Unknown Unknown instance Standalone Standalone instance Mongos Mongos proxy instance PossiblePrimary At least one server recognizes this as the primary, but is not yet\nverified by all instances. RSPrimary Primary instance RSSecondary Secondary instance RSArbiter Arbiter instance RSOther See the  RSGhost and RSOther specification \nfor more details RSGhost See the  RSGhost and RSOther specification \nfor more details The  type  field of the  TopologyDescription  object in this event contains\none of the following possible values: Type Description Single Standalone instance ReplicaSetWithPrimary Replica set with a primary ReplicaSetNoPrimary Replica set with no primary Sharded Sharded cluster Unknown Unknown topology",
            "code": [
                {
                    "lang": "javascript",
                    "value": "/* Subscribe to SDAM event */\n\nconst { MongoClient } = require(\"mongodb\");\n\n// Replace the following with your MongoDB deployment's connection string\nconst uri = \"mongodb+srv://<clusterUrl>/?replicaSet=rs&writeConcern=majority\";\n\nconst client = new MongoClient(uri);\n\n// Replace <event name> with the name of the event you are subscribing to\nconst eventName = \"<event name>\";\n\n// Subscribe to a specified event and print a message when the event is received\nclient.on(eventName, event => {\n  console.log(`received ${eventName}: ${JSON.stringify(event, null, 2)}`);\n});\n\nasync function run() {\n  try {\n    // Establish and verify connection to the database\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully\");\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "ServerDescriptionChangedEvent {\n  topologyId: 0,\n  address: 'localhost:27017',\n  previousDescription: ServerDescription {\n    address: 'localhost:27017',\n    error: null,\n    roundTripTime: 0,\n    lastUpdateTime: 1571251089030,\n    lastWriteDate: null,\n    opTime: null,\n    type: 'Unknown',\n    minWireVersion: 0,\n    maxWireVersion: 0,\n    hosts: [],\n    passives: [],\n    arbiters: [],\n    tags: []\n  },\n  newDescription: ServerDescription {\n    address: 'localhost:27017',\n    error: null,\n    roundTripTime: 0,\n    lastUpdateTime: 1571251089051,\n    lastWriteDate: 2019-10-16T18:38:07.000Z,\n    opTime: { ts: Timestamp, t: 18 },\n    type: 'RSPrimary',\n    minWireVersion: 0,\n    maxWireVersion: 7,\n    maxBsonObjectSize: 16777216,\n    maxMessageSizeBytes: 48000000,\n    maxWriteBatchSize: 100000,\n    me: 'localhost:27017',\n    hosts: [ 'localhost:27017' ],\n    passives: [],\n    arbiters: [],\n    tags: [],\n    setName: 'rs',\n    setVersion: 1,\n    electionId: ObjectID,\n    primary: 'localhost:27017',\n    logicalSessionTimeoutMinutes: 30,\n    '$clusterTime': ClusterTime\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "ServerHeartbeatStartedEvent {\n  connectionId: 'localhost:27017'\n}"
                },
                {
                    "lang": "javascript",
                    "value": "ServerHeartbeatSucceededEvent {\n  duration: 1.939997,\n  reply:{\n    hosts: [ 'localhost:27017' ],\n    setName: 'rs',\n    setVersion: 1,\n    isWritablePrimary: true,\n    secondary: false,\n    primary: 'localhost:27017',\n    me: 'localhost:27017',\n    electionId: ObjectID,\n    lastWrite: {\n      opTime: { ts: [Timestamp], t: 18 },\n      lastWriteDate: 2019-10-16T18:38:17.000Z,\n      majorityOpTime: { ts: [Timestamp], t: 18 },\n      majorityWriteDate: 2019-10-16T18:38:17.000Z\n    },\n    maxBsonObjectSize: 16777216,\n    maxMessageSizeBytes: 48000000,\n    maxWriteBatchSize: 100000,\n    localTime: 2019-10-16T18:38:19.589Z,\n    logicalSessionTimeoutMinutes: 30,\n    minWireVersion: 0,\n    maxWireVersion: 7,\n    readOnly: false,\n    ok: 1,\n    operationTime: Timestamp,\n    '$clusterTime': ClusterTime\n  },\n  connectionId: 'localhost:27017'\n}"
                },
                {
                    "lang": "javascript",
                    "value": "ServerHeartbeatFailed {\n  duration: 20,\n  failure: MongoError('some error'),\n  connectionId: 'localhost:27017'\n}"
                },
                {
                    "lang": "javascript",
                    "value": "ServerOpeningEvent {\n  topologyId: 0,\n  address: 'localhost:27017'\n}"
                },
                {
                    "lang": "javascript",
                    "value": "ServerClosedEvent {\n  topologyId: 0,\n  address: 'localhost:27017'\n}"
                },
                {
                    "lang": "javascript",
                    "value": "TopologyOpeningEvent {\n  topologyId: 0\n}"
                },
                {
                    "lang": "javascript",
                    "value": "TopologyClosedEvent {\n  topologyId: 0\n}"
                },
                {
                    "lang": "javascript",
                    "value": "TopologyDescriptionChangedEvent {\n  topologyId: 0,\n  previousDescription: TopologyDescription {\n    type: 'ReplicaSetNoPrimary',\n    setName: null,\n    maxSetVersion: null,\n    maxElectionId: null,\n    servers: Map {\n      'localhost:27017' => ServerDescription\n    },\n    stale: false,\n    compatible: true,\n    compatibilityError: null,\n    logicalSessionTimeoutMinutes: null,\n    heartbeatFrequencyMS: 10000,\n    localThresholdMS: 15,\n    options: Object,\n    error: undefined,\n    commonWireVersion: null\n  },\n  newDescription: TopologyDescription {\n    type: 'ReplicaSetWithPrimary',\n    setName: 'rs',\n    maxSetVersion: 1,\n    maxElectionId: null,\n    servers: Map {\n      'localhost:27017' => ServerDescription\n    },\n    stale: false,\n    compatible: true,\n    compatibilityError: null,\n    logicalSessionTimeoutMinutes: 30,\n    heartbeatFrequencyMS: 10000,\n    localThresholdMS: 15,\n    options: Object,\n    error: undefined,\n    commonWireVersion: 7\n  }\n}"
                }
            ],
            "preview": "This guide shows you how to monitor topology events in a MongoDB instance,\nreplica set, or sharded cluster. The driver creates topology events, also\nknown as Server Discovery and Monitoring (SDAM) events, when there is\na change in the state of the instance or cluster that you connected to.\nFor example, the driver creates an event when you establish a new connection\nor if the cluster elects a new primary.",
            "tags": "code example, node.js, watch",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/monitoring/command-monitoring",
            "title": "Command Monitoring",
            "headings": [
                "Overview",
                "Event Subscription Example",
                "Event Descriptions",
                "Example Event Documents",
                "commandStarted",
                "commandSucceeded",
                "commandFailed"
            ],
            "paragraphs": "This guide shows you how to monitor the success or failure of commands\nsent by the driver to your MongoDB deployment. The following sections demonstrate how to record command status in your\napplication and explore the information provided in these events. You can access one or more command monitoring events using the driver by\nsubscribing to them in your application. The following example demonstrates\nconnecting to a replica set and subscribing to one of the command monitoring\nevents created by the MongoDB deployment: Command monitoring is disabled by default. To enable command\nmonitoring, pass the  monitorCommands  option as  true  to\nyour  MongoClient  constructor. You can subscribe to any of the following command monitoring events: Event Name Description commandStarted Created when a command is started. commandSucceeded Created when a command succeeded. commandFailed Created when a command failed. The following sections show sample output for each type of command monitoring event.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "/* Subscribe to an event */\n\nconst { MongoClient } = require(\"mongodb\");\n\n// Replace the following with your MongoDB deployment's connection string\nconst uri = \"mongodb+srv://<clusterUrl>/?replicaSet=rs&writeConcern=majority\";\n\nconst client = new MongoClient(uri, { monitorCommands:true });\n\n// Replace <event name> with the name of the event you are subscribing to\nconst eventName = \"<event name>\";\n\n// Subscribe to a specified event and print a message when the event is received\nclient.on(eventName, event => {\n  console.log(`received ${eventName}: ${JSON.stringify(event, null, 2)}`);\n});\n\nasync function run() {\n  try {\n    // Establish and verify connection to the \"admin\" database\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"Connected successfully\");\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "CommandStartedEvent {\n  requestId: 1534,\n  databaseName: \"app\",\n  commandName: \"find\",\n  address: 'localhost:27017',\n  connectionId: 812613,\n  command: {\n    find: { firstName: \"Jane\", lastName: \"Doe\" }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "CommandSucceededEvent {\n  requestId: 1534,\n  commandName: \"find\",\n  address: 'localhost:27017',\n  connectionId: 812613,\n  duration: 15,\n  reply: {\n    cursor: {\n      firstBatch: [\n        {\n          _id: ObjectId(\"5e8e2ca217b5324fa9847435\"),\n          firstName: \"Jane\",\n          lastName: \"Doe\"\n        }\n      ],\n      _id: 0,\n      ns: \"app.users\"\n    },\n    ok: 1,\n    operationTime: 1586380205\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "CommandFailedEvent {\n  requestId: 1534,\n  commandName: \"find\",\n  address: 'localhost:27017',\n  connectionId: 812613,\n  failure: Error(\"something failed\"),\n  duration: 11\n}"
                }
            ],
            "preview": "This guide shows you how to monitor the success or failure of commands\nsent by the driver to your MongoDB deployment.",
            "tags": "code example, node.js, watch, command status",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/monitoring/connection-monitoring",
            "title": "Connection Pool Monitoring",
            "headings": [
                "Overview",
                "Event Subscription Examples",
                "Event Descriptions",
                "Example Event Documents",
                "connectionPoolCreated",
                "connectionPoolReady",
                "connectionPoolClosed",
                "connectionCreated",
                "connectionReady",
                "connectionClosed",
                "connectionCheckOutStarted",
                "connectionCheckOutFailed",
                "connectionCheckedOut",
                "connectionCheckedIn",
                "connectionPoolCleared"
            ],
            "paragraphs": "This guide shows you how to monitor the driver's  connection pool . A\nconnection pool is a set of open TCP connections your driver maintains\nwith a MongoDB instance. Connection pools help reduce the number of\nnetwork handshakes your application needs to perform and can help your\napplication run faster. The following sections demonstrate how to record connection pool events in your\napplication and explore the information provided in these events. You can access one or more connection pool events using the driver by\nsubscribing to them in your application. The following example demonstrates\nconnecting to a replica set and subscribing to one of the connection\npool monitoring events created by the MongoDB deployment: Connection pool monitoring events can aid you in debugging and understanding\nthe behavior of your application's connection pool. The following example uses connection\npool monitoring events to return a count of checked-out connections in the pool: You can subscribe to any of the following connection pool monitoring events: Event Name Description connectionPoolCreated Created when a connection pool is created. connectionPoolReady Created when a connection pool is ready. connectionPoolClosed Created when a connection pool is closed before server\ninstance destruction. connectionCreated Created when a connection is created, but not necessarily\nwhen it is used for an operation. connectionReady Created after a connection has successfully completed a\nhandshake and is ready to be used for operations. connectionClosed Created when a connection is closed. connectionCheckOutStarted Created when an operation attempts to acquire a connection for\nexecution. connectionCheckOutFailed Created when an operation fails to acquire a connection for\nexecution. connectionCheckedOut Created when an operation successfully acquires a connection for\nexecution. connectionCheckedIn Created when a connection is checked back into the pool after an operation\nis executed. connectionPoolCleared Created when a connection pool is cleared. The following sections show sample output for each type of connection\npool monitoring event.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the following with your MongoDB deployment's connection string\nconst uri =\n  \"mongodb+srv://<clusterUrl>/?replicaSet=rs&writeConcern=majority\";\n\nconst client = new MongoClient(uri);\n\n// Replace <event name> with the name of the event you are subscribing to\nconst eventName = \"<event name>\";\n\n// Subscribe to the event\nclient.on(eventName, (event) =>\n  console.log(\"\\nreceived event:\\n\", event)\n);\n\nasync function run() {\n  try {\n    // Establish and verify connection\n    await client.db(\"admin\").command({ ping: 1 });\n    console.log(\"\\nConnected successfully!\\n\");\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "function connectionPoolStatus(client) {\n  let checkedOut = 0;\n\n  function onCheckout() {\n    checkedOut++;\n  }\n\n  function onCheckin() {\n    checkedOut--;\n  }\n\n  function onClose() {\n    client.removeListener('connectionCheckedOut', onCheckout);\n    client.removeListener('connectionCheckedIn', onCheckin);\n\n    checkedOut = NaN;\n  }\n\n  // Decreases count of connections checked out of the pool when connectionCheckedIn event is triggered\n  client.on('connectionCheckedIn', onCheckin);\n\n  // Increases count of connections checked out of the pool when connectionCheckedOut event is triggered\n  client.on('connectionCheckedOut', onCheckout);\n\n  // Cleans up event listeners when client is closed\n  client.on('close', onClose);\n\n  return {\n    count: () => checkedOut,\n    cleanUp: onClose\n  };\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionPoolCreatedEvent {\n  time: 2023-02-13T15:54:06.944Z,\n  address: '...',\n  options: {...}\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionPoolReadyEvent {\n  time: 2023-02-13T15:56:38.440Z,\n  address: '...'\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionPoolClosedEvent {\n  time: 2023-02-13T15:56:38.440Z,\n  address: '...'\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionCreatedEvent {\n  time: 2023-02-13T15:56:38.291Z,\n  address: '...',\n  connectionId: 1\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionReadyEvent {\n  time: 2023-02-13T15:56:38.291Z,\n  address: '...',\n  connectionId: 1\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionClosedEvent {\n  time: 2023-02-13T15:56:38.439Z,\n  address: '...',\n  connectionId: 1,\n  reason: 'poolClosed',\n  serviceId: undefined\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionCheckOutStartedEvent {\n  time: 2023-02-13T15:56:38.291Z,\n  address: '...',\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionCheckOutFailedEvent {\n  time: 2023-02-13T15:56:38.291Z,\n  address: '...',\n  reason: ...\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionCheckedOutEvent {\n  time: 2023-02-13T15:54:07.188Z,\n  address: '...',\n  connectionId: 1\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionCheckedInEvent {\n  time: 2023-02-13T15:54:07.189Z,\n  address: '...',\n  connectionId: 1\n}"
                },
                {
                    "lang": "none",
                    "value": "ConnectionPoolClearedEvent {\n  time: 2023-02-13T15:56:38.439Z,\n  address: '...',\n  serviceId: undefined,\n  interruptInUseConnections: true,\n}"
                }
            ],
            "preview": "This guide shows you how to monitor the driver's connection pool. A\nconnection pool is a set of open TCP connections your driver maintains\nwith a MongoDB instance. Connection pools help reduce the number of\nnetwork handshakes your application needs to perform and can help your\napplication run faster.",
            "tags": "code example, node.js, watch, deployment",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/monitoring",
            "title": "Monitoring",
            "headings": [],
            "paragraphs": "Cluster Monitoring : monitoring\nchanges in a cluster Command Monitoring : monitoring\nthe execution status of commands Connection Pool Monitoring : monitoring\nthe driver's connection pool",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/promises",
            "title": "Promises",
            "headings": [
                "Overview",
                "Promises",
                "Await",
                "Operational Considerations"
            ],
            "paragraphs": "The Node.js driver uses the asynchronous Javascript API to communicate with\nyour MongoDB cluster. Asynchronous Javascript allows you to execute operations without waiting for\nthe processing thread to become free. This helps prevent your application\nfrom becoming unresponsive when\nexecuting long-running operations. For more information about asynchronous\nJavascript, see the MDN web documentation on\n Asynchronous Javascript . This section describes  Promises  that you can use with the Node.js driver to\naccess the results of your method calls to your MongoDB cluster. A Promise is an object returned by the asynchronous method call that allows\nyou to access information on the eventual success or failure of the operation\nthat they wrap. The Promise is in the  Pending  state if the operation is\nstill running,  Fulfilled  if the operation completed successfully, and\n Rejected  if the operation threw an exception. For more information on\nPromises and related terminology, see the MDN documentation on\n Promises . Most driver methods that communicate with your MongoDB cluster, such as\n findOneAndUpdate()  and  countDocuments() , return Promise\nobjects and already contain logic to handle the success or failure of the\noperation. You can define your own logic that executes once the Promise reaches the\n Fulfilled  or  Rejected  state by appending the  then()  method.\nThe first parameter of  then()  is the method that gets called when the\nPromise reaches the  Fulfilled  state and the optional second parameter is\nthe method that gets called when it reaches the  Rejected  state. The\n then()  method returns a Promise to which you can append more\n then()  methods. When you append one or more  then()  methods to a Promise, each call passes\nits execution result to the next one. This pattern is called\n Promise chaining . The following code snippet shows an example of Promise\nchaining by appending a single  then()  method. To handle only Promise transitions to the  Rejected  state, use the  catch()  method\nrather than passing a  null  first parameter to  then() . The  catch()  method\naccepts a single callback that is executed when the Promise transitions to the  Rejected \nstate. The  catch()  method is often appended at the end of a Promise chain to\nhandle any exceptions thrown. The following code snippet demonstrates appending\na  catch()  method to the end of a Promise chain. Certain methods in the driver such as  find()  return a  Cursor \ninstead of a Promise. To determine what type each method returns, see\nthe  Node.js API documentation . If you are using  async  functions, you can use the  await  operator on\na Promise to pause further execution until the Promise reaches either the\n Fulfilled  or  Rejected  state and returns. Since the  await  operator\nwaits for the resolution of the Promise, you can use it in place of\nPromise chaining to sequentially execute your logic. The following code\nsnippet uses  await  to execute the same logic as the first Promise\nchaining example. For more information, see the MDN documentation on\n await . One common mistake when using  async  methods is to forget to use  await \noperator on Promises to get the value of the result rather than the Promise\nobject. Consider the following example in which we iterate over a cursor\nusing  hasNext() , which returns a Promise that resolves to a boolean that\nindicates whether more results exist, and  next()  which returns a\nPromise that resolves to the next entry the cursor is pointing to. Since the call to  hasNext()  returns a  Promise , the conditional\nstatement returns  true  regardless of the value that it resolves to. If we alter the code to  await  the call to  next()  only, as demonstrated\nin the following code snippet, it throws the following error:\n MongoError: Cursor is closed . While  hasNext()  is not called until after the result of  next()  returns,\nthe call to  hasNext()  returns a Promise which evaluates to  true  rather\nthan the value it resolves to, similar to the prior example. The code\nattempts to call  next()  on a Cursor that has already returned its results\nand closed as a result. If we alter the code to only  await  the call to  hasNext()  as shown in\nthe following example, the console prints Promise objects rather than the\ndocument objects. Use  await  before both the  hasNext()  and  next()  method calls to\nensure that you are operating on the correct return values as demonstrated\nin the following code:",
            "code": [
                {
                    "lang": "js",
                    "value": "collection\n  .updateOne({ name: \"Mount McKinley\" }, { $set: { meters: 6190 } })\n  .then(\n    res => console.log(`Updated ${res.result.n} documents`),\n    err => console.error(`Something went wrong: ${err}`),\n  );"
                },
                {
                    "lang": "js",
                    "value": "deleteOne({ name: \"Mount Doom\" })\n  .then(result => {\n    if (result.deletedCount !== 1) {\n      throw \"Could not find Mount Doom!\";\n    }\n    return new Promise((resolve, reject) => {\n      ...\n    });\n  })\n  .then(result => console.log(`Vanquished ${result.quantity} Nazgul`))\n  .catch(err => console.error(`Fatal error occurred: ${err}`));"
                },
                {
                    "lang": "js",
                    "value": "async function run() {\n  ...\n  try {\n    res = await myColl.updateOne(\n      { name: \"Mount McKinley\" },\n      { $set: { meters: 6190 } },\n    );\n    console.log(`Updated ${res.result.n} documents`);\n  } catch (err) {\n    console.error(`Something went wrong: ${err}`);\n  }\n}"
                },
                {
                    "lang": "js",
                    "value": "async function run() {\n  ...\n  // WARNING: this snippet may cause an infinite loop\n  const cursor = myColl.find();\n\n  while (cursor.hasNext()) {\n    console.log(cursor.next());\n  }\n}"
                },
                {
                    "lang": "js",
                    "value": "async function run() {\n  ...\n  // WARNING: this snippet throws a MongoError\n  const cursor = myColl.find();\n\n  while (cursor.hasNext()) {\n    console.log(await cursor.next());\n  }\n}"
                },
                {
                    "lang": "js",
                    "value": "async function run() {\n  ...\n  // WARNING: this snippet prints Promises instead of the objects they resolve to\n  const cursor = myColl.find();\n\n  while (await cursor.hasNext()) {\n    console.log(cursor.next());\n  }\n}"
                },
                {
                    "lang": "js",
                    "value": "async function run() {\n  ...\n  const cursor = myColl.find();\n\n  while (await cursor.hasNext()) {\n    console.log(await cursor.next());\n  }\n}"
                }
            ],
            "preview": "The Node.js driver uses the asynchronous Javascript API to communicate with\nyour MongoDB cluster.",
            "tags": "code example, node.js, operation status, chain",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/run-command",
            "title": "Run a Command",
            "headings": [
                "Overview",
                "Execute a Command",
                "Command Options",
                "Response",
                "Example",
                "Output",
                "Additional Information"
            ],
            "paragraphs": "In this guide, you can learn how to run a database command with the\nNode.js driver. You can use database commands to perform a variety of\nadministrative and diagnostic tasks, such as fetching server statistics,\ninitializing a replica set, or running an aggregation pipeline. The driver provides wrapper methods for many database commands.\nWe recommend using driver methods instead of executing database\ncommands when possible. To perform administrative tasks, use the  MongoDB Shell \ninstead of the Node.js driver. Calling the  db.runCommand() \nmethod inside the shell is the preferred method to issue database\ncommands, as it provides a consistent interface between the shell and\ndrivers. To run a database command, you must specify the command and any relevant\nparameters in a document, then pass this document to a\ncommand execution method. The Node.js driver provides the following methods\nto run database commands: The following code shows how you can use the  command() \nmethod to run the  hello  command, which returns information about\nthe current member's role in the replica set, on a database: For a full list of database commands and corresponding parameters, see\nthe  Additional Information section . command() , which returns the command response as a\n Document  type. You can use this method with any database command. runCursorCommand() , which returns the command response as an iterable\n RunCommandCursor  type. You can use this method only if your database command\nreturns multiple result documents. You can specify optional command behavior for the  command() \nand  runCursorCommand()  methods. The  command()  method accepts a  RunCommandOptions  object. To learn\nmore about the  RunCommandOptions  type, see the  API documentation . The  runCursorCommand() method  accepts a  RunCursorCommandOptions \nobject. To learn more about the  RunCursorCommandOptions  type, see\nthe  API documentation . Starting in version 6.0 of the Node.js driver, you can pass only the\nfollowing options to the  command()  method: You can set more options in the document that you pass to the  command()  method. To\nlearn more about a command and the options that it accepts, locate the command and follow\nthe link on the  Database Commands  section of the Server\nmanual. The following code shows how to specify a  grantRolesToUser  command\nthat executes with a  majority  write concern: comment enableUtf8Validation raw readPreference session The  command()  and  runCursorCommand()  methods ignore\nthe read preference setting you may have set on your  Db  object.\nBy default, these methods use the  primary  read preference. The following code shows how to specify a read preference and pass it\nas an option to the  command()  method: For more information on read preference options, see  Read\nPreference  in the Server manual. Each method returns a  Document  object or a cursor that contains\nthe response from the database after the command has been executed. Each\ndatabase command performs a different function, so the response content\ncan vary across commands. However, every response contains documents\nwith the following fields: Field Description <command result> Provides fields specific to the database command. For example,\n count  returns the  n  field and  explain  returns the\n queryPlanner  field. ok Indicates whether the command has succeeded ( 1 )\nor failed ( 0 ). operationTime Indicates the logical time of the operation. MongoDB uses the\nlogical time to order operations. To learn more about logical time, see our  blog post about\nthe Global Logical Clock . $clusterTime Provides a document that returns the signed cluster time. Cluster time is a\nlogical time used for ordering of operations. The document contains the following fields: clusterTime , which is the timestamp of the highest known cluster time for the member. signature , which is a document that contains the hash of the cluster time and the ID\nof the key used to sign the cluster time. The following code shows how you can use the  runCursorCommand()  method to\nrun the  checkMetadataConsistency  command on the  testDB  database\nand iterate through the results: The output contains the contents of the cursor object. The documents\ndescribe any metadata inconsistencies in the database: If you store the command response in a cursor, you see only the\ncommand result documents when you access the contents of the cursor. You won't\nsee the  ok ,  operationTime , and  $clusterTime  fields. For more information about the concepts in this guide, see the following documentation: To learn how to retrieve data from a cursor, see the\n Access Data From a Cursor  fundamentals page. db.runCommand() Database Commands hello Command find Command",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const result = await myDB.command({ hello: 1 });"
                },
                {
                    "lang": "javascript",
                    "value": "const commandDoc = {\n    grantRolesToUser: \"user011\",\n    roles: [ \"readWrite\" ],\n    writeConcern: { w: \"majority\" }\n};\nconst result = await myDB.command(commandDoc);"
                },
                {
                    "lang": "javascript",
                    "value": "const commandOptions = { readPreference: \"nearest\" };\nconst result = await myDB.command(commandDoc, commandOptions);"
                },
                {
                    "lang": "javascript",
                    "value": "// Connect to the \"testDB\" database\nconst db = client.db(\"testDB\");\n\n// Run a cursor command to check metadata consistency within the database\nconst cursor = await db.runCursorCommand({\n    checkMetadataConsistency: 1,\n});\n// Iterate through the cursor's results and print the contents\nfor await (const doc of cursor) {\n  console.log(doc);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  type: ...,\n  description: ...,\n  details: {\n    namespace: ...,\n    info: ...\n  }\n}\n{\n  type: ...,\n  description: ...,\n  details: {\n    namespace: ...,\n    collectionUUID: ...,\n    maxKeyObj: ...,\n    ...\n  }\n}"
                }
            ],
            "preview": "In this guide, you can learn how to run a database command with the\nNode.js driver. You can use database commands to perform a variety of\nadministrative and diagnostic tasks, such as fetching server statistics,\ninitializing a replica set, or running an aggregation pipeline.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/stable-api",
            "title": "Stable API",
            "headings": [
                "Overview",
                "Enable the Stable API on a MongoDB Client",
                "Stable API Options"
            ],
            "paragraphs": "The Stable API feature requires MongoDB Server 5.0 or later. Use the Stable API feature only if all the MongoDB\nservers you are connecting to support this feature. In this guide, you can learn how to specify the  Stable API  when\nconnecting to a MongoDB instance or replica set. You can use the\nStable API feature to force the server to run operations with behavior\ncompatible with the specified  API version . An API version defines the\nexpected behavior of the operations it covers and the format of server\nresponses. If you change to a different API version, the operations are not\nguaranteed to be compatible and the server responses are not guaranteed to\nbe similar. When you use the Stable API feature with an official MongoDB driver, you\ncan update your driver or server without worrying about backward compatibility\nissues of the commands covered by the Stable API. See the MongoDB reference page on the  Stable API \nfor more information including a list of commands it covers. The following sections describe how you can enable the Stable API for\nyour MongoDB client and the options that you can specify. To enable the Stable API, you must specify an API version in the  MongoClientOptions \npassed to your  MongoClient . Once you instantiate a  MongoClient  instance with\na specified API version, all commands you run with that client use that\nversion of the Stable API. The example below shows how you can instantiate a  MongoClient  that\nsets the Stable API version and connects to a server by performing the\nfollowing operations: For more information on the methods and classes referenced in this\nsection, see the following API Documentation: To run commands that are not covered by the Stable API, make sure the\n\"strict\" option is disabled. See the section on\n Stable API Options  for more\ninformation. which you want to run a command. Specify a server URI to connect to. Specify a Stable API version in the  MongoClientOptions  object, using a\nconstant from the  ServerApiVersion  object. Instantiate a  MongoClient , passing the URI and the  MongoClientOptions \nto the constructor. If you specify an API version and connect to a MongoDB Server that does\nnot support the Stable API, your application may throw an error when\nconnecting to your MongoDB Server with the following text: ServerApiVersion MongoClientOptions MongoClient You can enable or disable optional behavior related to the Stable API as\ndescribed in the following table. The following example shows how you can set the options of the  ServerApi \ninterface. For more information on the options in this section, see the following\nAPI Documentation: Option Name Description version strict deprecationErrors ServerApi",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const { MongoClient, ServerApiVersion } = require(\"mongodb\");\n\n// Replace the placeholders in the connection string uri with your credentials\nconst uri = \"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&w=majority\";\n\n// Create a client with options to specify Stable API Version 1\nconst client = new MongoClient(uri, { serverApi: ServerApiVersion.v1 });"
                },
                {
                    "lang": "none",
                    "value": "MongoParseError: Invalid server API version=..."
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient, ServerApiVersion } = require(\"mongodb\");\n\n// Replace the placeholders in the connection string uri with your credentials\nconst uri = \"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&w=majority\";\n\n/* Create a client with options to specify Stable API Version 1, return\nerrors for commands outside of the API version, and raise exceptions\nfor deprecated commands */\nconst client = new MongoClient(uri,\n    {\n        serverApi: {\n            version: ServerApiVersion.v1,\n            strict: true,\n            deprecationErrors: true,\n        }\n    });"
                }
            ],
            "preview": "In this guide, you can learn how to specify the Stable API when\nconnecting to a MongoDB instance or replica set. You can use the\nStable API feature to force the server to run operations with behavior\ncompatible with the specified API version. An API version defines the\nexpected behavior of the operations it covers and the format of server\nresponses. If you change to a different API version, the operations are not\nguaranteed to be compatible and the server responses are not guaranteed to\nbe similar.",
            "tags": "code example, node.js, safe, breaking change",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/time-series",
            "title": "Time Series",
            "headings": [
                "Overview",
                "Create a Time Series Collection",
                "Query a Time Series Collection"
            ],
            "paragraphs": "In this guide, you can learn about time series collections in the MongoDB\nNode.js driver. We recommend that you create a time series collection using the MongoDB Shell.\nLearn more about how to install and run the MongoDB Shell in the  MongoDB Shell documentation .\nFor detailed instructions on creating a time series collection\nusing the MongoDB Shell, see our\n MongoDB Manual entry on time series collections . Since you query a time series collection in the same way you query other\ncollection types in MongoDB, the Node.js driver has no features specifically for\nquerying time series data. For more information on querying data in the MongoDB Node.js driver, see the\nfollowing resources: Guide On Read Operations Guide On Aggregation MongoDB version 5.0 introduces window functions into the MongoDB aggregation\npipeline. You can use window functions to perform operations on a\ncontiguous span of time series data. For more information, see\n the reference documentation for the $setWindowFields aggregation stage .",
            "code": [],
            "preview": "In this guide, you can learn about time series collections in the MongoDB\nNode.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/transactions",
            "title": "Transactions",
            "headings": [
                "Overview",
                "Transaction APIs",
                "Core API",
                "Convenient Transaction API",
                "Transaction Options",
                "Transaction Errors"
            ],
            "paragraphs": "In this guide, you can learn how to use the\nNode.js driver to perform  transactions . Transactions allow you\nto run a series of operations that do not change any data until the\nentire transaction is committed. If any operation in the transaction fails, the\ndriver ends the transaction and discards all data changes before they\never become visible. This feature is called  atomicity . Since all write operations on a single document in MongoDB are atomic, you\nmight want to use transactions to make an atomic change that\nmodifies multiple documents. This situation requires a multi-document transaction.\nMulti-document transactions are  ACID compliant  because MongoDB\nguarantees that the data involved in your transaction operations remains\nconsistent, even if the driver encounters unexpected errors. To learn more about ACID compliance and transactions, see our  article on\nACID transactions . In MongoDB, multi-document transactions run within a  client session .\nA client session is a grouping of related read or write operations that\nyou want to execute sequentially. We recommend you reuse\nyour client for multiple sessions and transactions instead of\ninstantiating a new client each time. When combined with  majority  read and\nwrite concerns, the driver guarantees causal consistency between the\noperations. To learn more, see  Client Sessions and Causal Consistency Guarantees  in the\nServer manual. Learn more about how to use the driver to perform multi-document\ntransactions in the following sections of this guide: To execute a multi-document transaction, you must be connected to a\ndeployment running MongoDB Server version 4.0 or later. For a detailed list of limitations, see the  Transactions and\nOperations  section in\nthe Server manual. Transaction APIs Transaction Options Transaction Errors The driver provides two APIs for performing transactions, the  Core\nAPI  and the  Convenient Transaction API . The  Core API  is a framework that enables\nyou to create, commit, and end transactions. When using this API,\nyou must explicitly perform the following actions: The  Convenient Transaction API  is a\nframework that enables you to perform transactions without being\nresponsible for committing or ending them. This API automatically\nincorporates error-handling logic to retry operations when the server\nraises certain error types. To learn more about this behavior, see the\n Transaction Errors  section of this guide. Create, commit, and end the transaction. Create and end the session in which you run the transaction. Implement error-handling logic. When you connect to MongoDB Server version 4.2 or\nearlier, you can perform write operations in a transaction only on\ncollections that already exist. When you connect to MongoDB Server\nversion 4.4 and later, the server automatically creates collections\nas necessary when you perform write operations in a transaction. To\nlearn more about this behavior, see  Create Collections and\nIndexes in a Transaction \nin the Server manual. The Core API provides the following methods to implement transactions: You must perform the following steps when using this API: The following code demonstrates how to perform a transaction by using\nthe Core API: To see a fully runnable example that uses this API, see the\n Use the Core API  usage example. startSession() :\ncreates a new  ClientSession  instance startTransaction() : starts a new\ntransaction commitTransaction() : commits the\nactive transaction in the session that it was created in abortTransaction() : ends the\nactive transaction in the session that it was created in endSession() : ends the\nactive session Pass the session instance to each operation that\nyou want to run in that session. Implement a  catch  block in which you identify\nserver transaction errors and implement error-handling logic. The driver throws an error if you provide a session from one  MongoClient \ninstance to a different client instance. For example, the following code generates an\n MongoInvalidArgumentError  error because it creates\na  ClientSession  instance from the  client1  client, but provides\nthis session to the  client2  client for a write operation: The Convenient Transaction API provides the following methods to\nimplement transactions: These methods return the value that the callback returns. For example,\nif a callback you pass to the  withTransaction()  method returns the\ndocument  { hello: \"world\" } , then the  withTransaction()  method\nalso returns that document. When you use the Convenient Transaction API, you\ncan propagate return values from the callback as the return values of\nthe  withTransaction()  and  withSession()  methods to\nwork with them elsewhere in your code. You must perform the following steps when using this API: The following code demonstrates how to perform a transaction by using\nthe Convenient Transaction API: To see a fully runnable example that uses this API, see the\n Use the Convenient Transaction API  usage example. withSession() : Runs\nthe callback passed to it within a session. The API handles the creation and\ntermination of the session automatically. withTransaction() :\nRuns the callback passed to it within a transaction and calls the\n commitTransaction()  method when the callback returns. Pass the session instance to each operation that\nyou want to run in that session. Implement the async  await  syntax for each operation in the\nsession. Avoid parallelism, such as calling the  Promise.all()  method.\nUsing sessions in parallel usually leads to server errors. You can pass a  TransactionOptions  instance to the\n startTransaction()  and  withTransaction()  methods to configure\nhow the driver performs a transaction. When you specify an option,\nit overrides the value of the option that you might have set on your\n MongoClient  instance. The following table includes options that you can specify\nin a  TransactionOptions  instance: For a full list of options, see the API documentation for\n TransactionOptions . The following code shows how to define and pass transaction options to\nthe  startTransaction()  method: Setting Description readConcern writeConcern readPreference maxCommitTimeMS Specifies the length of time that a commit action on a\ntransaction can run, in milliseconds. The transaction inherits settings from your  MongoClient  instance unless you\nspecify them in your transaction options. If you are using the Core API to perform a transaction, you must incorporate\nerror-handling logic into your application for the following errors: The Convenient Transaction API incorporates retry logic for these error\ntypes, so the driver retries the transaction until there is a successful commit. TransientTransactionError : Raised if a write operation errors\nbefore the driver commits the transaction. To learn more about this error, see the\n TransientTransactionError description  on\nthe Driver API page in the Server manual. UnknownTransactionCommitResult : Raised if the commit operation\nencounters an error. To learn more about this error, see the\n UnknownTransactionCommitResult description  on\nthe Driver API page in the Server manual.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "async function coreTest(client) {\n  const session = client.startSession();\n  try {\n    session.startTransaction();\n\n    const savingsColl = client.db(\"bank\").collection(\"savings_accounts\");\n    await savingsColl.findOneAndUpdate(\n      {account_id: \"9876\"}, \n      {$inc: {amount: -100 }}, \n      { session });\n\n    const checkingColl = client.db(\"bank\").collection(\"checking_accounts\");\n    await checkingColl.findOneAndUpdate(\n      {account_id: \"9876\"}, \n      {$inc: {amount: 100 }}, \n      { session });\n\n    // ... perform other operations\n\n    await session.commitTransaction();\n    console.log(\"Transaction committed.\");\n  } catch (error) {\n    console.log(\"An error occurred during the transaction:\" + error);\n    await session.abortTransaction();\n  } finally {\n    await session.endSession();\n  }\n}"
                },
                {
                    "lang": "js",
                    "value": "const session = client1.startSession();\nclient2.db('myDB').collection('myColl').insertOne({ name: 'Jane Eyre' }, { session });"
                },
                {
                    "lang": "javascript",
                    "value": "async function convTest(client) {\n  let txnRes = await client.withSession(async (session) =>\n    session.withTransaction(async (session) => {\n      const savingsColl = client.db(\"bank\").collection(\"savings_accounts\");\n      await savingsColl.findOneAndUpdate(\n        {account_id: \"9876\"}, \n        {$inc: {amount: -100 }}, \n        { session });\n  \n      const checkingColl = client.db(\"bank\").collection(\"checking_accounts\");\n      await checkingColl.findOneAndUpdate(\n        {account_id: \"9876\"}, \n        {$inc: {amount: 100 }}, \n        { session });\n\n      // ... perform other operations\n\n      return \"Transaction committed.\";\n    }, null)\n  );\n  console.log(txnRes);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const txnOpts = {\n  readPreference: 'primary',\n  readConcern: { level: 'local' },\n  writeConcern: { w: 'majority' },\n  maxCommitTimeMS: 1000\n};\nsession.startTransaction(txnOpts);"
                }
            ],
            "preview": "In this guide, you can learn how to use the\nNode.js driver to perform transactions. Transactions allow you\nto run a series of operations that do not change any data until the\nentire transaction is committed. If any operation in the transaction fails, the\ndriver ends the transaction and discards all data changes before they\never become visible. This feature is called atomicity.",
            "tags": "modify, customize",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals/typescript",
            "title": "TypeScript",
            "headings": [
                "Overview",
                "Features",
                "Type Parameters that Extend Document",
                "Type Parameters of Any Type",
                "Type Safety and Dot Notation",
                "Referencing Keys that Incorporate Variables",
                "Working with the _id Field",
                "Insert Operations and the _id Field",
                "Find Methods and the _id Field",
                "Known Limitations",
                "Recursive Types and Dot Notation",
                "Mutual Recursion"
            ],
            "paragraphs": "In this guide, you can learn about the  TypeScript  features and limitations\nof the MongoDB Node.js driver. TypeScript is a strongly typed programming\nlanguage that compiles to JavaScript. The TypeScript compiler offers type checking in real time. Code editors that\nsupport TypeScript can provide autocomplete suggestions, display documentation\ninline, and identify type-related errors. All TypeScript features of the driver are optional. All valid JavaScript\ncode written with the driver is also valid TypeScript code. For more information, see the\n TypeScript website . If you use TypeScript, you can specify a type for some classes in the driver.\nAll classes that accept a type parameter in the driver have the default type\n Document . The  Document  interface has the following definition: All object types extend the  Document  interface. For more information on object types, see the\n TypeScript handbook . The following classes accept all types that extend the  Document  interface: You can pass a type parameter that extends the  Document  interface like this: Collection ChangeStream Keys not listed in your specified type parameter receive the  any  type.\nThe following code snippet demonstrates this behavior: The following classes accept all type parameters: You can find a code snippet that shows how to specify a type for the  FindCursor \nclass in the\n Find Multiple Documents Usage Example . FindCursor AggregationCursor Starting in version 5.0, by default, the Node.js driver does not provide type\nsafety for operations that search on fields expressed in  dot notation . Dot\nnotation is a syntax you can use to navigate nested JSON objects. When\nyou construct a filter to pass to a query, the driver will not raise a\ntype error even if you specify an incorrectly typed value for a field expressed\nin dot notation. The following code snippet defines the  ClassificationPet  interface,\nwhich includes a  classification  field that enables you to specify the\ngenus and color of dogs and cats: The driver does not raise a type error for the following code sample,\neven though the value of  classification.color  is a boolean\ninstead of a string: You can enable type-checking by constructing filters as  StrictFilter  or\n StrictUpdateFilter  types. In the following code sample, the filter is assigned a\n StrictFilter  type. Given this filter type, the Node.js driver\nreports a type error because the value of  classification.color  is a\nboolean instead of a string. The following example assigns a  StrictUpdateFilter  type to an update\nfilter. The Node.js driver reports a type error because the value of\n classification.color  is a boolean instead of a string. The  StrictFilter  and  StrictUpdateFilter  types are experimental and\nmight incorrectly show type errors in valid queries. To query a collection or perform another operation with a key that incorporates\nvariables, you must use an  as const  assertion when specifying the key. This\nmechanism allows your code to compile successfully if the input types are\ncorrect. The following code snippet defines the  ClassificationPet  interface\nand the  Mealtime  interface.  ClassificationPet  includes a\n mealtimes  field that contains an array of  Mealtime  interfaces,\neach of which includes a  time  field: The following code snippet performs a find-and-update operation on a\ncollection of  ClassificationPet  documents. The operation\nupdates the nested  time  field of the  Mealtime  instance at index\n 1 . The index position is specified by the variable  mealCounter : To learn more about dot notation, see\n Dot Notation \nin the MongoDB manual. To learn more about the limitations of dot notation in the\nNode.js driver, see the\n Recursive Types and Dot Notation \nsection. MongoDB does not recommend specifying the  _id  as a part of your model.\nOmitting the  _id  field makes the model more generic and reusable and more accurately\nmodels the data important to an application. The Node driver\u2019s TypeScript integration\ntakes care of adding the  _id  field to the return types for relevant methods. The following sections provide information about write and read operations that\nuse the  _id  field. How you specify the  _id  field in type parameters passed to your\n Collection  instance affects the behavior\nof insert operations. The following table describes how different\n _id  field specifications affect insert operations: If you must specify the  _id  field as required in the type you define to represent\ndocuments in your collection but you do not want to specify values for the\n _id  field in insert operations, use the  OptionalId  helper type when you\ncreate your collection. The  OptionalId  type accepts a type parameter as an\nargument and returns that type with an optional  _id  field. The following code snippet defines the  IdPet  interface, which\nincludes a type for the  _id  field: The following code uses the preceding interface and the\n OptionalId  type to insert a document without specifying a value for the\n _id  field: To learn more about the  _id  field, see\n The _id Field  in the MongoDB\nmanual. To learn more about the types, interfaces, and classes discussed in this section, see the\nfollowing resources: _id  field type Example Type Required on insert Behavior on insert OptionalId  API documentation PkFactory  API documentation ObjectId  source code The  find  and  findOne  methods of the  Collection  class include\nthe  _id  field in their return type. The driver infers the type of the\nreturned  _id  field based on the type parameter you passed to your\n Collection  instance. If the type parameter you passed to your  Collection  instance includes the\n _id  field in its schema, the driver infers that the  _id  field returned\nfrom the method is of the type specified in the schema. However, if the type parameter you passed to your  Collection  instance does not\ninclude the  _id  field in its schema, the driver infers that the type of the\n _id  field returned from the method is  ObjectId . The following code uses the  Pet \ninterface to return a document with an  _id  inferred to be of type  ObjectId : The following code uses the  IdNumberPet  interface to return a\ndocument with an  _id  inferred to be of type  number : To learn more about the classes and methods discussed in this section, see the following\nAPI documentation: The type parameter passed to your  Collection  influences only the type\ninference of the fields returned from the method. The driver does not convert\nthe field to the specified type. The type of each field in your type\nparameter's schema must match the type of the corresponding field in the\ncollection. If you specify a  projection  in a find\nmethod, you must pass a type parameter to your find method that reflects\nthe structure of your projected documents.\nWithout a type parameter, TypeScript cannot check at compile time that you\nare using your projected documents safely. To show this behavior, the following code snippet passes type checking but\nraises an error at runtime: To catch this error at compile time, pass a type parameter that does not include\nthe  _id  field to your find method: To view a runnable TypeScript example that includes a find method applying a\nprojection, see the\n Find a Document  page. Collection find findOne Learn about the following TypeScript specific limitations of the Node.js driver: No type safety for dot notation references to nested instances of recursive types Depth limitations on type safety for mutually recursive types The Node.js driver cannot provide type safety within nested instances of\n recursive types  referenced through dot notation. A recursive type is a type that references itself. You can update\nthe  Pet  interface\nto be recursive by allowing a pet to have its own pet. The following is the\nrecursive  Pet  interface: The following code snippet references a nested instance of the\n RecursivePet  interface\nwith an incorrect type using dot notation, but the TypeScript compiler\ndoes not raise a type error: The following code snippet references a top-level instance of the\n RecursivePet  interface with an incorrect type and raises a type error: The error raised by the preceding code snippet is as follows: If you must have type safety within nested instances of recursive types,\nyou must write your query or update without dot notation. To learn more about dot notation, see\n Dot Notation \nin the MongoDB manual. The Node.js driver does not traverse nested recursive types when\ntype checking dot notation keys to avoid hitting\nTypeScript's recursive depth limit. A   mutually recursive   type exists when two types contain a property that is of\nthe other's type. You can update the  Pet \ninterface to be mutually recursive by allowing a pet to have a handler, and\ndefining a handler to have a pet. The following examples reference the mutually\nrecursive  Pet  and  Handler  interfaces: The Node.js driver provides type safety for mutually recursive types\nreferenced through dot notation up to a depth of eight. The following code\nsnippet assigns a  string  to a  number  and raises a type error because\nthe referenced property is at a depth of four: The error raised by the preceding code snippet is as follows: At a depth greater than or equal to eight, TypeScript compiles your code but no\nlonger type checks it. The following code assigns a  string  to a  number \nproperty but does not cause a compilation error because the referenced property\nis at a depth of 10:",
            "code": [
                {
                    "lang": "typescript",
                    "value": "interface Document {\n  [key: string]: any;\n}"
                },
                {
                    "lang": "typescript",
                    "value": "interface Pet {\n  name: string;\n  age: number;\n}\n\nconst database = client.db(\"<your database>\");\nconst collection = database.collection<Pet>(\"<your collection>\");\n"
                },
                {
                    "lang": "typescript",
                    "value": "interface User {\n  email: string;\n}\n\nconst database = client.db(\"<your database>\");\nconst myColl = db.collection<User>(\"<your collection>\");\nmyColl.find({ age: \"Accepts any type!\" });"
                },
                {
                    "lang": "typescript",
                    "value": "interface ClassificationPet {\n  name: string;\n  age: number;\n  classification: { genus: \"Canis\" | \"Felis\"; color: string };\n}"
                },
                {
                    "lang": "typescript",
                    "value": "await myColl.findOneAndDelete({ \"classification.color\": false });"
                },
                {
                    "lang": "typescript",
                    "value": "const filterPredicate: StrictFilter<ClassificationPet> = { \"classification.color\": false };\nawait myColl.findOneAndDelete(filterPredicate);"
                },
                {
                    "lang": "typescript",
                    "value": "const updateFilter: StrictUpdateFilter<ClassificationPet> = { $set: { \"classification.color\": false } }\nawait pets.updateOne({}, updateFilter);"
                },
                {
                    "lang": "typescript",
                    "value": "interface ClassificationPet {\n  name: string;\n  mealtimes: Mealtime[];\n}\n\ninterface Mealtime{\n  time: string;\n  amount: number;\n}"
                },
                {
                    "lang": "typescript",
                    "value": "const mealCounter = 1;\n\nawait myColl.findOneAndUpdate(\n  { name: \"Lassie\" },\n  { $set: { [`mealtimes.${mealCounter}.time` as const]: '04:00 PM' } },\n);"
                },
                {
                    "lang": "typescript",
                    "value": "interface IdPet {\n  _id: ObjectId;\n  name: string;\n  age: number;\n}"
                },
                {
                    "lang": "typescript",
                    "value": "const database = client.db(\"<your database>\");\nconst collection = db.collection<OptionalId<IdPet>>(\"<your collection>\");\n\nmyColl.insertOne({\n  name: \"Spot\",\n  age: 2\n});"
                },
                {
                    "lang": "typescript",
                    "value": "const database = client.db(\"<your database>\");\nconst collection = db.collection<Pet>(\"<your collection>\");\n\nconst document = await myColl.findOne({\n  name: \"Spot\",\n});\nconst id : ObjectId = document._id;"
                },
                {
                    "lang": "typescript",
                    "value": "interface IdNumberPet {\n  _id: number;\n  name: string;\n  age: number;\n}\n\nconst database = client.db(\"<your database>\");\nconst collection = db.collection<IdNumberPet>(\"<your collection>\");\n\nconst document = await myColl.findOne({\n  name: \"Spot\",\n});\nconst id : number = document._id;"
                },
                {
                    "lang": "typescript",
                    "value": "const doc = await myColl.findOne(\n  {},\n  { projection: { _id: 0, name: 1 } }\n);\nconsole.log(doc._id.generationTime);"
                },
                {
                    "lang": "typescript",
                    "value": "interface ProjectedDocument {\n   name: string\n}\n\nconst doc = await myColl.findOne<ProjectedDocument>(\n  {},\n  { projection: { _id: 0, name: 1 } }\n);\n// Compile time error: Property '_id' does not exist on type 'ProjectedDocument'.\nconsole.log(doc._id.generationTime);"
                },
                {
                    "lang": "typescript",
                    "value": "interface RecursivePet {\n   pet?: RecursivePet;\n   name: string;\n   age: number;\n}"
                },
                {
                    "lang": "typescript",
                    "value": "database\n   .collection<RecursivePet>(\"<your collection>\")\n   .findOne({ \"pet.age\": \"Spot\" });"
                },
                {
                    "lang": "typescript",
                    "value": "database\n   .collection<RecursivePet>(\"<your collection>\")\n   .findOne({ pet: \"Spot\" });"
                },
                {
                    "lang": "none",
                    "value": "index.ts(19,59): error TS2769: No overload matches this call.\nThe last overload gave the following error.\nType 'string' is not assignable to type 'Condition<Pet>'."
                },
                {
                    "lang": "typescript",
                    "value": "interface Pet {\n   handler?: Handler;\n   name: string;\n   age: number;\n}\n\ninterface Handler {\n   pet: Pet;\n   name: string;\n}"
                },
                {
                    "lang": "typescript",
                    "value": "database\n   .collection<Pet>(\"<your collection>\")\n   .findOne({'handler.pet.handler.pet.age': \"four\"});"
                },
                {
                    "lang": "none",
                    "value": "index.ts(19,59): error TS2769: No overload matches this call.\nThe last overload gave the following error.\nType 'string' is not assignable to type 'Condition<number> | undefined'."
                },
                {
                    "lang": "typescript",
                    "value": "database\n   .collection<Pet>(\"<your collection>\")\n   .findOne({'handler.pet.handler.pet.handler.pet.handler.pet.handler.pet.age': \"four\"});"
                }
            ],
            "preview": "In this guide, you can learn about the TypeScript features and limitations\nof the MongoDB Node.js driver. TypeScript is a strongly typed programming\nlanguage that compiles to JavaScript.",
            "tags": "code example, node.js, static typing",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "fundamentals",
            "title": "Fundamentals",
            "headings": [],
            "paragraphs": "Learn how to perform the following tasks using the Node.js driver in the\nFundamentals section: Connect to MongoDB Use the Stable API Authenticate with MongoDB Read from and Write to MongoDB Access Return Values Transform your Data Create and Manage Transactions Run a Database Command Create Indexes to Speed Up Queries Sort Using Collations Log Events in the Driver Monitor Driver Events Store and Retrieve Large Files in MongoDB Encrypt Fields from the Client Create and Query Time Series Collection Specify Type Parameters with TypeScript Specify BSON Serialization Settings",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "",
            "title": "MongoDB Node Driver",
            "headings": [
                "Introduction",
                "Quick Start",
                "Quick Reference",
                "What's New",
                "Usage Examples",
                "Fundamentals",
                "Aggregation Tutorials",
                "API",
                "FAQ",
                "Connection Troubleshooting",
                "Issues & Help",
                "Compatibility",
                "Upgrade Driver Versions",
                "Related Tools and Libraries",
                "Object Document Mappers",
                "Packages",
                "Learn",
                "Developer Hub",
                "MongoDB University",
                "Take the Following Free Online Courses Taught by MongoDB Instructors"
            ],
            "paragraphs": "Welcome to the documentation site for the official MongoDB Node.js driver.\nYou can add the driver to your application to work with MongoDB\nin JavaScript or TypeScript. For more information about downloading and\ninstalling the Node.js driver, see\n Download and Install  in the\nQuick Start guide. You can connect using the Node.js driver for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB Learn how to establish a connection to MongoDB Atlas and begin\nworking with data in the step-by-step  Quick Start . See driver syntax examples for common MongoDB commands in the\n Quick Reference  section. For a list of new features and changes in each version, see the\n What's New  section. For fully runnable code snippets and explanations for common\nmethods, see the  Usage Examples  section. Learn how to perform the following tasks using the Node.js driver in the\nFundamentals section: Connect to MongoDB Use the Stable API Authenticate with MongoDB Read from and Write to MongoDB Access Return Values Transform your Data Create and Manage Transactions Run a Database Command Create Indexes to Speed Up Queries Sort Using Collations Log Events in the Driver Monitor Driver Events Store and Retrieve Large Files in MongoDB Encrypt Fields from the Client Create and Query Time Series Collection Specify Type Parameters with TypeScript Specify BSON Serialization Settings For step-by-step explanations of common\naggregation tasks, see the  Aggregation Tutorials \nsection. For detailed information about classes and methods in the MongoDB\nNode.js driver, see the  MongoDB Node.js driver API documentation . For answers to commonly asked questions about the MongoDB\nNode.js Driver, see the  Frequently Asked Questions (FAQ) \nsection. For solutions to issues you might encounter when using the driver to connect to\na MongoDB deployment, see the  Connection Troubleshooting  section. Learn how to report bugs, contribute to the driver, and to find help in the\n Issues & Help  section. For the compatibility tables that show the recommended Node.js driver\nversion for each MongoDB Server version, see the\n Compatibility  section. Learn what changes you must make to your application to upgrade\ndriver versions in the  Upgrade Driver Versions  section. MongoDB and our partners provide several object-document mappers (ODMs) for Node.js that\nlet developers work with MongoDB data as objects. One popular ODM is  Mongoose ,\nwhich helps enforce a semi-rigid schema at the application level and provides features\nto assist with data modeling and manipulation.  Prisma , another ODM, helps\nensure data consistency by offering a type-safe database client and an intuitive schema. For more information about using ODMs with MongoDB, see the following resources: MongoDB ORMs, ODMs, and Libraries Mongoose  official documentation Prisma  official documentation You can install the following packages to expand the functionality of the Node.js driver: For information about each package's version compatibility, see the  Component Support Matrix  in the Node.js driver Github\nrepository. Package Description kerberos C++ extension for Node.js that provides support for Kerberos authentication mongodb-legacy Legacy Node.js driver with optional callback support Visit the Developer Hub and MongoDB University to learn more about the\nNode.js driver. The Developer Hub provides tutorials and social engagement for developers. To learn how to use MongoDB features with the Node.js driver, see the\n How To's and Articles page . To ask questions and engage in discussions with fellow developers using\nthe Node.js driver, see the  Developer Community forums . MongoDB University provides free courses to teach everyone how to use MongoDB. Using MongoDB with Node.js Learn the essentials of Node.js application development with MongoDB. MongoDB Node.js Developer Path Gain a comprehensive understanding of Node.js application development, complex operations, interactions\nwith MongoDB Atlas datasets, and more.",
            "code": [],
            "preview": "Learn how to connect to and interact with data stored in MongoDB by using JavaScript or TypeScript with the Node.js driver.",
            "tags": "node.js, object-relational, object-document",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "issues-and-help",
            "title": "Issues & Help",
            "headings": [
                "Bugs / Feature Requests",
                "Pull Requests"
            ],
            "paragraphs": "Our developer community is vibrant and highly engaged, with extensive experience using Node.js with MongoDB. Often, the quickest way to get support for general questions is through the\n MongoDB Community Forums . Refer to our  support channels  documentation for more information. To report a bug or to request a new feature in the Node.js driver,\nplease open a case in our issue management tool, JIRA: Bug reports in JIRA for the Node.js driver and the Core Server (SERVER) project are  public . If you\u2019ve identified a security vulnerability in a driver or any other\nMongoDB project, please report it according to the instructions found in\nthe  Create a Vulnerability Report . Create an account and login . Navigate to  the NODE project . Click  Create Issue . Please provide as much information as possible about the\nissue and the steps to reproduce it. We are happy to accept contributions to help improve the driver. We will review user\ncontributions to ensure they meet the standards of the codebase. Pull requests must pass\nthe  travis.ci  checks, include documentation, and include tests. To get started check out the source and work on a branch: To run the test suite, you must have a server topology running and provide the URI to the command.\nFor example, if you have a single server running at  \"mongodb://localhost:27017\" , you can run the following: Note that the tests run on your feature are different depending on the type of topology\nthat you are running, such as for a standalone instance or replica set. There are many tools that can help you with setting up different topologies for local testing.\nSome examples are  mtools  and  mongo-orchestration .",
            "code": [
                {
                    "lang": "bash",
                    "value": "git clone https://github.com/mongodb/node-mongodb-native.git\ncd node-mongodb-native\nnpm install\ngit checkout -b myNewFeature"
                },
                {
                    "lang": "bash",
                    "value": "MONGODB_URI=\"mongodb://localhost:27017\" npm test"
                }
            ],
            "preview": "Our developer community is vibrant and highly engaged, with extensive experience using Node.js with MongoDB.",
            "tags": null,
            "facets": {
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-reference",
            "title": "Quick Reference",
            "headings": [
                "Compatibility"
            ],
            "paragraphs": "This page shows the driver syntax for several MongoDB commands and links to\ntheir related reference and API documentation. You can use the Node.js driver to connect and  execute commands  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  performing common CRUD operations in the Atlas UI  for deployments hosted in MongoDB\nAtlas, see  Create, View, Update, and Delete Documents . Command Syntax",
            "code": [
                {
                    "lang": "js",
                    "value": "await coll.findOne({ title: 'Hamlet' });"
                },
                {
                    "lang": "js",
                    "value": "{ title: 'Hamlet', type: 'movie', ... }"
                },
                {
                    "lang": "js",
                    "value": "coll.find({ year: 2005 });"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: 'Christmas in Boston', year: 2005, ... },\n  { title: 'Chicken Little', year: 2005, ... },\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "await coll.insertOne({ title: 'Jackie Robinson' });"
                },
                {
                    "lang": "javascript",
                    "value": "await coll.insertMany([\n  { title: 'Dangal', rating: 'Not Rated' },\n  { title: 'The Boss Baby', rating: 'PG' }\n ]);"
                },
                {
                    "lang": "js",
                    "value": "await coll.updateOne(\n  { title: 'Amadeus' },\n  { $set: { 'imdb.rating': 9.5 } }\n);"
                },
                {
                    "lang": "js",
                    "value": "{ title: 'Amadeus', imdb: { rating: 9.5, ... } }"
                },
                {
                    "lang": "js",
                    "value": "await coll.updateMany(\n  { year: 2001 },\n  { $inc: { 'imdb.votes': 100 } }\n);"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: 'A Beautiful Mind', year: 2001, imdb: { votes: 826257, ... },\n  { title: 'Shaolin Soccer', year: 2001, imdb: { votes: 65442, ... },\n  ...\n]"
                },
                {
                    "lang": "js",
                    "value": "await coll.updateOne(\n  { title: 'Cosmos' },\n  { $push: { genres: 'Educational' } }\n):"
                },
                {
                    "lang": "js",
                    "value": "{ title: 'Cosmos', genres: [ 'Documentary', 'Educational' ] }"
                },
                {
                    "lang": "js",
                    "value": "await coll.replaceOne(\n  { name: 'Deli Llama', address: '2 Nassau St' },\n  { name: 'Lord of the Wings', zipcode: 10001 }\n);"
                },
                {
                    "lang": "js",
                    "value": "{ name: 'Lord of the Wings', zipcode: 10001 }"
                },
                {
                    "lang": "javascript",
                    "value": "await coll.deleteOne({ title: 'Congo' });"
                },
                {
                    "lang": "javascript",
                    "value": "await coll.deleteMany({ title: { $regex: /^Shark.*/ } });"
                },
                {
                    "lang": "js",
                    "value": "await coll.bulkWrite([\n  {\n    insertOne: {\n      document: {\n        title: 'A New Movie',\n        year: 2022\n      }\n    }\n  },\n  {\n    deleteMany: {\n      filter: { year: { $lt: 1970 } }\n    }\n  }\n]);"
                },
                {
                    "lang": "js",
                    "value": "BulkWriteResult {\n  result: {\n    ...\n  },\n  ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "coll.watch([ { $match: { year: { $gte: 2022 } } } ]);"
                },
                {
                    "lang": "js",
                    "value": "const cursor = coll.find();\nfor await (const doc of cursor) {\n   console.dir(doc);\n}"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: '2001: A Space Odyssey', ... },\n  { title: 'The Sound of Music', ... },\n  ...\n]"
                },
                {
                    "lang": "js",
                    "value": "const cursor = coll.find();\nconst results = await cursor.toArray();"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: '2001: A Space Odyssey', ... },\n  { title: 'The Sound of Music', ... },\n  ...\n]"
                },
                {
                    "lang": "js",
                    "value": "await coll.countDocuments({ year: 2000 });"
                },
                {
                    "lang": "js",
                    "value": "618"
                },
                {
                    "lang": "js",
                    "value": "await coll.distinct('year');"
                },
                {
                    "lang": "js",
                    "value": "[ 1891, 1893, 1894, 1896, 1903, ... ]"
                },
                {
                    "lang": "js",
                    "value": "coll.find().limit(2);"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: 'My Neighbor Totoro', ... },\n  { title: 'Am\u00e9lie', ... }\n]"
                },
                {
                    "lang": "js",
                    "value": "coll.find({ title: { $regex: /^Rocky/} }, { skip: 2 });"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: 'Rocky III', ... },\n  { title: 'Rocky IV', ... },\n  { title: 'Rocky V'}, ... }\n]"
                },
                {
                    "lang": "js",
                    "value": "coll.find().sort({ year: 1});"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: 'Newark Athlete', year: 1891, ... },\n  { title: 'Blacksmith Scene', year: 1893, ...},\n  { title: 'Dickson Experimental Sound Film', year: 1894},\n  ...\n]"
                },
                {
                    "lang": "js",
                    "value": "coll.find().project({ _id: 0, year: 1, imdb: 1 });"
                },
                {
                    "lang": "js",
                    "value": "[\n  { year: 2012, imdb: { rating: 5.8, votes: 230, id: 8256 }},\n  { year: 1985, imdb: { rating: 7.0, votes: 447, id: 1654 }},\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "await coll.createIndex({ title: 1, year: -1 });"
                },
                {
                    "lang": "js",
                    "value": "// only searches fields with text indexes\ncoll.find({ $text: { $search: 'zissou' } });"
                },
                {
                    "lang": "js",
                    "value": "[\n  { title: 'The Life Aquatic with Steve Zissou', ... }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "\"dependencies\": {\n  \"mongodb\": \"^6.8\",\n  ...\n}"
                }
            ],
            "preview": "See Node.js driver code examples of frequently-used MongoDB commands and links to their related reference and API documentation.",
            "tags": "node.js, code example",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-start/connect-to-mongodb",
            "title": "Connect to MongoDB",
            "headings": [
                "Create your Node.js Application",
                "Assign the Connection String",
                "Run your Node.js Application"
            ],
            "paragraphs": "After you complete these steps, you have a working application that\nuses the driver to connect to your MongoDB deployment, runs a query on\nthe sample data, and prints out the result. Create a file to contain your application called  index.js  in your\n node_quickstart  project directory. Copy and paste the following code into the  index.js  file: Replace the  <connection string uri>  placeholder with the\nconnection string that you copied from the  Create a Connection String \nstep of this guide. In your shell, run the following command to start this application: The output includes details of the retrieved movie document: If you encounter an error or see no output, check whether you specified the\nproper connection string in the  index.js  file, and that you loaded the\nsample data. If you run into issues on this step, ask for help in the\n MongoDB Community Forums \nor submit feedback by using the  Rate this page \ntab on the right or bottom right side of this page.",
            "code": [
                {
                    "lang": "js",
                    "value": "const { MongoClient } = require(\"mongodb\");\n\n// Replace the uri string with your connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db('sample_mflix');\n    const movies = database.collection('movies');\n\n    // Query for a movie that has the title 'Back to the Future'\n    const query = { title: 'Back to the Future' };\n    const movie = await movies.findOne(query);\n\n    console.log(movie);\n  } finally {\n    // Ensures that the client will close when you finish/error\n    await client.close();\n  }\n}\nrun().catch(console.dir);"
                },
                {
                    "lang": "none",
                    "value": "node index.js"
                },
                {
                    "lang": "none",
                    "value": "{\n  _id: ...,\n  plot: 'A young man is accidentally sent 30 years into the past...',\n  genres: [ 'Adventure', 'Comedy', 'Sci-Fi' ],\n  ...\n  title: 'Back to the Future',\n  ...\n}"
                }
            ],
            "preview": "After you complete these steps, you have a working application that\nuses the driver to connect to your MongoDB deployment, runs a query on\nthe sample data, and prints out the result.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-start/create-a-connection-string",
            "title": "Create a Connection String",
            "headings": [
                "Find your MongoDB Atlas Connection String",
                "Copy your Connection String",
                "Update the Placeholders"
            ],
            "paragraphs": "You can connect to your MongoDB deployment by providing a\n connection URI , also called a  connection string , which\ninstructs the driver on how to connect to a MongoDB deployment\nand how to behave while connected. The connection string includes the hostname or IP address and\nport of your deployment, the authentication mechanism, user credentials\nwhen applicable, and connection options. To connect to an instance or deployment not hosted on Atlas, see\n Other Ways to Connect to MongoDB . After completing these steps, you have a connection string that\ncontains your database username and password. To retrieve your connection string for the deployment that\nyou created in the  previous step ,\nlog into your Atlas account and navigate to the\n Database  section and click the  Connect  button\nfor your new deployment. Proceed to the  Connect your application  section and select\n\"Node.js\" from the  Driver  selection menu and the version\nthat best matches the version you installed from the  Version \nselection menu. Select the  Password (SCRAM)  authentication mechanism. Deselect the  Include full driver code example  to view\nthe connection string. Click the button on the right of the connection string to copy it to\nyour clipboard as shown in the following screenshot: Paste this connection string into a a file in your preferred text editor\nand replace the \"<username>\" and \"<password>\" placeholders with\nyour database user's username and password. Save this file to a safe location for use in the next step. If you run into issues on this step, ask for help in the\n MongoDB Community Forums \nor submit feedback by using the  Rate this page \ntab on the right or bottom right side of this page.",
            "code": [],
            "preview": "You can connect to your MongoDB deployment by providing a\nconnection URI, also called a connection string, which\ninstructs the driver on how to connect to a MongoDB deployment\nand how to behave while connected.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-start/create-a-deployment",
            "title": "Create a MongoDB Deployment",
            "headings": [
                "Create a Free MongoDB deployment on Atlas",
                "Save your Credentials"
            ],
            "paragraphs": "You can create a free tier MongoDB deployment on MongoDB Atlas\nto store and manage your data. MongoDB Atlas hosts and manages\nyour MongoDB database in the cloud. After you complete these steps, you have a new free tier MongoDB\ndeployment on Atlas, database user credentials, and sample data loaded\nin your database. Complete the  Get Started with Atlas \nguide to set up a new Atlas account and load sample data into a new free\ntier MongoDB deployment. After you create your database user, save that user's\nusername and password to a safe location for use in an upcoming step. If you run into issues on this step, ask for help in the\n MongoDB Community Forums \nor submit feedback by using the  Rate this page \ntab on the right or bottom right side of this page.",
            "code": [],
            "preview": "You can create a free tier MongoDB deployment on MongoDB Atlas\nto store and manage your data. MongoDB Atlas hosts and manages\nyour MongoDB database in the cloud.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-start/download-and-install",
            "title": "Download and Install",
            "headings": [
                "Install Node and npm",
                "Create a Project Directory",
                "Install the Node.js Driver"
            ],
            "paragraphs": "After you complete these steps, you have Node.js and npm installed\nand a new project directory with the driver dependencies installed. Ensure you have Node.js v16 or later and\nnpm (Node Package Manager) installed in your development environment. For information on how to install Node.js and npm, see\n downloading and installing Node.js and npm . In your shell, run the following command to create a\ndirectory called  node_quickstart  for this project: Run the following command to navigate into the project\ndirectory: Run the following command to initialize your Node.js project: When this command successfully completes, you have a  package.json \nfile in your  node_quickstart  directory. Run the following command in your shell to install\nthe driver in your project directory: This command performs the following actions: Downloads the  mongodb  package and the dependencies it requires Saves the package in the  node_modules  directory Records the dependency information in the  package.json  file If you run into issues on this step, ask for help in the\n MongoDB Community Forums \nor submit feedback by using the  Rate this page \ntab on the right or bottom right side of this page.",
            "code": [
                {
                    "lang": "bash",
                    "value": "mkdir node_quickstart"
                },
                {
                    "lang": "bash",
                    "value": "cd node_quickstart"
                },
                {
                    "lang": "bash",
                    "value": "npm init -y"
                },
                {
                    "lang": "bash",
                    "value": "npm install mongodb@6.8"
                }
            ],
            "preview": "After you complete these steps, you have Node.js and npm installed\nand a new project directory with the driver dependencies installed.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-start/next-steps",
            "title": "Next Steps",
            "headings": [],
            "paragraphs": "Congratulations on completing the quick start tutorial! In this tutorial, you created a Node.js application that\nconnects to a MongoDB deployment hosted on MongoDB Atlas\nand retrieves a document that matches a query. Learn more about the MongoDB Node.js driver from the following resources: Discover how to perform read and write operations in the\n CRUD Operations  section. See examples of frequently-used operations in the\n Usage Examples  section.",
            "code": [],
            "preview": "Congratulations on completing the quick start tutorial!",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "quick-start",
            "title": "Node Driver Quick Start",
            "headings": [
                "Overview"
            ],
            "paragraphs": "This guide shows you how to create an application that uses the\nMongoDB Node.js driver to connect to a MongoDB cluster hosted on MongoDB Atlas. If\nyou prefer to connect to MongoDB using a different driver or programming\nlanguage, see our  list of official drivers . The Node.js driver is a library of functions that you can use to connect\nto and communicate with MongoDB. MongoDB Atlas is a fully managed cloud database service that hosts your\nMongoDB deployments. You can create your own free (no credit card\nrequired) MongoDB Atlas deployment by following the steps in this guide. Follow the steps in this guide to connect a sample Node.js application to\na MongoDB Atlas deployment.",
            "code": [],
            "preview": "Learn how to create an app to connect to MongoDB deployment by using the Node.js driver.",
            "tags": "node.js",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "upgrade",
            "title": "Upgrade Driver Versions",
            "headings": [
                "Overview",
                "How to Upgrade",
                "Breaking Changes",
                "Version 6.0 Breaking Changes",
                "Version 5.0 Breaking Changes",
                "Version 4.0 Breaking Changes",
                "Server Release Compatibility Changes",
                "Version 4.2 Server Release Support Changes"
            ],
            "paragraphs": "On this page, you can learn how to upgrade your driver to a new version. This page also\nincludes the changes you must make to your application to upgrade your driver\nwithout losing functionality, if applicable. Before you upgrade, perform the following actions: To upgrade your driver version, run the following command in your application's\ndirectory: To upgrade to a different version of the driver, replace the information after the\n @  symbol with your preferred version number. For more information about the\n npm install  command, see the  npm-install \nnpm documentation. Ensure the new driver version is compatible with the MongoDB Server version\nyour application connects to and the version of Node.js that your\napplication runs on. See the  Compatibility \npage for this information. Address any breaking changes between the version of the driver\nyour application uses now and your planned upgrade version in the\n Breaking Changes  section of this guide. To learn\nmore about the MongoDB Server release compatibility changes, see the\n Server Release Compatibility Changes  section. You can minimize the amount of changes that you must make to your\napplication when upgrading driver versions by using the\n Stable API . A breaking change is a modification in a convention or behavior in\na specific version of the driver that may prevent your application from\nworking as expected. The breaking changes in this section are categorized by the major\nversion releases that introduced them. When upgrading driver versions,\naddress all the breaking changes between your current version and the\nplanned upgrade version. For example, if you are upgrading the driver\nfrom v3.x to v5.x, address all breaking changes listed under v4.0 and\nv5.0. Version 6.0 of the Node.js driver requires Node.js v16.20.1 or later. The driver removes support for the  addUser()  helper command. Use the\n createUser  MongoDB Shell command instead. The driver removes support for the  collStats  operation. Use the\n $collStats  aggregation operator\ninstead. The driver removes all the deprecated  ssl -prefixed options and the\n tlsCertificateFile  option in the  MongoClientOptions  type.\nCreate a  SecureContext  object or set the  tls -prefixed options\nin your  MongoClientOptions  instance instead. The driver reads files set in the  tlsCAFile  and\n tlsCertificateKeyFile  connection options when you call the\n MongoClient.connect()  method, not when you create the\n MongoClient  instance. The driver removes the  keepAlive  and  keepAliveInitialDelay  connection\noptions. The value of  keepAlive  is permanently set to  true  and the\nvalue of  keepAliveInitialDelay  is set to 300000 milliseconds (300\nseconds). The  Db.command()  method accepts only options that are not related\nto a specific command. To learn more about these options, see the\n Command Options  section of the Run a\nCommand guide. If you add  mongodb-client-encryption  as a dependency,\nthe major version number must match that of the Node.js driver. For example,\nNode.js driver v6.x.x requires  mongodb-client-encryption  v6.x.x. Automatic Encryption methods are now in the Node.js driver. You must\nimport these methods from the driver instead of from\n mongodb-client-encryption . Removed the  ObjectId  constructor that accepted a 12-character string. Modified  abortTransaction()  and  commitTransaction()  methods to return\n null  instead of the raw command results. Removed connection option helpers that accepted values other than  true \nor   false  as booleans. You must provide either  true  or  false  values in\nthe connection string or to the MongoClient constructor. Removed the  Binary  BSON type constructor that accepted a string. The  Binary.write()  method no longer accepts a string to write to the binary\nBSON object. The ClientEncryption API returns promises instead of callbacks. The  socks  package, which enables SOCKS5 proxy support, is a\npeer-optional dependency. You must install the package to enable\nSOCKS5 in your application. To learn more, see  Enable SOCKS5 Proxy Support . If you start a session on a client, then pass that session to a\ndifferent client, the driver throws an error when you\nperform any operations in the session. The  includeResultMetadata  option for compound operation methods is\n false  by default. See the  Built-in Methods \nsection of the Compound Operations guide for more information. The  withSession()  method returns the value that the provided\nfunction returns. In previous driver versions, this method returns\n undefined . The  withTransaction()  method returns the value that the\ncallback returns. In previous driver versions, this method\nreturns the server command response, which varies depending on the MongoDB\nServer version or type that the driver connects to. To learn more\nabout transactions, see the  Perform a Transaction  usage\nexamples and the  Transactions  guide. Raised the optional  kerberos  dependency minimum version to 2.0.1 and\nremoved support for version 1.x. Raised the optional  zstd  dependency minimum version to 1.1.0. The driver is no longer compatible with Node.js v12 or earlier. If you want to use\nthis version of the driver, you must use Node.js v14.20.1 or greater. The driver removes support for callbacks in favor of a promise-based API.\nThe following list provides some strategies for callback users to adopt this\nversion: For more information about these strategies, see\n the v5.0 changelog . Migrate to the promise-based API (recommended) Use the promise-based API and  util.callbackify Add  mongodb-legacy  to continue using callbacks The driver removes support for the  Collection.insert() ,\n Collection.update() , and  Collection.remove()  helper methods.\nThe following list provides instructions on how to replace the\nfunctionality of the removed methods: Migrate from  Collection.insert()  to  insertOne()  or  insertMany() Migrate from  Collection.update()  to  updateOne()  or  updateMany() Migrate from  Collection.remove()  to  deleteOne()  or  deleteMany() The driver no longer includes AWS SDK modules by default. The driver no longer automatically imports the  bson-ext  package. The driver removes support for custom  Promise  libraries. The driver no\nlonger supports the  promiseLibrary  option of  MongoClient  and the  Promise.set \nexport that allows specifying a custom  Promise  library. The driver removes support for the  Collection.mapReduce()  helper. The  BulkWriteResult  type no longer has the publicly enumerable\n result  property. The following types, options, and methods have been removed: BulkResult.lastOp()  method opTime  property of  BulkResult BulkWriteOptions.keepGoing  option WriteConcernError.err()  method AddUserOptions.digestPassword  option Kerberos  gssapiCanonicalizeHostName  option slaveOk  options and method removed in favor of  secondaryOk ObjectID  type removed in favor of  ObjectId AsyncIterator  interface removed in favor of  AsyncGenerator For more information about these changes, see\n the v4.0 changelog . The driver is no longer compatible with Node.js v12.8 or earlier. If you\nwant to use this version of the driver, you must use Node.js v12.9 or greater. Cursor  types no longer extend  Readable  directly. You cannot use a  ChangeStream  instance as an iterator after using\nit as an  EventEmitter . You also cannot do the reverse\u2014using an\n EventEmitter  instance as an iterator after using it as a  ChangeStream . The following methods no longer accept a callback parameter: Collection.find() Collection.aggregate() Db.aggregate() The default value of the  maxPoolSize  connection option is now\n 100 . The driver no longer supports the  gssapiServiceName  Kerberos\noption. Use  authMechanismProperties.SERVICE_NAME  instead. The driver no longer accepts non-boolean types, such as  0  or\n 1 , for boolean options. The  db.collection  type no longer accepts a callback. The  Db  type is no longer an  EventEmitter . You can listen to\nany events directly from the  MongoClient  instance. The driver removes support for the  Collection.group()  helper. The driver no longer includes the deprecated  GridStore  API. A server release compatibility change is a modification\nto the driver that discontinues support for a set of\nMongoDB Server versions. The driver discontinues support for a MongoDB Server version after it reaches\nend-of-life (EOL). To learn more about the MongoDB support for EOL products,\nsee the  Legacy Support Policy . The v4.2 driver drops support for MongoDB Server v3.4 and earlier.\nTo use the v4.2 driver, your MongoDB Server must be v3.6 or later. To learn\nhow to upgrade your MongoDB Server deployment, see  Release\nNotes  in the MongoDB Server manual.",
            "code": [
                {
                    "lang": "bash",
                    "value": "npm install mongodb@6.8"
                }
            ],
            "preview": "On this page, you can learn how to upgrade your driver to a new version. This page also\nincludes the changes you must make to your application to upgrade your driver\nwithout losing functionality, if applicable.",
            "tags": "backwards compatibility, update",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/bulkWrite",
            "title": "Perform Bulk Operations",
            "headings": [
                "Example"
            ],
            "paragraphs": "The  bulkWrite()  method performs batch write operations against a\n single  collection. This method reduces the number of network round trips from\nyour application to the server which therefore increases the throughput and\nperformance. Bulk writes return a collection of results for all operations\nonly after  all  operations passed to the method complete. You can specify one or more of the following write operations in\n bulkWrite() : The  bulkWrite()  method accepts the following parameters: If you create an index with a  unique index \nconstraint, you might encounter a duplicate key write error during an\noperation in the following format: Similarly, if you attempt to perform a bulk write against a collection\nthat uses  schema validation , you may\nencounter warnings or errors related to the formatting of inserted or\nmodified documents. insertOne updateOne updateMany deleteOne deleteMany replaceOne operations : specifies the bulk write operations to\nperform. Pass each operation to  bulkWrite()  as an object in\nan array. For examples that show the syntax for each write operation, see\nthe  bulkWrite API documentation . options :  optional  settings that affect the execution\nof the operation, such as whether the write operations executes in\nsequential order and the write concern. By default, MongoDB executes bulk write operations one-by-one in the specified order\n(serially). During an ordered bulk write, if an error occurs during the processing of an\noperation, MongoDB returns without processing the remaining operations in the list. In\ncontrast, when  ordered  is  false , MongoDB continues to process remaining write\noperations in the list. Unordered operations are theoretically faster since MongoDB can\nexecute them in parallel, but only use them if the writes do not depend on order. The following code sample performs a bulk write operation on the\n theaters  collection in the  sample_mflix  database. The example call\nto  bulkWrite()  includes examples of  insertOne ,  updateMany , and\n deleteOne  write operations: Running the preceding example results in the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "sh",
                    "value": "Error during bulkWrite, BulkWriteError: E11000 duplicate key error collection: ..."
                },
                {
                    "lang": "javascript",
                    "value": "BulkWriteResult {\n  insertedCount: 2,\n  matchedCount: 1,\n  modifiedCount: 1,\n  deletedCount: 0,\n  upsertedCount: 0,\n  upsertedIds: {},\n  insertedIds: {\n    '0': new ObjectId(\"...\"),\n    '1': new ObjectId(\"...\")\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// Bulk write operation\n\n// Import MongoClient from the MongoDB node driver package\nconst { MongoClient } = require(\"mongodb\");\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const theaters = database.collection(\"theaters\");\n\n    // Insert a new document into the \"theaters\" collection\n    const result = await theaters.bulkWrite([\n      {\n        insertOne: {\n          document: {\n            location: {\n              address: {\n                street1: \"3 Main St.\",\n                city: \"Anchorage\",\n                state: \"AK\",\n                zipcode: \"99501\",\n              },\n            },\n          },\n        },\n      },\n      {\n        insertOne: {\n          document: {\n            location: {\n              address: {\n                street1: \"75 Penn Plaza\",\n                city: \"New York\",\n                state: \"NY\",\n                zipcode: \"10001\",\n              },\n            },\n          },\n        },\n      },\n      {\n        // Update documents that match the specified filter\n        updateMany: {\n          filter: { \"location.address.zipcode\": \"44011\" },\n          update: { $set: { is_in_ohio: true } },\n          upsert: true,\n        },\n      },\n      {\n        // Delete a document that matches the specified filter\n        deleteOne: { filter: { \"location.address.street1\": \"221b Baker St\" } },\n      },\n    ]);\n    // Log the result of the bulk write operation \n    console.log(result);\n  } finally {\n    // Close the database connection when the operations are completed or if an error occurs\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ninterface Address {\n  street1: string;\n  city: string;\n  state: string;\n  zipcode: string;\n}\n\ninterface Theater {\n  location: { address: Address };\n  is_in_ohio?: boolean;\n}\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const theaters = database.collection<Theater>(\"theaters\");\n\n    const result = await theaters.bulkWrite([\n      {\n        insertOne: {\n          document: {\n            location: {\n              address: {\n                street1: \"3 Main St.\",\n                city: \"Anchorage\",\n                state: \"AK\",\n                zipcode: \"99501\",\n              },\n            },\n          },\n        },\n      },\n      {\n        insertOne: {\n          document: {\n            location: {\n              address: {\n                street1: \"75 Penn Plaza\",\n                city: \"New York\",\n                state: \"NY\",\n                zipcode: \"10001\",\n              },\n            },\n          },\n        },\n      },\n      {\n        updateMany: {\n          // Important: You lose type safety when you use dot notation in queries\n          filter: { \"location.address.zipcode\": \"44011\" },\n          update: { $set: { is_in_ohio: true } },\n          upsert: true,\n        },\n      },\n      {\n        deleteOne: {\n          filter: { \"location.address.street1\": \"221b Baker St\" },\n        },\n      },\n    ]);\n\n    console.log(result);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "The bulkWrite() method performs batch write operations against a\nsingle collection. This method reduces the number of network round trips from\nyour application to the server which therefore increases the throughput and\nperformance. Bulk writes return a collection of results for all operations\nonly after all operations passed to the method complete.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/changeStream",
            "title": "Watch for Changes",
            "headings": [
                "Open a Change Stream",
                "Examples",
                "Iteration",
                "Listener Function"
            ],
            "paragraphs": "You can watch for changes in MongoDB using the  watch()  method on the\nfollowing objects: For each object, the  watch()  method opens a  change stream  to\nemit  change event  documents when they occur. The  watch()  method optionally takes an  aggregation pipeline  which consists of an array of  aggregation stages \nas the first parameter. The aggregation stages filter and transform the change events. In the following snippet, the  $match  stage matches all change event documents with a  runtime  value of less than\n15, filtering all others out. The  watch()  method accepts an  options  object as the second parameter. Refer to the links at the end of this\nsection for more information on the settings you can configure with this object. The  watch()  method returns an instance of a  ChangeStream . You can read events from\nchange streams by iterating over them or listening for events. Select the tab that corresponds to the way you want to\nread events from the change stream: Collection Database MongoClient Starting in version 4.12,  ChangeStream  objects are async\niterables. With this change, you can use  for-await  loops to\nretrieve events from an open change stream: You can call methods on the  ChangeStream  object such as: hasNext()  to check for remaining documents in the stream next()  to request the next document in the stream close()  to close the ChangeStream You can attach listener functions to the  ChangeStream   object\nby calling the  on()  method. This method is inherited from the\nJavascript  EventEmitter  class. Pass the string  \"change\"  as\nthe first parameter and your listener function as the second parameter as shown below: The listener function triggers when a  change  event is emitted. You\ncan specify logic in the listener to process the change event document\nwhen it is received. You can control the change stream by calling  pause()  to stop emitting events or  resume()  to continue to emit events. To stop processing change events, call the  close()  method on the\n ChangeStream  instance. This closes the change stream and frees resources. Using a  ChangeStream  in  EventEmitter  and  Iterator  mode\nconcurrently is not supported by the driver and causes an error. This\nis to prevent undefined behavior, where the driver cannot guarantee\nwhich consumer receives documents first. The following example opens a change stream on the  haikus  collection in\nthe  insertDB  database and prints change events as they occur: When you run this code and then make a change to the  haikus \ncollection, such as performing an insert or delete operation, you can\nsee the change event document printed in your terminal. For example, if you insert a document to the collection, the code prints\nthe following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide . The JavaScript and TypeScript code snippets above are identical. There are no\nTypeScript specific features of the driver relevant to this use case. Change events that contain information on update operations only return the modified\nfields by default rather than the full updated document. You can configure\nyour change stream to also return the most current version of the document\nby setting the  fullDocument  field of the options object to\n \"updateLookup\"  as follows: The following example opens a change stream on the  haikus  collection in\nthe  insertDB  database. Let's create a listener function to receive and\nprint change events that occur on the collection. First, open the change stream on the collection and then define a listener\non the change stream using the  on()  method. Once you set the\nlistener, generate a change event by performing a change to the collection. To generate the change event on the collection, let's use the  insertOne() \nmethod to add a new document. Since  insertOne()  may run before the\nlistener function can register, we use a timer, defined as\n simulateAsyncPause  to wait 1 second before executing the insert. We also use  simulateAsyncPause  after the insertion of the document.\nThis provides ample time for the listener function to receive the change\nevent and for the listener to complete its execution before\nclosing the  ChangeStream  instance using the  close()  method. Visit the following resources for more material on the classes and\nmethods mentioned on this page: The timers used in this example are only for demonstration\npurposes. They make sure that there is enough time to register\nthe listener and have the listener process the change event before\nexiting. The JavaScript and TypeScript code snippets above are identical. There are no\nTypeScript specific features of the driver relevant to this use case. Change streams Change events Aggregation pipeline Aggregation stages ChangeStream class API documentation Collection.watch() , Db.watch() , MongoClient.watch() API documentation",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const pipeline = [ { $match: { runtime: { $lt: 15 } } } ];\nconst changeStream = myColl.watch(pipeline);"
                },
                {
                    "lang": "js",
                    "value": "for await (const change of changeStream) {\n  console.log(\"Received change: \", change);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "changeStream.on(\"change\", (changeEvent) => { /* your listener function */ });"
                },
                {
                    "lang": "javascript",
                    "value": "changeStream.close();"
                },
                {
                    "lang": "none",
                    "value": "Received change:\n{\n  _id: {\n    _data: '...'\n  },\n  operationType: 'insert',\n  clusterTime: new Timestamp({ t: 1675800603, i: 31 }),\n  fullDocument: {\n    _id: new ObjectId(\"...\"),\n    ...\n  },\n  ns: { db: 'insertDB', coll: 'haikus' },\n  documentKey: { _id: new ObjectId(\"...\") }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// Watch for changes in a collection by using a change stream\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\n// Declare a variable to hold the change stream\nlet changeStream;\n\n// Define an asynchronous function to manage the change stream\nasync function run() {\n  try {\n    const database = client.db(\"insertDB\");\n    const haikus = database.collection(\"haikus\");\n\n    // Open a Change Stream on the \"haikus\" collection\n    changeStream = haikus.watch();\n\n    // Print change events as they occur\n    for await (const change of changeStream) {\n      console.log(\"Received change:\\n\", change);\n    }\n    // Close the change stream when done\n    await changeStream.close();\n    \n  } finally {\n    // Close the MongoDB client connection\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "// Watch for changes in a collection by using a change stream\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\n// Declare a variable to hold the change stream\nlet changeStream;\n\n// Define an asynchronous function to manage the change stream\nasync function run() {\n  try {\n    const database = client.db(\"insertDB\");\n    const haikus = database.collection(\"haikus\");\n\n    // Open a Change Stream on the \"haikus\" collection\n    changeStream = haikus.watch();\n\n    // Print change events as they occur\n    for await (const change of changeStream) {\n      console.log(\"Received change:\\n\", change);\n    }\n    // Close the change stream when done\n    await changeStream.close();\n    \n  } finally {\n    // Close the MongoDB client connection\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "const options = { fullDocument: \"updateLookup\" };\n// This could be any pipeline.\nconst pipeline = [];\n\nconst changeStream = myColl.watch(pipeline, options);"
                },
                {
                    "lang": "javascript",
                    "value": "/* Change stream listener */\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nconst simulateAsyncPause = () =>\n  new Promise(resolve => {\n    setTimeout(() => resolve(), 1000);\n  });\n\nlet changeStream;\nasync function run() {\n  try {\n    const database = client.db(\"insertDB\");\n    const haikus = database.collection(\"haikus\");\n\n    // Open a Change Stream on the \"haikus\" collection\n    changeStream = haikus.watch();\n\n    // Set up a change stream listener when change events are emitted\n    changeStream.on(\"change\", next => {\n      // Print any change event\n      console.log(\"received a change to the collection: \\t\", next);\n    });\n\n    // Pause before inserting a document\n    await simulateAsyncPause();\n\n    // Insert a new document into the collection\n    await myColl.insertOne({\n      title: \"Record of a Shriveled Datum\",\n      content: \"No bytes, no problem. Just insert a document, in MongoDB\",\n    });\n\n    // Pause before closing the change stream\n    await simulateAsyncPause();\n\n    // Close the change stream and print a message to the console when it is closed\n    await changeStream.close();    \n    console.log(\"closed the change stream\");\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);"
                },
                {
                    "lang": "javascript",
                    "value": "/* Change stream listener */\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nconst simulateAsyncPause = () =>\n  new Promise(resolve => {\n    setTimeout(() => resolve(), 1000);\n  });\n\nlet changeStream;\nasync function run() {\n  try {\n    const database = client.db(\"insertDB\");\n    const haikus = database.collection(\"haikus\");\n\n    // Open a Change Stream on the \"haikus\" collection\n    changeStream = haikus.watch();\n\n    // Set up a change stream listener when change events are emitted\n    changeStream.on(\"change\", next => {\n      // Print any change event\n      console.log(\"received a change to the collection: \\t\", next);\n    });\n\n    // Pause before inserting a document\n    await simulateAsyncPause();\n\n    // Insert a new document into the collection\n    await myColl.insertOne({\n      title: \"Record of a Shriveled Datum\",\n      content: \"No bytes, no problem. Just insert a document, in MongoDB\",\n    });\n\n    // Pause before closing the change stream\n    await simulateAsyncPause();\n\n    // Close the change stream and print a message to the console when it is closed\n    await changeStream.close();    \n    console.log(\"closed the change stream\");\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);"
                }
            ],
            "preview": "You can watch for changes in MongoDB using the watch() method on the\nfollowing objects:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/command",
            "title": "Run a Command",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can execute database commands by using the\n command()  method on a  Db \ninstance. You can specify a command and options in a document. To run the\ncommand, pass this document to the  command()  method. To see a full\nlist of database commands, see  Database Commands  in the Server manual. You can specify optional command behavior by passing a\n RunCommandOptions  object to the  command()  method. To learn more\nabout the supported options, see the\n Db.command() API documentation . Use the  MongoDB Shell  for administrative tasks instead of\nthe Node.js driver whenever possible. Running the preceding command, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide . The JavaScript and TypeScript code snippets above are identical. There are no\nTypeScript specific features of the driver relevant to this use case.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{\n  db: 'sample_mflix',\n  collections: 6,\n  views: 0,\n  objects: 75620,\n  ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "/* Run a database command */\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Get the \"sample_mflix\" database\n    const db = client.db(\"sample_mflix\");\n\n    // Find and print the storage statistics for the \"sample_mflix\" database using the 'dbStats' command\n    const result = await db.command({\n      dbStats: 1,\n    });\n    console.log(result);\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "/* Run a database command */\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Get the \"sample_mflix\" database\n    const db = client.db(\"sample_mflix\");\n\n    // Find and print the storage statistics for the \"sample_mflix\" database using the 'dbStats' command\n    const result = await db.command({\n      dbStats: 1,\n    });\n    console.log(result);\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can execute database commands by using the\ncommand() method on a Db\ninstance.",
            "tags": "code example, multiple, modify, customize, debug",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/count",
            "title": "Count Documents",
            "headings": [
                "Example"
            ],
            "paragraphs": "The Node.js driver provides two methods for counting documents in a\ncollection: estimatedDocumentCount()  is faster than  countDocuments()  because\nthe estimation uses the collection's metadata rather than scanning the\ncollection. In contrast,  countDocuments()  takes longer to return, but\nprovides an  accurate  count of the number of documents and supports\nspecifying a filter. Choose the appropriate method for your workload. To specify which documents you wish to count,  countDocuments() \naccepts a  query  parameter.\n countDocuments()  counts the documents that match the specified query. countDocuments()  and  estimatedDocumentCount()  support optional\nsettings that affect the method's execution. Refer to the reference\ndocumentation for each method for more information. collection.countDocuments()  returns the number of documents in\nthe collection that match the specified query. If you specify an empty\nquery document,  countDocuments()  returns the total number of\ndocuments in the collection. collection.estimatedDocumentCount()  returns an\n estimation  of the number of documents in the collection based on\ncollection metadata. You can improve performance when using  countDocuments()  to return the\ntotal number of documents in a collection by avoiding a collection scan. To\ndo this, use a  hint  to take\nadvantage of the built-in index on the  _id  field. Use this technique only\nwhen calling  countDocuments()  with an empty query parameter. The following example estimates the number of documents in the\n movies  collection in the  sample_mflix  database, and then returns\nan accurate count of the number of documents in the  movies \ncollection with  Canada  in the  countries  field. Running the preceding sample code, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide . The JavaScript and TypeScript code snippets above are identical. There are no\nTypeScript specific features of the driver relevant to this use case.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "collection.countDocuments({}, { hint: \"_id_\" });"
                },
                {
                    "lang": "none",
                    "value": "Estimated number of documents in the movies collection: 23541\nNumber of movies from Canada: 1349"
                },
                {
                    "lang": "javascript",
                    "value": "// Count documents in a collection\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    /* Print the estimate of the number of documents in the\n    \"movies\" collection */\n    const estimate = await movies.estimatedDocumentCount();\n    console.log(`Estimated number of documents in the movies collection: ${estimate}`);\n\n    /* Print the number of documents in the \"movies\" collection that\n    match the specified query */\n    const query = { countries: \"Canada\" };\n    const countCanada = await movies.countDocuments(query);\n    console.log(`Number of movies from Canada: ${countCanada}`);\n  } finally {\n    // Close the connection after the operations complete\n    await client.close();\n  }\n}\n// Run the program and print any thrown exceptions\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "// Count documents in a collection\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    /* Print the estimate of the number of documents in the\n    \"movies\" collection */\n    const estimate = await movies.estimatedDocumentCount();\n    console.log(`Estimated number of documents in the movies collection: ${estimate}`);\n\n    /* Print the number of documents in the \"movies\" collection that\n    match the specified query */\n    const query = { countries: \"Canada\" };\n    const countCanada = await movies.countDocuments(query);\n    console.log(`Number of movies from Canada: ${countCanada}`);\n  } finally {\n    // Close the connection after the operations complete\n    await client.close();\n  }\n}\n// Run the program and print any thrown exceptions\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "The Node.js driver provides two methods for counting documents in a\ncollection:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/delete-operations",
            "title": "Delete Operations",
            "headings": [],
            "paragraphs": "Delete a Document Delete Multiple Documents",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/deleteMany",
            "title": "Delete Multiple Documents",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can delete multiple documents in a collection at once using the\n collection.deleteMany()  method.\nPass a query document to the  deleteMany()  method to specify a subset\nof documents in the collection to delete. If you do not provide a query\ndocument (or if you provide an empty document), MongoDB matches all documents\nin the collection and deletes them. While you can use  deleteMany() \nto delete all documents in a collection, consider using\n drop()  instead for better performance\nand clearer code. You can specify more options in the  options  object passed in\nthe second parameter of the  deleteMany()  method. For more detailed\ninformation, see the\n deleteMany() API documentation . The following snippet deletes multiple documents from the  movies \ncollection. It uses a  query document  that configures the query to\nmatch and delete movies with the title \"Santa Claus\". Running the preceding example for the first time, you see the following output: If you run the example more than once, you see the following output because\nyou deleted the matching documents in the first run: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "none",
                    "value": "Deleted 19 documents"
                },
                {
                    "lang": "none",
                    "value": "Deleted 0 documents"
                },
                {
                    "lang": "javascript",
                    "value": "// Delete multiple documents\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    /* Delete all documents that match the specified regular\n    expression in the title field from the \"movies\" collection */\n    const query = { title: { $regex: \"Santa\" } };\n    const result = await movies.deleteMany(query);\n\n    // Print the number of deleted documents\n    console.log(\"Deleted \" + result.deletedCount + \" documents\");\n  } finally {\n    // Close the connection after the operation completes\n    await client.close();\n  }\n}\n// Run the program and print any thrown exceptions\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "// Delete multiple documents\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    /* Delete all documents that match the specified regular\n    expression in the title field from the \"movies\" collection */\n    const result = await movies.deleteMany({ title: { $regex: \"Santa\" } });\n    \n    // Print the number of deleted documents\n    console.log(\"Deleted \" + result.deletedCount + \" documents\");\n  } finally {\n    // Close the connection after the operation completes\n    await client.close();\n  }\n}\n// Run the program and print any thrown exceptions\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can delete multiple documents in a collection at once using the\ncollection.deleteMany() method.\nPass a query document to the deleteMany() method to specify a subset\nof documents in the collection to delete. If you do not provide a query\ndocument (or if you provide an empty document), MongoDB matches all documents\nin the collection and deletes them. While you can use deleteMany()\nto delete all documents in a collection, consider using\ndrop() instead for better performance\nand clearer code.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/deleteOne",
            "title": "Delete a Document",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can delete a single document in a collection with\n collection.deleteOne() .\nThe  deleteOne()  method uses a query document that you provide\nto match the subset of the documents in the collection that match\nthe query. If you do not provide a query document (or if you provide an\nempty document), MongoDB matches all documents in the collection and\ndeletes the first match. You can specify more query options using the\n options  object passed as the second parameter of the\n deleteOne  method. For more information on this method,\nsee the\n deleteOne() API documentation . If your application requires the deleted document after deletion,\nconsider using the\n collection.findOneAndDelete() \nmethod, which has a similar interface to  deleteOne()  but also\nreturns the deleted document. The following snippet deletes a single document from the  movies \ncollection. It uses a  query document  that configures the query\nto match movies with a  title  value of \"Annie Hall\". Running the preceding example, you see the following output: If you run the example more than once, you see the following output because\nyou deleted the matching document in the first run: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide . The JavaScript and TypeScript code snippets above are identical. There are no\nTypeScript specific features of the driver relevant to this use case.",
            "code": [
                {
                    "lang": "none",
                    "value": "Successfully deleted one document."
                },
                {
                    "lang": "none",
                    "value": "No documents matched the query. Deleted 0 documents."
                },
                {
                    "lang": "javascript",
                    "value": "// Delete a document\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    /* Delete the first document in the \"movies\" collection that matches\n    the specified query document */\n    const query = { title: \"Annie Hall\" };\n    const result = await movies.deleteOne(query);\n\n    /* Print a message that indicates whether the operation deleted a\n    document */\n    if (result.deletedCount === 1) {\n      console.log(\"Successfully deleted one document.\");\n    } else {\n      console.log(\"No documents matched the query. Deleted 0 documents.\");\n    }\n  } finally {\n    // Close the connection after the operation completes\n    await client.close();\n  }\n}\n// Run the program and print any thrown exceptions\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "javascript",
                    "value": "// Delete a document\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    /* Delete the first document in the \"movies\" collection that matches\n    the specified query document */\n    const query = { title: \"Annie Hall\" };\n    const result = await movies.deleteOne(query);\n\n    /* Print a message that indicates whether the operation deleted a\n    document */\n    if (result.deletedCount === 1) {\n      console.log(\"Successfully deleted one document.\");\n    } else {\n      console.log(\"No documents matched the query. Deleted 0 documents.\");\n    }\n  } finally {\n    // Close the connection after the operation completes\n    await client.close();\n  }\n}\n// Run the program and print any thrown exceptions\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can delete a single document in a collection with\ncollection.deleteOne().\nThe deleteOne() method uses a query document that you provide\nto match the subset of the documents in the collection that match\nthe query. If you do not provide a query document (or if you provide an\nempty document), MongoDB matches all documents in the collection and\ndeletes the first match.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/distinct",
            "title": "Retrieve Distinct Values of a Field",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can retrieve a list of distinct values for a field across a collection by using\nthe  collection.distinct() \nmethod. Call the  distinct()  method on a  Collection  object with a document\nfield name parameter as a  String  to produce a list that contains one of each\nof the different values found in the specified document field as shown below: You can specify a document field within an  embedded document  using\n dot notation . If you call\n distinct()  on an document field that contains an array, the method\ntreats each element as a separate value. See the following example of\na method call to the  wins  field in the  awards  subdocument: You can specify more query options using the  options  object passed\nas the third parameter to the  distinct()  method. For details on the\nquery parameters, see the\n distinct() method in the API documentation . If you specify a value for the document field name that is not of type\n String  such as a  Document ,  Array ,  Number , or  null ,\nthe method does not execute and returns a  TypeMismatch  error with a\nmessage that resembles the following: Visit  Retrieve Distinct Values  for more information about the  distinct() \nmethod. \"key\" had the wrong type. Expected string, found <non-string type> The following snippet retrieves a list of distinct values for the  year \ndocument field from the  movies  collection. It uses a query document to\nmatch movies that include \"Barbara Streisand\" as a  director . Running the preceding example, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const distinctValues = myColl.distinct(\"countries\", query);"
                },
                {
                    "lang": "javascript",
                    "value": "const distinctValues = myColl.distinct(\"awards.wins\", query);"
                },
                {
                    "lang": "json",
                    "value": "[ 1983, 1991, 1996 ]"
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    \n    // Get the database and collection on which to run the operation\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    // Specify the document field to find distinct values for\n    const fieldName = \"year\";\n\n    // Specify an optional query document to narrow results\n    const query = { directors: \"Barbra Streisand\" };\n\n    // Execute the distinct operation\n    const distinctValues = await movies.distinct(fieldName, query);\n\n    // Print the result\n    console.log(distinctValues);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ninterface Movie {\n  directors: string;\n  year: number;\n}\n\nasync function run() {\n  try {\n    // define a database and collection on which to run the method\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection<Movie>(\"movies\");\n\n    const distinctValues = await movies.distinct(\"year\", {\n      directors: \"Barbra Streisand\",\n    });\n\n    console.log(distinctValues);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can retrieve a list of distinct values for a field across a collection by using\nthe collection.distinct()\nmethod. Call the distinct() method on a Collection object with a document\nfield name parameter as a String to produce a list that contains one of each\nof the different values found in the specified document field as shown below:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/find-operations",
            "title": "Find Operations",
            "headings": [],
            "paragraphs": "Find a Document Find Multiple Documents",
            "code": [],
            "preview": "Learn by example: how to create queries and retrieve data from MongoDB by using the MongoDB Node.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/find",
            "title": "Find Multiple Documents",
            "headings": [
                "Compatibility",
                "Example"
            ],
            "paragraphs": "You can query for multiple documents in a collection with\n collection.find() . The  find()  method uses a query document that you\nprovide to match the subset of the documents in the collection that match the\nquery. If you don't provide a query document (or if you provide an empty\ndocument), MongoDB returns all documents in the collection. For more\ninformation on querying MongoDB, see our\n documentation on query documents . You can also define more query options such as\n sort \nand\n projection \nto configure the result set. You can specify these in the options\nparameter in your  find()  method call in  sort  and  projection \nobjects. See  collection.find()  for more\ninformation on the parameters you can pass to the method. The  find()  method returns a  FindCursor  that\nmanages the results of your query. You can iterate through the matching\ndocuments using the  for await...of  syntax, or one of the following\n cursor methods : If no documents match the query,  find()  returns an empty cursor. next() toArray() You can use the Node.js driver to connect and  use the  find()  method  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  finding documents in the Atlas UI  for deployments hosted in MongoDB\nAtlas, see  Create, View, Update, and Delete Documents . The following snippet finds documents from the  movies  collection. It\nuses the following parameters: Running the preceding example, you see the following output: The  sort  and  projection  options can also be specified as methods\n( sort()  and  project() ) chained to the  find()  method.\nThe following two commands are equivalent: A  query document  that configures the query to return only\nmovies with a runtime of less than 15 minutes. A  sort  that organizes returned documents in ascending order by\ntitle (alphabetical order in which \"A\" comes before \"Z\" and \"1\" before\n\"9\"). A  projection  that explicitly excludes the  _id  field from\nreturned documents and explicitly includes only the  title  and\n imdb  object (and its embedded fields). You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{ title: '10 Minutes', imdb: { rating: 7.9, votes: 743, id: 339976 } }\n{ title: '3x3', imdb: { rating: 6.9, votes: 206, id: 1654725 } }\n{ title: '7:35 in the Morning', imdb: { rating: 7.3, votes: 1555, id: 406501 } }\n{ title: '8', imdb: { rating: 7.8, votes: 883, id: 1592502 } }\n..."
                },
                {
                    "lang": "javascript",
                    "value": "movies.find({ runtime: { $lt: 15 } }, { sort: { title: 1 }, projection: { _id: 0, title: 1, imdb: 1 }});\nmovies.find({ runtime: { $lt: 15 } }).sort({ title: 1}).project({ _id: 0, title: 1, imdb: 1 });"
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    \n    // Get the database and collection on which to run the operation\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    // Query for movies that have a runtime less than 15 minutes\n    const query = { runtime: { $lt: 15 } };\n\n    const options = {\n      // Sort returned documents in ascending order by title (A->Z)\n      sort: { title: 1 },\n      // Include only the `title` and `imdb` fields in each returned document\n      projection: { _id: 0, title: 1, imdb: 1 },\n    };\n\n    // Execute query \n    const cursor = movies.find(query, options);\n\n    // Print a message if no documents were found\n    if ((await movies.countDocuments(query)) === 0) {\n      console.log(\"No documents found!\");\n    }\n\n    // Print returned documents\n    for await (const doc of cursor) {\n      console.dir(doc);\n    }\n\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ntype Minutes = number;\n\ninterface IMDB {\n  rating: number;\n  votes: number;\n  id: number;\n}\n\ninterface Movie {\n  title: string;\n  imdb: IMDB;\n  runtime: Minutes;\n}\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection<Movie>(\"movies\");\n\n    const query = { runtime: { $lt: 15 } };\n    const cursor = movies.find<Movie>(\n      query,\n      {\n        sort: { title: 1 },\n        projection: { _id: 0, title: 1, imdb: 1 },\n      }\n    );\n\n    if ((await movies.countDocuments(query)) === 0) {\n      console.warn(\"No documents found!\");\n    }\n\n    for await (const doc of cursor) {\n      console.dir(doc);\n    } \n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "Learn how to retrieve multiple documents from MongoDB by using the Node.js driver.",
            "tags": "code example, node.js, sample dataset",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/findOne",
            "title": "Find a Document",
            "headings": [
                "Compatibility",
                "Example"
            ],
            "paragraphs": "You can query for a single document in a collection with the\n collection.findOne()  method. The  findOne()  method uses a query\ndocument that you provide to match only the subset of the documents in the\ncollection that match the query. If you don't provide a query document or if\nyou provide an empty document, MongoDB matches all documents in the\ncollection. The  findOne()  operation only returns the first matched\ndocument. For more information on querying MongoDB, see our\n documentation on query documents . You can also define more query options such as\n sort \nand  projection \nto configure the returned document. You can specify the more options\nin the  options  object passed as the second parameter of the\n findOne  method. For detailed reference documentation, see\n collection.findOne() . You can use the Node.js driver to connect and  use the  findOne()  method  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  finding documents in the Atlas UI  for deployments hosted in MongoDB\nAtlas, see  Create, View, Update, and Delete Documents . The following snippet finds a single document from the  movies \ncollection. It uses the following parameters: Running the preceding example, you see the following output: A  query document  that configures the query to return only\nmovies with the title of exactly the text  'The Room' . A  sort  that organizes matched documents in descending order by\nrating, so if our query matches multiple documents the returned\ndocument will be the document with the highest rating. A  projection  that explicitly excludes the  _id  field from\nreturned documents and explicitly includes only the  title  and\n imdb  object (and its embedded fields). You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{ title: 'The Room', imdb: { rating: 3.5, votes: 25673, id: 368226 } }"
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    \n    // Get the database and collection on which to run the operation\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    // Query for a movie that has the title 'The Room'\n    const query = { title: \"The Room\" };\n\n    const options = {\n      // Sort matched documents in descending order by rating\n      sort: { \"imdb.rating\": -1 },\n      // Include only the `title` and `imdb` fields in the returned document\n      projection: { _id: 0, title: 1, imdb: 1 },\n    };\n\n    // Execute query\n    const movie = await movies.findOne(query, options);\n\n    // Print the document returned by findOne()\n    console.log(movie);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ninterface IMDB {\n  rating: number;\n  votes: number;\n  id: number;\n}\n\nexport interface Movie {\n  title: string;\n  year: number;\n  released: Date;\n  plot: string;\n  type: \"movie\" | \"series\";\n  imdb: IMDB;\n}\n\ntype MovieSummary = Pick<Movie, \"title\" | \"imdb\">;\n\nasync function run(): Promise<void> {\n  try {\n    const database = client.db(\"sample_mflix\");\n    // Specifying a Schema is always optional, but it enables type hinting on\n    // finds and inserts\n    const movies = database.collection<Movie>(\"movies\");\n\n    const movie = await movies.findOne<MovieSummary>(\n      { title: \"The Room\" },\n      {\n        sort: { rating: -1 },\n        projection: { _id: 0, title: 1, imdb: 1 },\n      }\n    );\n    console.log(movie);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "Learn how to retrieve one document from MongoDB by using the Node.js driver.",
            "tags": "code example, node.js, sample dataset",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/insert-operations",
            "title": "Insert Operations",
            "headings": [],
            "paragraphs": "Insert a Document Insert Multiple Documents",
            "code": [],
            "preview": "Learn by example: how to insert data into MongoDB by using the MongoDB Node.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/insertMany",
            "title": "Insert Multiple Documents",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can insert multiple documents using the\n collection.insertMany()  method. The  insertMany()  takes an array\nof documents to insert into the specified collection. You can specify more options in the  options  object passed as the\nsecond parameter of the  insertMany()  method. Specify  ordered:true \nto prevent inserting the remaining documents if the insertion failed for a\nprevious document in the array. Specifying incorrect parameters for your  insertMany()  operation can\ncause problems. Attempting to insert a field with a value that violates\nunique index rules results in a  duplicate key error . Running the preceding example, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "none",
                    "value": "3 documents were inserted"
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n\n    // Get the database and collection on which to run the operation\n    const database = client.db(\"insertDB\");\n    const foods = database.collection(\"foods\");\n\n    // Create an array of documents to insert\n    const docs = [\n      { name: \"cake\", healthy: false },\n      { name: \"lettuce\", healthy: true },\n      { name: \"donut\", healthy: false }\n    ];\n\n    // Prevent additional documents from being inserted if one fails\n    const options = { ordered: true };\n\n    // Execute insert operation\n    const result = await foods.insertMany(docs, options);\n   \n    // Print result\n    console.log(`${result.insertedCount} documents were inserted`);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ninterface Food {\n  name: string;\n  healthy: boolean;\n}\n\nasync function run() {\n  try {\n    const database = client.db(\"insertDB\");\n    // Specifying a schema is optional, but it enables type hints on\n    // finds and inserts\n    const foods = database.collection<Food>(\"foods\");\n\n    const result = await foods.insertMany(\n      [\n        { name: \"cake\", healthy: false },\n        { name: \"lettuce\", healthy: true },\n        { name: \"donut\", healthy: false },\n      ],\n      { ordered: true }\n    );\n    console.log(`${result.insertedCount} documents were inserted`);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can insert multiple documents using the\ncollection.insertMany() method. The insertMany() takes an array\nof documents to insert into the specified collection.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/insertOne",
            "title": "Insert a Document",
            "headings": [
                "Compatibility",
                "Example"
            ],
            "paragraphs": "You can insert a document into a collection using the\n collection.insertOne()  method. To\ninsert a document, define an object that contains the fields and values that\nyou want to store. If the specified collection does not exist, the\n insertOne()  method creates the collection. You can specify more query options using the  options  parameter.\nFor more information on the method parameters, see the\n insertOne() API documentation .\nFor more information on this method, see the\n insertOne() API documentation . If the operation successfully inserts a document, it appends an\n insertedId  field to the object passed in the method call, and sets the\nvalue of the field to the  _id  of the inserted document. You can use the Node.js driver to connect and  use the  insertOne()  method  for\ndeployments hosted in the following environments: MongoDB Atlas : The fully\nmanaged service for MongoDB deployments in the cloud MongoDB Enterprise : The\nsubscription-based, self-managed version of MongoDB MongoDB Community : The\nsource-available, free-to-use, and self-managed version of MongoDB To learn more about  inserting documents in the Atlas UI  for deployments hosted in MongoDB\nAtlas, see  Create, View, Update, and Delete Documents . Running the preceding example, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "none",
                    "value": "A document was inserted with the _id: <your _id value>"
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\n// Create a new client and connect to MongoDB\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Connect to the \"insertDB\" database and access its \"haiku\" collection\n    const database = client.db(\"insertDB\");\n    const haiku = database.collection(\"haiku\");\n    \n    // Create a document to insert\n    const doc = {\n      title: \"Record of a Shriveled Datum\",\n      content: \"No bytes, no problem. Just insert a document, in MongoDB\",\n    }\n    // Insert the defined document into the \"haiku\" collection\n    const result = await haiku.insertOne(doc);\n\n    // Print the ID of the inserted document\n    console.log(`A document was inserted with the _id: ${result.insertedId}`);\n  } finally {\n     // Close the MongoDB client connection\n    await client.close();\n  }\n}\n// Run the function and handle any errors\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ninterface Haiku {\n  title: string;\n  content: string;\n}\n\nasync function run() {\n  try {\n    const database = client.db(\"insertDB\");\n    // Specifying a Schema is optional, but it enables type hints on\n    // finds and inserts\n    const haiku = database.collection<Haiku>(\"haiku\");\n    const result = await haiku.insertOne({\n      title: \"Record of a Shriveled Datum\",\n      content: \"No bytes, no problem. Just insert a document, in MongoDB\",\n    });\n    console.log(`A document was inserted with the _id: ${result.insertedId}`);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "Learn how to insert a document into MongoDB by using the Node.js driver.",
            "tags": "code example, node.js, sample dataset",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/replaceOne",
            "title": "Replace a Document",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can replace a single document using the\n collection.replaceOne()  method.\n replaceOne()  accepts a query document and a replacement document. If\nthe query matches a document in the collection, it replaces the first\ndocument that matches the query with the provided replacement document.\nThis operation removes all fields and values in the original document and\nreplaces them with the fields and values in the replacement document. The\nvalue of the  _id  field remains the same unless you explicitly specify\na new value for  _id  in the replacement document. You can specify more options, such as  upsert , using the\noptional  options  parameter. If you set the  upsert  option field to\n true  the method inserts a new document if no document matches the query. The  replaceOne()  method throws an exception if an error occurs\nduring execution. For example, if you specify a value that violates a\nunique index rule,  replaceOne()  throws a  duplicate key error . If your application requires the document after updating,\nuse the  collection.findOneAndReplace() \nmethod which has a similar interface to  replaceOne() .\nYou can configure  findOneAndReplace()  to return either the\noriginal matched document or the replacement document. Running the preceding example, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "none",
                    "value": "Modified 1 document(s)"
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    \n    // Get the database and collection on which to run the operation\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    // Create a query for documents where the title contains \"The Cat from\"\n    const query = { title: { $regex: \"The Cat from\" } };\n    \n    // Create the document that will replace the existing document\n    const replacement = {\n      title: `The Cat from Sector ${Math.floor(Math.random() * 1000) + 1}`,\n    };\n\n    // Execute the replace operation\n    const result = await movies.replaceOne(query, replacement);\n    \n    // Print the result \n    console.log(`Modified ${result.modifiedCount} document(s)`);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "import { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\ninterface Movie {\n  title: string;\n}\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection<Movie>(\"movies\");\n\n    const result = await movies.replaceOne(\n      { title: { $regex: \"The Cat from\" } },\n      {\n        title: `The Cat from Sector ${Math.floor(Math.random() * 1000) + 1}`,\n      }\n    );\n    console.log(`Modified ${result.modifiedCount} document(s)`);\n  } finally {\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can replace a single document using the\ncollection.replaceOne() method.\nreplaceOne() accepts a query document and a replacement document. If\nthe query matches a document in the collection, it replaces the first\ndocument that matches the query with the provided replacement document.\nThis operation removes all fields and values in the original document and\nreplaces them with the fields and values in the replacement document. The\nvalue of the _id field remains the same unless you explicitly specify\na new value for _id in the replacement document.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/transaction-conv",
            "title": "Use the Convenient Transaction API",
            "headings": [
                "Example",
                "Sample Data",
                "Implementation",
                "Sample Orders and Transaction Results",
                "API Documentation"
            ],
            "paragraphs": "You can perform a transaction to run a series of operations that do not\nchange any data until the entire transaction is committed. This usage\nexample uses the  Convenient Transaction API  to perform a transaction. To learn more about the performing transactions in the\nNode.js driver, see the  Transactions  guide. The Node.js driver also provides the Core API to perform\ntransactions. To learn more about the Core API, see the\n Use the Core API  usage example. Consider a situation in which a customer purchases items from your shop.\nTo record the purchase, your application must update\nyour inventory and record the order information. The following table describes the collections that store purchase data\nand how a purchase changes the data in each collection. Collection Operation Description of the Change orders insert Inserts a document that describes the order inventory update Updates the quantities of items available after a purchase The  inventory  collection contains the\nfollowing documents: You store purchase records in the  orders  collection of the\n testdb  database. This collection is empty, as there have been no\npurchases. The code example in this section demonstrates how to use the Convenient\nTransaction API to perform a multi-document transaction in a session. In\nthis example, the transaction makes the changes needed when a\ncustomer purchases items from your shop. This example code performs a transaction through the following actions: Calls the  withSession()  method on the client to implicitly create\nthe session and run the callback passed to it within the session. Calls the  withTransaction()  method on the session to create a\ntransaction, run the callback passed to it, and commit the\ntransaction. If the transaction fails, this method ends the\ntransaction and returns an error message. Performs the following operations within the transaction: Updates the  inventory  and  orders  collections if there is\nsufficient inventory to fulfill the purchase Ends the transaction and throws an exception if there isn't\nsufficient inventory for any item in the order Returns a message acknowledging that the transaction\ncommitted successfully with a copy of the purchase record Prints the return type of  withSession() , which is either the\nerror message or the acknowledgment that the transaction completed. This section describes the results of the transactions performed for two\nsample orders. Sufficient inventory exists for the following order, so the transaction\nsuccessfully completes: After passing this order to the example transaction code, the code\noutputs the following result: In the  inventory  collection, the quantity of\n \"sunblock\"  is now  82  and the quantity of  \"beach chair\" \nis  29 . The  orders  collection contains the record of the\npurchase. There is not sufficient inventory for the following order, so the\ndriver ends the transaction: After passing this order to the example transaction code, the code\noutputs the following result: Since the driver ends the transaction, there are no changes to\nthe  inventory  and  orders  collections. To learn more about any of the methods or types discussed in this\nusage example, see the following API Documentation: withSession() withTransaction() abortTransaction() \"",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{ item: \"sunblock\", qty: 85, price: 6.0 },\n{ item: \"beach chair\", qty: 30, price: 25.0 }"
                },
                {
                    "lang": "javascript",
                    "value": "const txnResult = await client.withSession(async (session) =>\n  session\n    .withTransaction(async (session) => {\n      const invColl = client.db(\"testdb\").collection(\"inventory\");\n      const recColl = client.db(\"testdb\").collection(\"orders\");\n\n      let total = 0;\n      for (const item of order) {\n        /* Update the inventory for the purchased items. End the\n        transaction if the quantity of an item in the inventory is\n        insufficient to complete the purchase. */\n        const inStock = await invColl.findOneAndUpdate(\n          {\n            item: item.item,\n            qty: { $gte: item.qty },\n          },\n          { $inc: { qty: -item.qty } },\n          { session }\n        );\n        if (inStock === null) {\n          await session.abortTransaction();\n          return \"Item not found or insufficient quantity.\";\n        }\n        const subTotal = item.qty * inStock.price;\n        total = total + subTotal;\n      }\n\n      // Create a record of the purchase\n      const receipt = {\n        date: new Date(),\n        items: order,\n        total: total,\n      };\n      await recColl.insertOne(receipt, { session });\n      return (\n        \"Order successfully completed and recorded!\\nReceipt:\\n\" +\n        JSON.stringify(receipt, null, 1)\n      );\n    }, null)\n    .finally(async () => await client.close())\n);\n\nconsole.log(txnResult);"
                },
                {
                    "lang": "none",
                    "value": "Order successfully completed and recorded!\nReceipt:\n{\n  \"date\": \"2023-08-25T20:06:52.564Z\",\n  \"items\": [\n    { \"item\": \"sunblock\", \"qty\": 3 },\n    { \"item\": \"beach chair\", \"qty\": 1 }\n  ],\n  \"total\": 43,\n  \"_id\": \"...\"\n}"
                },
                {
                    "lang": "none",
                    "value": "Item not found or insufficient quantity."
                },
                {
                    "lang": "javascript",
                    "value": "{ item: \"sunblock\", qty: 3 },\n{ item: \"beach chair\", qty: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "{ item: \"volleyball\", qty: 1 }"
                }
            ],
            "preview": "You can perform a transaction to run a series of operations that do not\nchange any data until the entire transaction is committed. This usage\nexample uses the Convenient Transaction API to perform a transaction.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/transaction-core",
            "title": "Use the Core API",
            "headings": [
                "Example",
                "Sample Data",
                "Implementation",
                "Transaction Results",
                "API Documentation"
            ],
            "paragraphs": "You can perform a transaction to run a series of operations that do not\nchange any data until the entire transaction is committed. This usage\nexample uses the  Core API  to perform a transaction. To learn more about the performing transactions in the\nNode.js driver, see the  Transactions  guide. The Node.js driver also provides the Convenient Transaction API to\nperform transactions. To learn more about the Convenient Transaction\nAPI, see the  Use the Convenient Transaction API  usage example. Consider a situation in which a customer purchases items from your online\nshop. To record the purchase, your application must update\nyour inventory and the customer's orders. Your\napplication also needs to save the order details. The following table describes the collections that store purchase data\nand how a purchase changes the data in each collection. Collection Operation Description of the Change orders insert Inserts a document that describes the order customers update or upsert Appends the  _id  from the order document to the order history\nin the customer document inventory update Updates the quantities of items available after a purchase The code examples use the following sample data in the  testdb \ndatabase: The following document is in the  customers  collection: The  inventory  collection contains the following documents: You store purchase records in the  orders  collection of the\n testdb  database. This collection is empty, as there have been no\npurchases. The code examples use the  cart  and  payment  variables to represent\na sample list of items purchased and the order payment details. The\nfollowing code describes the contents of the  cart  and  payment  variables: Documents in the  customers  collection that describe customers and\ntheir past orders Documents in the  inventory  collection that include quantities and\ndescriptions of all items The code example in this section demonstrates how to use the Core API to\nperform a multi-document transaction in a session. In this example, the\ntransaction makes the changes needed when a customer purchases items from\nyour shop. This example code performs a transaction through the following actions: Calls the  startSession()  method to create a new session Calls the  startTransaction()  method with an options parameter to\ncreate a new transaction Performs the following operations within the transaction: Inserts a document to the  orders  collection that contains\ninformation about the purchase and customer Updates the  inventory  collection if there is\nsufficient inventory to fulfill the purchase Ends the transaction and throws an exception if there isn't\nsufficient inventory for any item in the order Adds the ID of the order to the list of past orders for the customer Returns a message acknowledging that the transaction\ncommitted successfully with a copy of the purchase record Calls the  commitTransaction()  method to commit the transaction if\nall operations complete successfully Implements a  catch  block that contains error-handling logic Calls the  abortTransaction()  method to end the transaction Calls the  endSession()  method to end the session This section describes the data changes created by the transaction. The  customers  collection contains the customer document with an\norder  _id  appended to the orders field: The  inventory  collection contains updated quantities for the\nitems  \"sunblock\"  and  \"beach towel\" : The  orders  collection contains the order and payment\ninformation: To learn more about any of the methods or types discussed in this\nusage example, see the following API Documentation: TransactionOptions ClientSession startSession() startTransaction() commitTransaction() abortTransaction() endSession()",
            "code": [
                {
                    "lang": "json",
                    "value": "{ _id: 98765, orders: [] }"
                },
                {
                    "lang": "json",
                    "value": "{ item: \"sunblock\", item_id: 5432, qty: 85 },\n{ item: \"beach towel\", item_id: 7865, qty: 41 }"
                },
                {
                    "lang": "javascript",
                    "value": "const cart = [\n  { item: 'sunblock', item_id: 5432, qty: 1, price: 5.19 },\n  { item: 'beach towel', item_id: 7865, qty: 2, price: 15.99 }\n];\nconst payment = { customer: 98765, total: 37.17 };"
                },
                {
                    "lang": "javascript",
                    "value": "async function placeOrder(client, cart, payment) {\n  const transactionOptions = {\n    readConcern: { level: 'snapshot' },\n    writeConcern: { w: 'majority' },\n    readPreference: 'primary'\n  };\n\n  // Start the session\n  const session = client.startSession();\n  try {\n    // Start the transaction in the session, specifying the transaction options\n    session.startTransaction(transactionOptions);\n\n    const ordersCollection = client.db('testdb').collection('orders');\n    /* Within the session, insert an order that contains information about the\n    customer, items purchased, and the total payment */\n    const orderResult = await ordersCollection.insertOne(\n      {\n        customer: payment.customer,\n        items: cart,\n        total: payment.total,\n      },\n      { session }\n    );\n\n    const inventoryCollection = client.db('testdb').collection('inventory');\n    \n    for (const item of order) {  \n      /* Update the inventory for the purchased items. End the\n      transaction if the quantity of an item in the inventory is\n      insufficient to complete the purchase. */\n      const inStock = await inventoryCollection.findOneAndUpdate(\n        {\n          item_id: item.item_id,\n          item_id: { $gte: item.qty }\n        },\n        { $inc: { 'qty': -item.qty }},\n        { session }\n      )\n      if (inStock === null) {\n        throw new Error('Insufficient quantity or item ID not found.');\n      }\n    }\n\n    const customerCollection = client.db('testdb').collection('customers');\n\n    // Within the session, add the order details to the \"orders\" array of the customer document\n    await customerCollection.updateOne(\n      { _id: payment.customer },\n      { $push:  { orders: orderResult.insertedId }},\n      { session }\n    );\n\n    // Commit the transaction to apply all updates performed within it\n    await session.commitTransaction();\n    console.log('Transaction successfully committed.');\n\n  } catch (error) {\n    /*\n      Handle any exceptions thrown during the transaction and end the\n      transaction. Roll back all the updates performed in the transaction.\n    */\n    if (error instanceof MongoError && error.hasErrorLabel('UnknownTransactionCommitResult')) {\n      // Add your logic to retry or handle the error\n    }\n    else if (error instanceof MongoError && error.hasErrorLabel('TransientTransactionError')) {\n      // Add your logic to retry or handle the error\n    } else {\n      console.log('An error occured in the transaction, performing a data rollback:' + error);\n    }\n    await session.abortTransaction();\n  } finally {\n    // End the session\n    await session.endSession();\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": 98765,\n  \"orders\": [\n    \"61dc...\"\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"_id\": ...,\n    \"item\": \"sunblock\",\n    \"item_id\": 5432,\n    \"qty\": 84\n  },\n  {\n    \"_id\": ...,\n    \"item\": \"beach towel\",\n    \"item_id\": 7865,\n    \"qty\": 39\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"_id\": \"...\",\n    \"customer\": 98765,\n    \"items\": [\n      {\n        \"item\": \"sunblock\",\n        \"item_id\": 5432,\n        \"qty\": 1,\n        \"price\": 5.19\n      },\n      {\n        \"item\": \"beach towel\",\n        \"item_id\": 7865,\n        \"qty\": 2,\n        \"price\": 15.99\n      }\n    ],\n    \"total\": 37.17\n  }\n]"
                }
            ],
            "preview": "You can perform a transaction to run a series of operations that do not\nchange any data until the entire transaction is committed. This usage\nexample uses the Core API to perform a transaction.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/transactions",
            "title": "Perform a Transaction",
            "headings": [],
            "paragraphs": "The following usage examples demonstrate how to perform transactions by\nusing the transaction APIs in the Node.js driver: Use the Convenient Transaction API Use the Core API",
            "code": [],
            "preview": "The following usage examples demonstrate how to perform transactions by\nusing the transaction APIs in the Node.js driver:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/update-and-replace-operations",
            "title": "Update & Replace Operations",
            "headings": [],
            "paragraphs": "Update a Document Update Multiple Documents Replace a Document",
            "code": [],
            "preview": "Learn by example: how to update and replace data in MongoDB by using the MongoDB Node.js driver.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/updateMany",
            "title": "Update Multiple Documents",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can update multiple documents using the\n collection.updateMany()  method.\nThe  updateMany()  method accepts a filter document and an update document. If the query matches documents in the\ncollection, the method applies the updates from the update document to fields\nand values of the matching documents. The update document requires an  update operator  to modify a field in a document. You can specify more options in the  options  object passed in\nthe third parameter of the  updateMany()  method. For more detailed\ninformation, see\n the updateMany() API documentation . Running the preceding example, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "none",
                    "value": "Updated 477 documents"
                },
                {
                    "lang": "javascript",
                    "value": "/* Update multiple documents */\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    // Get the \"movies\" collection in the \"sample_mflix\" database\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    // Create a filter to update all movies with a 'G' rating\n    const filter = { rated: \"G\" };\n\n    // Create an update document specifying the change to make\n    const updateDoc = {\n      $set: {\n        random_review: `After viewing I am ${\n          100 * Math.random()\n        }% more satisfied with life.`,\n      },\n    };\n    // Update the documents that match the specified filter\n    const result = await movies.updateMany(filter, updateDoc);\n    console.log(`Updated ${result.modifiedCount} documents`);\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "/* Update multiple documents */\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string.\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nenum Rating {\n  G = \"G\",\n  PG = \"PG\",\n  PG_13 = \"PG-13\",\n  R = \"R\",\n  NR = \"NOT RATED\",\n}\n\n// Create a Movie interface\ninterface Movie {\n  rated: Rating;\n  random_review?: string;\n}\n\nasync function run() {\n  try {\n    // Get the \"movies\" collection in the \"sample_mflix\" database\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection<Movie>(\"movies\");\n\n    // Update all documents that match the specified filter\n    const result = await movies.updateMany(\n      { rated: Rating.G },\n      {\n        $set: {\n          random_review: `After viewing I am ${\n            100 * Math.random()\n          }% more satisfied with life.`,\n        },\n      }\n    );\n    console.log(`Updated ${result.modifiedCount} documents`);\n  } finally {\n    // Close the database connection on completion or error\n    await client.close();\n  }\n}\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can update multiple documents using the\ncollection.updateMany() method.\nThe updateMany() method accepts a filter document and an update document. If the query matches documents in the\ncollection, the method applies the updates from the update document to fields\nand values of the matching documents. The update document requires an update operator to modify a field in a document.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples/updateOne",
            "title": "Update a Document",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can update a single document using the\n collection.updateOne() \nmethod. The  updateOne()  method accepts a filter\ndocument and an update document. If the query matches documents in the\ncollection, the method applies the updates from the update document to fields\nand values of them. The update document contains  update operators  that instruct the method\non the changes to make to the matches. You can specify more query options using the  options  object\npassed as the second parameter of the  updateOne()  method.\nSet the  upsert  option to  true  to create a new document\nif no documents match the filter. For more information, see the\n updateOne() API documentation . updateOne()  throws an exception if an error occurs during execution.\nIf you specify a value in your update document for the immutable field\n _id , the method throws an exception. If your update document contains\na value that violates unique index rules, the method throws a  duplicate\nkey error  exception. If your application requires the document after updating,\nconsider using the\n collection.findOneAndUpdate() .\nmethod, which has a similar\ninterface to  updateOne()  but also returns the original or updated\ndocument. The following example uses the  $set  update operator which specifies\nupdate values for document fields. For more information on update operators,\nsee the  MongoDB update operator reference documentation . If you run the example above, you see the following output: You can use this example to connect to an instance of MongoDB\nand interact with a database that contains sample data. To learn more about connecting to your MongoDB\ninstance and loading a sample dataset, see the  Usage Examples\nguide .",
            "code": [
                {
                    "lang": "none",
                    "value": "1 document(s) matched the filter, updated 1 document(s)"
                },
                {
                    "lang": "javascript",
                    "value": "// Update a document\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection(\"movies\");\n\n    // Create a filter for movies with the title \"Random Harvest\"\n    const filter = { title: \"Random Harvest\" };\n\n    /* Set the upsert option to insert a document if no documents match\n    the filter */\n    const options = { upsert: true };\n\n    // Specify the update to set a value for the plot field\n    const updateDoc = {\n      $set: {\n        plot: `A harvest of random numbers, such as: ${Math.random()}`\n      },\n    };\n\n    // Update the first document that matches the filter\n    const result = await movies.updateOne(filter, updateDoc, options);\n    \n    // Print the number of matching and modified documents\n    console.log(\n      `${result.matchedCount} document(s) matched the filter, updated ${result.modifiedCount} document(s)`,\n    );\n  } finally {\n    // Close the connection after the operation completes\n    await client.close();\n  }\n}\n// Run the program and print any thrown errors\nrun().catch(console.dir);\n"
                },
                {
                    "lang": "typescript",
                    "value": "// Update a document\n\nimport { MongoClient } from \"mongodb\";\n\n// Replace the uri string with your MongoDB deployment's connection string\nconst uri = \"<connection string uri>\";\n\nconst client = new MongoClient(uri);\n\n// Define the Movie interface\ninterface Movie {\n  plot: string;\n  title: string;\n}\n\nasync function run() {\n  try {\n    const database = client.db(\"sample_mflix\");\n    const movies = database.collection<Movie>(\"movies\");\n\n    /* Update a document that has the title \"Random Harvest\" to have a\n    plot field with the specified value */\n    const result = await movies.updateOne(\n      { title: \"Random Harvest\" },\n      {\n        $set: {\n          plot: `A harvest of random numbers, such as: ${Math.random()}`,\n        },\n      },\n      /* Set the upsert option to insert a document if no documents\n      match the filter */\n      { upsert: true }\n    );\n\n    // Print the number of matching and modified documents\n    console.log(\n      `${result.matchedCount} document(s) matched the filter, updated ${result.modifiedCount} document(s)`\n    );\n  } finally {\n    // Close the connection after the operation completes\n    await client.close();\n  }\n}\n// Run the program and print any thrown errors\nrun().catch(console.dir);\n"
                }
            ],
            "preview": "You can update a single document using the\ncollection.updateOne()\nmethod. The updateOne() method accepts a filter\ndocument and an update document. If the query matches documents in the\ncollection, the method applies the updates from the update document to fields\nand values of them. The update document contains update operators that instruct the method\non the changes to make to the matches.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "usage-examples",
            "title": "Usage Examples",
            "headings": [
                "Overview",
                "How to Use the Usage Examples",
                "Available Usage Examples"
            ],
            "paragraphs": "Usage examples provide convenient starting points for popular MongoDB\noperations. Each example provides the following information: Explanation of the operation in the example, including the\npurpose and a sample use case for the method Explanation of how to use the operation, including parameters,\nreturn values, and common exceptions you might encounter Full Node.js program that you can copy and paste to run the example\nin your own environment These examples use the\n MongoDB Atlas sample data \ndatabase. You can use this sample data on the free tier\nof MongoDB Atlas by following the  Get Started with Atlas  guide or you\ncan  import the sample dataset into a local MongoDB instance . Once you have imported the dataset, you can copy and paste a usage\nexample into your development environment of choice. You can follow the\n quick start guide  to learn more about getting\nstarted with Node.js, npm, and the Node.js driver. Once you've copied\na usage example, you must edit one line to get the example running\nwith your instance of MongoDB: All examples use ES module imports. You can\n enable ES module imports \nby adding the following key-value pair to your package.json file: You can use the  Atlas Connectivity Guide  to enable connectivity to your instance of\nAtlas and find the  connection string  to replace the  uri  variable in the\nusage example. If your instance uses  SCRAM authentication , you can replace  <user>  with your username,\n <password>  with your password, and  <cluster-url>  with the IP\naddress or URL of your instance. Consult the\n Connection Guide  for more information\nabout getting connected to your MongoDB instance. You can use any usage example with CommonJS  require . To use CommonJS  require , you\nmust swap out the ES module  import  statement for your CommonJS  require \nstatement. Click on the tabs to see the syntax for importing the driver with ES module\n import  and CommonJS  require : Find Operations Insert Operations Update Operations Delete Operations Count Documents Retrieve Distinct Values of a Field Run a Command Watch for Changes Perform Bulk Operations Perform a Transaction",
            "code": [
                {
                    "lang": "javascript",
                    "value": "// Replace the following with your MongoDB deployment's connection string.\nconst uri =\n   \"mongodb+srv://<user>:<password>@<cluster-url>?retryWrites=true&writeConcern=majority\";"
                },
                {
                    "lang": "json",
                    "value": "\"type\": \"module\""
                },
                {
                    "lang": "javascript",
                    "value": "import { MongoClient } from 'mongodb'"
                },
                {
                    "lang": "javascript",
                    "value": "const { MongoClient } = require('mongodb')"
                }
            ],
            "preview": "Learn how to load sample data into a MongoDB Atlas deployment and run Node.js driver usage examples.",
            "tags": "node.js",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        },
        {
            "slug": "whats-new",
            "title": "What's New",
            "headings": [
                "What's New in 6.8",
                "What's New in 6.7",
                "What's New in 6.6",
                "What's New in 6.5",
                "What's New in 6.4",
                "What's New in 6.3",
                "What's New in 6.2",
                "What's New in 6.1",
                "What's New in 6.0",
                "What's New in 5.9",
                "What's New in 5.8",
                "What's New in 5.7",
                "What's New in 5.6",
                "What's New in 5.5",
                "What's New in 5.4",
                "What's New in 5.3",
                "What's New in 5.2",
                "What's New in 5.1",
                "What's New in 5.0",
                "What's New in 4.17",
                "What's New in 4.16",
                "What's New in 4.15",
                "What's New in 4.14",
                "What's New in 4.13",
                "What's New in 4.12",
                "What's New in 4.11",
                "Prioritization Order in Monitoring",
                "Changes to AWS Authentication",
                "Mutually Recursive Schema Type Checking",
                "Example",
                "What's New in 4.10",
                "What's New in 4.9",
                "What's New in 4.8",
                "What's New in 4.7",
                "What's New in 4.6",
                "What's New in 4.5",
                "What's New in 4.4",
                "What's New in 4.3",
                "What's New in 4.2",
                "What's New in 4.1",
                "What's New in 4.0",
                "TypeScript",
                "Key Changes",
                "Node.js Version",
                "Cursor Improvements",
                "Cursor Stream API",
                "MongoClientOptions Interface",
                "createCollection()",
                "BulkWriteError \u2192 MongoBulkWriteError",
                "DB",
                "Collection.group()",
                "Authentication",
                "GridStore Removal",
                "Construction",
                "File Seeking",
                "File Upload & Download",
                "File Deletion",
                "Finding File Metadata",
                "Unified Topology",
                "Explain",
                "Command Monitoring",
                "What's New in 3.7",
                "What's New in 3.6"
            ],
            "paragraphs": "Learn what's new in: Version 6.8 Version 6.7 Version 6.6 Version 6.5 Version 6.4 Version 6.3 Version 6.2 Version 6.1 Version 6.0 Version 5.9 Version 5.8 Version 5.7 Version 5.6 Version 5.5 Version 5.4 Version 5.3 Version 5.2 Version 5.1 Version 5.0 Version 4.17 Version 4.16 Version 4.15 Version 4.14 Version 4.13 Version 4.12 Version 4.11 Version 4.10 Version 4.9 Version 4.8 Version 4.7 Version 4.6 Version 4.5 Version 4.4 Version 4.3 Version 4.2 Version 4.1 Version 4.0 Version 3.7 Version 3.6 The Node.js driver v6.8 release includes the following features: To learn more about this release, see the\n v6.8.0 Release Notes  on\nGitHub. Fixes a bug where a local KMS provider accepted a  BSON Binary  instance at\nruntime, but the TypeScript compiler allowed only values of type  Buffer  and\n string . The  ReadConcernMajorityNotAvailableYet  error is now a retryable read error. You can now associate a name with, and provide multiple keys for, KMS providers.\nThis feature requires  mongodb-client-encryption  v6.0.1 or later.\nYou can't use named KMS providers if your application uses the automatic\nKMS provider refresh capability. The following code example shows how to configure a  ClientEncryption  object with\nmultiple AWS keys: When you create a KMIP data key, you can now specify the  delegated  option. If this\noption is set to  true , the KMIP provider performs encryption and decryption of\nthe data key locally, ensuring that the encryption key never leaves the KMIP server.\nThis feature requires  mongodb-client-encryption  v6.0.1 or later. The following code example shows how to specify this option: The driver now decodes BSON responses as the cursor iterates over them,\nrather than decoding the entire BSON response when it is received. The Github release for the  mongodb  package now contains a detached signature file,\n mongodb-X.Y.Z.tgz.sig , for the NPM package. This change applies to every major\nand patch release for versions 5.x and 6.x of the driver. To verify the package signature,\nfollow the instructions in the Release Integrity section of the\n README.md \nfile in the driver's GitHub repository. The Node.js driver v6.7 release includes the following features: To learn more about this release, see the\n v6.7.0 Release Notes  on\nGitHub. Adds support for the  MONGODB-OIDC  authentication mechanism when connected to\nMongoDB Server v7.0 and later. The driver supports authentication with Azure\nmachine authentication, GCP machine authentication, callback authentication,\nand human interaction callback authentication facets. Fixes an issue where setting the  useBigInt64  flag to  true  caused the internal\n compareTopologyVersion  function to generate an error. The Node.js driver v6.6 release includes the following features: To learn more about this release, see the\n v6.6.0 Release Highlights  on\nGitHub. Upgrades to using BSON 6.7.0. For details about the new BSON features, see the\nrelease notes for  BSON 6.5.0 ,\n BSON 6.6.0 , and\n BSON 6.7.0 . Adds the  addStage()  method to the fluid aggregation API. You can use this method to\nadd aggregation pipeline stages individually, as shown in the following\nexample: Adds the  cause  and  dependencyName  fields to the  MongoMissingDependencyError \nclass. You can use these fields to programmatically determine if a package is missing\nor why a package didn't load. Adds the  minRoundTripTime  property to the  ServerDescription  class. This\nproperty contains the minimum round-trip time over the last 10 heartbeats. Adds the  toJSON()  method to the  TopologyDescription  class. Although you can use\nthis method to stringify  TopologyDescription  objects to JSON, we\nrecommend using Node's  util.inspect()  method instead, because it properly handles\nall types used in JavaScript and the driver. Adds cursor options support for the  Collection.indexExists() ,\n Collection.indexes() , and  Collection.indexInformation()  methods in Typescript. Removes support for the  readConcern  and  writeConcern  options from the\n Collection.listSearchIndexes()  method.  listSearchIndexes()  is an Atlas-specific method, and Atlas\nsearch indexes don't support these options. Redefines the  ServerDescription.roundTripTime  property as a moving average. Previously,\nit was a weighted average of the most recently observed heartbeat duration and the\nprevious duration. You can specify the type of a search index when creating the index, as shown\nin the following example: The  UpdateFilter.$currentDate  property no longer throws an error when you pass\nit to a compound method, such as  findOneAndUpdate() , on a collection with a limited schema. The driver throws a  MongoTransactionError  only if you provide a\n ReadPreferenceMode  other than  primary  and then try to perform a command that\ninvolves a read operation. The data type of the  TopologyDescription.error  property is  MongoError . The  Collection.indexExists()  method no longer supports the  full  option. The  Collection.indexInformation() ,  Collection.indexes() , and\n Db.indexInformation()  methods have a return type of\n IndexDescriptionCompact | IndexDescriptionInfo[]  in TypeScript. When retrieving AWS KMS (Key Management System) credentials, the driver no longer\nthrows an error when it receives an access key that includes\nan expiration timestamp. The  ClusterTime  interface no longer defines the  signature  field as required in\nTypeScript. The Node.js driver v6.5 release includes the following features: To learn more about this release, see the\n v6.5.0 Release Highlights  on\nGitHub. Updates bulk write operations to use the  pkFactory  class for document\nID generation. If you previously specified an instance of a  pkFactory  to handle\nbulk writes, the  _id  fields of the documents inserted by using bulk\nwrites may be inconsistent with the behavior in this version. Fixes the read preference that is sent with read operations to\n primaryPreferred  when the driver is connected to a secondary node in\nthe replica set. Fixes a memory leak in promise creation for socket operations. Reduces first-time connection latency when connecting to a DNS seedlist by\nquerying the  SRV  and  TXT  records in parallel. Adds tracking to container metadata when running a client in Kubernetes\nor a container environment in the  client.env.container  field of the\nhandshake document. Adds the original error document returned by the server to the\n errorResponse  field of the  MongoServerError  document. Deprecates the  CloseOptions  interface which is unused by the driver. The Node.js driver v6.4 release includes the following features: To learn more about this release, see the\n v6.4.0 Release Highlights  on\nGitHub. When multiple  mongos  instances are available, different servers are used\nfor read and write retry attempts. Caches AWS credentials at the client level, rather than for each\nauthentication. Upgrades to using BSON 6.4.0. For details about the new BSON features, see the\nrelease notes for  BSON 6.3.0  and  BSON 6.4.0 . Read operations that result in an  ExceededTimeLimit  error are retried. Fixes a request issue related to  TLS sockets  and\n KMS Providers . Fixes the base64 padding on the  saslContinue  command to allow for mongosh\nauthentication. Types  countDocuments  using  Filter<Schema>  rather than  Document ,\nwhich enables autocompletion and helps prevent downstream typing issues. Fixes a type error in the  $addToSet  option of the  bulkWrite  command.\nThe driver skips  $addToSet  validation you extend your types from\n Document  or  any , or use properties of any type. Fixes the  ServerHeartbeatSucceeded  and  ServerHeartbeatFailed  event\nheartbeat duration so that it does not include the time to create the socket. Appropriately emits errors from cursor transform streams, rather than\nabsorbing them. Makes AWS session tokens optional when a username and password are provided,\nand allows AWS SDK to handle the authentication requests. The Node.js driver v6.3 release includes the following features: To learn more about this release, see the\n v6.3.0 Release Highlights . Adds the  serverMonitoringMode  client option to control the\nbehavior of the monitoring connection among the nodes in a topology.\nThis option takes a value of  auto  (default),  poll , or\n stream . To learn more, see the entry for this option in the\n Connection Options  guide. You can set the  serverMonitoringMode  option in a\n MongoClientOptions  instance or as a connection string option. The\nfollowing example shows how to create a client with the option set to\n stream : Fixes a connection leak when the  serverApi  client option is set. Deprecates the  contentType  and  aliases  GridFS options. To\nstore the content type and aliases of a file, add  contentType  and  aliases \nfields to the  metadata  document. The Node.js driver v6.2 release includes the following features: To learn more about this release, see the\n v6.2.0 Release Highlights . Updates the  bson  package version to 6.2.0 to include\ncolor visualization of types, as shown in the following image: To learn more, see the  bson v6.2.0 release notes . Ensures that the  result.insertedIds  property of a bulk write error type\ncontains the  _id  values of successfully inserted documents. In\nprevious versions, when a bulk write operation rejected an insert\noperation, the  result.insertedIds  property contained the\n _id  values for all attempted inserts. Closes the implicit session created when running the  findOne() \nmethod on a time series collection regardless of the outcome of the\noperation. Allows the creation of collections that have names that start or end with the\n .  character. This change aligns the driver's database and\ncollection name-checking behavior with the server's. The Node.js driver v6.1 release includes the following features: To learn more about this release, see the\n v6.1.0 Release Highlights . Updates the  bson  package version to 6.1.0 to expose the\n Decimal128.fromStringWithRounding()  method. To learn more, see the\n v6.1.0 bson release notes . Detects environment variables for region settings when\nauthenticating by using the  MONGODB-AWS  authentication mechanism.\nTo instruct the driver to use your region options, you must set both\nof the following environment variables: AWS_STS_REGIONAL_ENDPOINTS AWS_REGION Fixes a memory leak issue caused by recursive calls to the  next() \nmethod of the  ChangeStream  type. The Node.js driver v6.0 release includes the following features: To learn more about this release, see the\n v6.0.0 Release Highlights . This driver version introduces breaking changes. For a list of these changes, see\nthe  Version 6.0 Breaking Changes section  in the\nUpgrade guide. All of the  ssl -prefixed options in the  MongoClientOptions \ntype are deprecated. In addition, the  tlsCertificateFile  option\nis deprecated. Instead, you should store your certificates in a  SecureContext \nobject or set the  tls -prefixed options in your\n MongoClientOptions  instance. To learn more, see  Enable TLS on a Connection . Removal of support for the  addUser()  helper command. Use the\n createUser  MongoDB Shell command instead. Removal of support for the  collStats  operation. Use the\n $collStats  aggregation operator\ninstead. The  options  field of the  ConnectionPoolCreatedEvent  type\ncontains only the following fields, which are the non-default pool\noptions: maxConnecting maxPoolSize minPoolSize maxIdleTimeMS waitQueueTimeoutMS The driver asynchronously reads files set in the  tlsCAFile  and\n tlsCertificateKeyFile  connection options when you call\nthe  MongoClient.connect()  method, not when you create a\n MongoClient  instance. Removal of the  keepAlive  and  keepAliveInitialDelay  connection\noptions. The value of  keepAlive  is permanently set to  true  and the\nvalue of  keepAliveInitialDelay  is set to 300000 milliseconds (300\nseconds). To learn how to set keepalive settings at a system level,\nsee the  Does TCP keepalive time affect MongoDB Deployments? \nFAQ entry in the Server manual. Removes the following options for the  Db.command()  method: Although you cannot pass these options to the\n Db.command()  method, you can still set them in the command\ndocument. To learn more, see the  Command Options  section of the Run a Command guide. willRetryWrite omitReadPreference writeConcern explain readConcern collation maxTimeMS comment retryWrites dbName authdb noResponse The Node.js driver v5.9 release includes the following features: To learn more about this release, see the\n v5.9.0 Release Highlights . This version includes a fix to a memory leak introduced in v5.7.\nWe recommend upgrading to v5.9. Fixed a memory leak introduced in v5.7. The  Decimal128  constructor and  fromString()  methods now throw an exception\nwhen they detect a loss of precision of more than 34 significant digits.\nThe  Decimal128  class exposes a new  fromStringWithRounding()  static method that\nuses the rounding behavior from previous versions of the driver. For more information,\nsee the  Release Notes for v5.5 of the js-bson package \non GitHub. Added support for detecting the  AWS_STS_REGIONAL_ENDPOINTS  and  AWS_REGION \nenvironment variables and setting the appropriate options when calling the\n fromNodeProviderChain()  method in the AWS SDK. The Node.js driver v5.8 release includes the following features: To learn more about this release, see the\n v5.8.0 Release Highlights . This version includes a fix to a memory leak introduced in v5.7.\nWe recommend upgrading to v5.9. The  AutoEncrypter  interface is deprecated. Support for Kerberos versions 1.x and 2.x. Deprecation errors are not emitted for the\n tlsCertificateFile  property when you set the\n tlsCertificateKeyFile  property. Removes credential availability in the\n ConnectionPoolCreatedEvent  type. You can still access credentials\nthrough the  credentials  property of a  MongoOptions  instance. Lowers the  @aws-sdk/credential-providers  version to 3.188.0\nand  zstd  to ^1.0.0. The Node.js driver v5.7 release includes the following features: To learn more about this release, see the\n v5.7.0 Release Highlights . The following Write Concern options are deprecated: To specify the write concern behavior, use the  wtimeoutMS  and\n journal  options instead. To learn more about these options, see the\n Connection Options  page. wtimeout j fsync SSL options and other transport encryption options are deprecated.\nTo learn more about the deprecated options and which options to use\ninstead, see the Legacy SSL options deprecated section in the\nv5.7.0 Release Highlights linked at the end of this section. A new option for compound operation methods. The\n includeResultMetaData \noption allows you to specify whether to include information about the\noperation result. See the  Built-in Methods  section of the Compound Operations\nguide for more information. Support for change stream split events which enables processing change\nstream documents that exceed the 16MB maximum BSON size limit. An API to manage Search indexes from within your application. To\nlearn more, see  Search Indexes . The Node.js driver v5.6 release includes the following features: To learn more about this release, see the  v5.6.0 Release Highlights . The driver now supports Node.js v20. The driver can return a cursor as the response to a server command when you\ncall the  runCursorCommand()  method. To learn more about this feature,\nsee the  runCursorCommand API documentation . Support for specifying time series collection creation options\n bucketMaxSpanSeconds \nand\n bucketRoundingSeconds .\nTo learn more about these time series collection options, see\n Set Granularity for Time Series Data \nin the Server manual. New features of the 5.5 Node.js driver release include: To learn more about this release, see the  v5.5.0 Release Highlights . The driver now accurately detects Function-as-a-Service (FaaS)\nenvironments in AWS by considering AWS environment variables only if\nthey begin with  AWS_Lambda_ . You must upgrade  mongodb-client-encryption  to version 2.8.0 or\nlater if you want to create an encrypted collection by using the\nQueryable Encryption feature. New features of the 5.4 Node.js driver release include: To learn more, see the  v5.4.0 Release Highlights . The  collStats  operation is deprecated. Use the  $collStats  aggregation operator instead. The TypeScript interface passed to the  db.command()  method incorrectly\nincludes certain options. These options have been deprecated. The  ChangeStream.tryNext  method now uses the schema-specific\n TChange  generic type instead of the  Document  interface. New features of the 5.3 Node.js driver release include: To learn more, see the  v5.3.0 Release Highlights . The  forEach()  cursor method, which allows you to iteratively access\nresults from queries and aggregations, is deprecated. Use the\n for await...of  syntax instead, as shown\n here. The  addUser()  method is deprecated. Use  createUser()  instead. The  keepAlive  and  keepAliveInitialDelay  connection options are\ndeprecated. Methods that contain duplicated functionality in the  BulkWriteResult  class are deprecated.\nSee the\n API documentation \nfor a full list of deprecated methods and the preferred alternatives. Client metadata now includes function as a service (FaaS) environment information\nand alternative runtime detection. The driver now allows SRV record addresses that contain a trailing dot. UpdateResult.upsertedId  now returns null when no documents are updated. New features of the 5.2 Node.js driver release include: To learn more, see the  v5.2.0 Release Highlights . The driver now supports automatically obtaining Azure credentials when using\nautomatic Queryable Encryption. New features of the 5.1 Node.js driver release include: To learn more, see the  v5.1.0 Release Highlights . The driver now supports automatic serialization of JavaScript  bigint  to\n BSON.Long . It also supports the deserialization of  BSON.Long  values returned\nfrom the server to  bigint  values when the  useBigInt64  flag is passed\nas true. New features of the 5.0 Node.js driver release include: This driver version introduces breaking changes. For a list of these changes, see\nthe  Version 5.0 Breaking Changes section  in the\nUpgrade guide. By default, the driver no longer checks types referenced in dot notation\nunless the  StrictFilter  type annotation is explicitly\nused. To learn more about this change, see the  Typescript fundamentals\npage . This change is for Typescript only, and does not affect queries or operations\nat runtime. Optional installation of  @aws-sdk/credential-providers  as a peer dependency. The driver no longer includes AWS SDK modules by default. Use the\nfollowing  npm  command to install the SDK: If you install the SDK,  npm  notifies you if the version of the SDK you\ninstalled is incompatible with the driver. Once you install the\ndependency successfully, the driver uses the AWS SDK itself to\nmanage credentials from the environment. New features of the 4.17 Node.js driver release include: To learn more, see the  v4.17.0 Release Highlights . Adds the  mongodb-js/saslprep  package as a driver dependency. Improves compatibility with the Queryable Encryption feature. New features of the 4.16 Node.js driver release include: To learn more, see the  v4.16.0 Release Highlights . Includes Function-as-a-Service (FaaS) platform information in the driver\nhandshake metadata. Identifies Deno runtime usage in the client metadata. New features of the 4.15 Node.js driver release include: To learn more, see the  v4.15.0 Release Highlights . Support for AWS IAM roles for service accounts. New features of the 4.14 Node.js driver release include: This version includes a fix to a memory leak introduced in v4.13.\nWe recommend upgrading to v4.14. Fixed a memory leak introduced in v4.13. Deprecated methods and options that reference the legacy Logger. New features of the 4.13 Node.js driver release include: Automatic cancellation of in-flight operations in the connection pool when\nthe driver encounters network timeout errors. Disabled causal consistency in implicit sessions to prevent conflicting\nwith the  linearizable  and  available  read concern settings. Fixed a potential memory leak by ensuring that the driver destroys\n MessageStream  instances whenever their connections are destroyed. New features of the 4.12 Node.js driver release include: To learn more, see the  v4.12.0 Release Highlights . The 4.12.1 Node.js driver includes a fix to a regression in monitoring logic\nthat could cause processes to crash. Redefinition of the  ChangeStream  class as an async iterable.\nYou can use  ChangeStream  instances in any context that expects an\n AsyncIterator . Notably, change streams can now be used in Javascript  for-await \nloops: Fix to server monitoring when the driver skips monitoring events. In\nthis release, the driver always updates its view of the topology when\nprocessing monitoring events. Performance improvements with buffering as a result of modification to\ndata structures used internally in the driver. When connecting to MongoDB Server version 6.0 or later, the driver prioritizes\n electionId  settings before  setVersion  settings during Server Discovery and\nMonitoring events. In previous versions, the prioritization order was reversed. When you install the optional  aws-sdk/credential-providers \ndependency, the driver uses the AWS SDK to retrieve AWS credentials from the\nenvironment. To learn more about this behavior, see the  MONGODB-AWS  section of the Authentication Mechanisms guide. This release includes added support for  mutually\nrecursive  collection schema types. The driver also provides type safety for\ndot-notation queries up to a depth of eight in this release. At a depth greater\nthan or equal to eight, Typescript successfully compiles your code but does not\nprovide type safety. This depth limit on recursive types is a current limitation\nof TypeScript. Suppose we have a collection of type  Collection<Author>  that contains the\nfollowing mutually recursive types: TypeScript enforces type checking up to a depth of eight. The following\ncode causes a TypeScript compilation error because the  name  property\nvalue must be a  string  type: At a depth greater than or equal to eight, TypeScript compiles your code but no\nlonger type checks it. For example, the following code assigns a  number  to a\n string  property but does not cause a compilation error because the\nreferenced property is at a depth of 10: To learn more, see the  v4.11.0 Release Highlights . New features of the 4.10 Node.js driver release include: To learn more, see  v4.10.0 Release Highlights . Callback Deprecation Callbacks are now deprecated in favor of Promises. Callbacks will\nbe removed in the next major release. The Node driver team recommends\nmigrating to promises where possible: Use  async/await  syntax. Use the Node.js  callbackify utility : Use  then  syntax: If you are unable to migrate to Promises in a large codebase, you can\nuse the legacy  Node.js driver with optional callback support . New features of the 4.9 Node.js driver release include: To learn more, see  v4.9.0 Release Highlights . Fixed an inconsistency with  writeConcern  options in the type definitions. Included the latest BSON release, which adds automatic UUID support. See the\nBSON release notes  here . New features of the 4.8 Node.js driver release include: To learn more, see  v4.8.0 Release Highlights . Version 4.8.1 fixes a type regression issue introduced in v4.8.0. By\nupgrading to v4.8.1, you can specify  _id  values and sub-documents\nwhen performing updates with the  $set  or  $setOnInsert  operators. Added auto-completion and type safety for nested keys in an update filter client.startSession()  can now be called before connecting to MongoDB estimatedDocumentCount()  method can now accept a comment New features of the 4.7 Node.js driver release include: The  MongoClient.connect()  method is now optional when connecting to your MongoDB instance Ability to  compress messages with the  Zstandard   compression algorithm Added support for the  maxConnecting  connection option Ability for change stream documents to show your documents before and after an update Added support for new change stream fields related to Cluster to Cluster Replication The  estimatedDocumentCount()  method now uses the  $count  database command Improved connecting to MongoDB in the AWS Lambda Init phase The  ResumeOptions  interface is deprecated. Use the\n ChangeStreamCursorOptions  interface instead. New features of the 4.6 Node.js driver release include: To learn more, see  v4.6.0 Release Highlights . Improved the  ChangeStreamDocument  in TypeScript. Even distribution of server selection based on load across servers. See  v4.5.0 Release Highlights \non GitHub. New features of the 4.4 Node.js driver release include: KMIP provider support when using CSFLE. TLS support when using CSFLE. Hostname canonicalization now accepts \"none\", \"forward\", and \"forwardAndReverse\" as  authMechanismProperties  when using GSSAPI. In the 4.0.0 release of the driver, the deprecated  collection.count()  method was inadvertently changed to behave like  collection.countDocuments() .\nIn this release, the  collection.count()  method is updated to match legacy behavior: If a query is provided,  collection.count()  behaves the same as  collection.countDocuments()  and performs a collection scan. If no query is  provided,  collection.count()  behaves the same as  collection.estimatedDocumentCount()  and relies on\ncollection metadata. The  cursor.count()  method is deprecated and will be removed in the next major version, along with  collection.count() .\nUse the  collection.estimatedDocumentCount()  or  collection.countDocuments() \nmethods instead. New features of the 4.3 Node.js driver release include: SOCKS5 support Option to  disable UTF-8 validation Type inference for nested documents New features of the 4.2 Node.js driver release include: srvMaxHosts  and  srvServiceName  DNS seedlist  connection options New features of the 4.1 Node.js driver release include: Added load balanced connection support for all cluster types including\nthe beta  Serverless platform . Added support for the  advanceClusterTime()  method to determine if\nthe  ClientSession  should update its cluster time. New features of the 4.0 Node.js driver release include: This driver version introduces breaking changes. For a list of these changes, see\nthe  Version 4.0 Breaking Changes section  in\nthe Upgrade guide. In this release of the driver, the deprecated  collection.count()  method was\ninadvertently changed to behave like  collection.countDocuments() . This behavior\nis corrected in  version 4.4 . We'd love to hear your TypeScript related feature requests. Please submit\nideas on our  JIRA project here . We've migrated the driver to TypeScript. You can now harness the type\nhinting and intellisense features in editors that support it to develop\nyour MongoDB applications. Enjoy the benefits of this work in pure JavaScript\nprojects as well. The underlying BSON library used by this version is now migrated to\nTypeScript. Inline documentation is now consistently formatted to improve display\nin editors. If you are a user of the community types  @types/mongodb , there will\n likely be issues  adopting the types from our codebase. We could not\nachieve a one to one match in types due to the details of writing the\ncodebase in TypeScript. The minimum supported version of Node.js is now v12.9 or greater for\nversion 4 of the driver. Support for our 3.x branches will continue\nuntil mid-year 2022 to allow time for users to upgrade. 3.x supports back to Node.js v4. Our Cursor implementation is now updated to make it clear what is possible\nbefore and after execution of an operation. There was inconsistency surrounding how the cursor would error if a\nsetting was applied after cursor execution began. Now, the cursor will\nthrow an error when attempting to apply operations in an invalid state,\nsimilar to the following: MongoError: Cursor is already initialized Affected classes: AbstractCursor FindCursor AggregationCursor ChangeStreamCursor  (This is the underlying cursor for  ChangeStream ) ListCollectionsCursor Our Cursor types no longer extend  Readable  directly. They must be\ntransformed into a stream by calling  cursor.stream() . Use  hasNext()  and  next()  for manual iteration.\nUse  for await of  syntax or any  Promise  helpers for\nasynchronous iteration. With type hinting, you should find that options passed to a  MongoClient \nare enumerated and discoverable. We've made a large effort to process\nall options in the driver to give early warnings about incompatible settings\nto get your app up and running in a correct state quickly. checkServerIdentity  is no longer checked before being passed to the\nunderlying Node API. Previously, accepted values were  false , or\na function. Now, the argument must be a function. Specifying a\nboolean will result in an error being thrown. It is no longer required to specify  useUnifiedTopology  or  useNewUrlParser . This method no longer supports a  strict  option, which returned\nan error if the collection did not exist. To assert the existence of\na collection, use the  listCollections()  method instead. BulkWriteError  is now renamed to  MongoBulkWriteError . When running bulk operations that make writes you can encounter errors\ndepending on your settings. Import the new class name  MongoBulkWriteError \nwhen testing for errors in bulk operations. DB  is no longer an  EventEmitter . Listen for events directly from your\n MongoClient  instance. The  Collection.group()  helper, deprecated since MongoDB 3.4,\nis now removed. Use the aggregation pipeline  $group \noperator instead. gssapiServiceName  is now removed. Use  authMechanismProperties.SERVICE_NAME  in the URI or as an option on  MongoClientOptions . Specifying username and password as options is only supported in the URI\nor as an option on  MongoClientOptions . The GridStore API (already deprecated in 3.x) is now replaced with  GridFSBucket .\nFor more information on  GridFS , see the  mongodb manual . Below are some snippets that represent equivalent operations. GridFSBucket uses the Node.js Stream API. You can replicate file seeking\nby using the  start  and  end  options, creating a download stream\nfrom your  GridFSBucket . GridFSBucket  does not need to be closed like  GridStore . File metadata that used to be accessible on the  GridStore  instance can be\nfound by querying the bucket. We internally now only manage a  unifiedTopology  when you connect\nto a  mongod . The differences between this and previous versions\nis  detailed here . It is no longer required to specify  useUnifiedTopology  or  useNewUrlParser . You must use the new  directConnection   option \nto connect to uninitialized replica set members. Support is now added for fine-grained verbosity modes. You can learn more\nabout each mode  here . The  instrument()  method is now removed. Use command monitoring instead.\nSee our guide on  command monitoring \nfor more information. New features of the 3.7 Node.js driver release include: Added support for load balancer mode while enabling the  useUnifiedTopology  option Added support for  Stable API  while enabling the  useUnifiedTopology  option New features of the 3.6 Node.js driver release include: Added support for the  MONGODB-AWS  authentication mechanism using Amazon Web Services (AWS) Identity and Access Management (IAM) credentials The  find()  method supports  allowDiskUse()  for sorts that require too much memory to execute in RAM The  update()  and  replaceOne()  methods support index hints A reduction in recovery time for topology changes and failover events Improvements in validation testing for the default  writeConcern Authentication requires fewer round trips to the server, resulting in faster connection setup Shorter Salted Challenge Response Authentication Mechanism ( SCRAM ) conversations Ability to create collections and indexes for multiple document transactions Running validation for a collection in the background",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const clientEncryption = new ClientEncryption(keyVaultClient, {\n  'aws:key1': {\n    accessKeyId: ...,\n    secretAccessKey: ...\n  },\n  'aws:key2': {\n    accessKeyId: ...,\n    secretAccessKey: ...\n },\n\nclientEncryption.createDataKey('aws:key-1', { ... });"
                },
                {
                    "lang": "javascript",
                    "value": "clientEncryption.createDataKey('kmip', { masterKey: { delegated: true } } );"
                },
                {
                    "lang": "javascript",
                    "value": "const documents = await users.aggregate().addStage({ $project: { name: true } }).toArray();"
                },
                {
                    "lang": "js",
                    "value": "const indexName = await collection.createSearchIndex({\n    name: 'my-vector-search-index',\n    type: 'vectorSearch',\n    definition: {\n        mappings: { dynamic: false }\n    }\n});"
                },
                {
                    "lang": "js",
                    "value": "new MongoClient('<connection string>', { serverMonitoringMode: 'stream' });"
                },
                {
                    "lang": "bash",
                    "value": "npm install --save \"@aws-sdk/credential-providers@^3.201.0\""
                },
                {
                    "lang": "js",
                    "value": "const changeStream = myColl.watch();\nfor await (const change of changeStream) {\n  console.log(\"Received change: \", change);\n}"
                },
                {
                    "lang": "js",
                    "value": "interface Author {\n    name: string;\n    bestBook: Book;\n}\n\ninterface Book {\n    title: string;\n    author: Author;\n }"
                },
                {
                    "lang": "js",
                    "value": "myColl.findOne({ 'bestBook.author.bestBook.title': 25 });"
                },
                {
                    "lang": "js",
                    "value": "myColl.findOne({\n    'bestBook.author.bestBook.author.bestBook.author.bestBook.author.bestBook.author.name': 25\n});"
                },
                {
                    "lang": "js",
                    "value": "require('util').callbackify(() => myColl.findOne())(callback)"
                },
                {
                    "lang": "js",
                    "value": "myColl.findOne().then(res => callback(null, res), err => callback(err))"
                },
                {
                    "lang": "js",
                    "value": "const fc = myColl.find({a: 2.3}).skip(1)\nfor await (const doc of fc) {\n  console.log(doc)\n  fc.limit(1) // incorrect usage, cursor already executing\n}"
                },
                {
                    "lang": "js",
                    "value": "const cursor = myColl.find({});\nconst stream = cursor.stream();\nstream.on(\"data\", data => console.log);\nstream.on(\"error\", () => client.close());"
                },
                {
                    "lang": "js",
                    "value": "const collections = (await db.listCollections({}, { nameOnly: true })\n  .toArray()).map(\n    ({name}) => name\n  );\nif (!collections.includes(myNewCollectionName)) {\n  throw new Error(`${myNewCollectionName} doesn't exist`);\n}"
                },
                {
                    "lang": "js",
                    "value": "?authMechanismProperties.SERVICE_NAME\n// or\nnew MongoClient(url, { SERVICE_NAME: \"alternateServiceName\" })"
                },
                {
                    "lang": "js",
                    "value": "new MongoClient(\"mongodb://username:password@<host><port>\")\n// or\nnew MongoClient(url, { auth: { username: \"<>\", password: \"<>\" } })"
                },
                {
                    "lang": "javascript",
                    "value": "// old way\nconst gs = new GridStore(db, filename, mode[, options])\n// new way\nconst bucket = new GridFSBucket(client.db('test')[,options])"
                },
                {
                    "lang": "js",
                    "value": "bucket.openDownloadStreamByName(filename, { start: 0, end: 100 })"
                },
                {
                    "lang": "javascript",
                    "value": "await client.connect();\nconst filename = 'test.txt'; // whatever local file name you want\nconst db = client.db();\nconst bucket = new GridFSBucket(db);\n\nfs.createReadStream(filename)\n  .pipe(bucket.openUploadStream(filename))\n  .on('error', console.error)\n  .on('finish', () => {\n    console.log('done writing to db!');\n\n    bucket\n      .find()\n      .toArray()\n      .then(files => {\n        console.log(files);\n\n        bucket\n          .openDownloadStreamByName(filename)\n          .pipe(fs.createWriteStream('downloaded_' + filename))\n          .on('error', console.error)\n          .on('finish', () => {\n            console.log('done downloading!');\n            client.close();\n          });\n      });\n  });"
                },
                {
                    "lang": "js",
                    "value": "// old way\nGridStore.unlink(db, name, callback);\n// new way\nbucket.delete(file_id);"
                },
                {
                    "lang": "js",
                    "value": "const fileMetaDataList: GridFSFile[] = bucket.find({}).toArray();"
                }
            ],
            "preview": "Learn what's new in:",
            "tags": "version, update, upgrade, backwards compatibility",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "drivers"
                ],
                "programming_language": [
                    "javascript/typescript"
                ]
            }
        }
    ]
}